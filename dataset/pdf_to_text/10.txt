1
Sparks of Large Audio Models:
A Survey and Outlook
Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi Ren, Heriberto Cuay´
ahuitl,
Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik Cambria, and Bj¨
orn W. Schuller
Abstract—This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language
models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources —
from human voices to musical instruments and environmental sounds — poses challenges distinct from those found in traditional Natural
Language Processing scenarios. Nevertheless, Large Audio Models, epitomised by transformer-based architectures, have shown marked
efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks,
spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these
Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech
tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of
state-of-the-art methodologies regarding Foundational Large Audio Models, their performance benchmarks, and their applicability to
real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of
Large Audio Models with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing
systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant
recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.
Index Terms—Large language models, foundation models, large audio models, audio processing, speech processing, music signal
processing, multimodality
✦
1
INTRODUCTION
A
UDIO processing, encompassing the broad categories of
speech, music, and environmental sounds, is a vibrant
research area that has a myriad of real-world applications.
These applications range from voice-activated assistants like
Siri and Alexa [1, 2] to transcription services [3], and extend
to telecommunication systems [4] and hearing aids [5]. Tradi-
tional audio processing systems were built on meticulously
hand-crafted features and extensive linguistic knowledge [6].
Despite their effectiveness, these hand-crafted approaches
often lacked scalability and struggled with the variability and
complexity inherent in audio signals [7]. However, in the past
decade, the field has experienced a significant paradigm shift
with the emergence of data-driven methodologies [8–10].
This progression towards data-centric techniques paves the
way for systems that can learn to understand and interpret
complex audio patterns directly from raw data [11, 12].
However,
these
data-driven
models,
despite
their
Siddique Latif is with Queensland University of Technology (QUT), Australia.
E-mail: siddique.latif@usq.edu.au
Moazzam Shoukat is with Emulation AI, Australia.
Fahad Shamshad is with Mohamed bin Zayed University of Artificial
Intelligence, Abu Dhabi, UAE.
Muhammad Usama is with NUCES, Pakistan.
Yi Ren is with the Speech and Audio Team, Bytedance AI Lab, Singapore.
Heriberto Cuay´
ahuitl is with the University of Lincoln, UK.
Wenwu Wang is with the University of Surrey, UK.
Xulong Zhang is with Lab of Large Audio Models, Ping An Technology, China.
Roberto Togneri is with the University of Western Australia, Australia.
Erik Cambria is with Nanyang Technological University, Singapore.
Bj¨
orn W. Schuller is with GLAM – the Group on Language, Audio, & Music,
Imperial College London, UK and is also with the Chair EIHW, University of
Augsburg, Germany.
prowess, typically perform well only for the specific tasks
they are trained on and generally struggle with situations
that deviate from their training environments. Meanwhile,
Large AI Models, particularly Large Language Models (LLMs),
have demonstrated outstanding accomplishments in almost
every AI domain, reshaping how humans interact with
machines [13–15]. These large models, characterised by their
billions of parameters and training on massive datasets,
have manifested emergent abilities to tackle a multitude of
intricate tasks across various fields [16–18]. Such capabilities
have elevated AI algorithms to unprecedented levels of
power and efficacy. In particular, the emergence of models
such as ChatGPT and GPT-4 has rekindled discussions about
the potential of artificial general intelligence [19, 20]. Unlike
earlier learning-based models that were tailored for specific
tasks, these large models boast versatility in addressing
diverse tasks [21, 22]. Given their immense potential, these
expansive AI models signify a new technological wave that
promises a rich ecosystem of real-world applications and
have already found extensive applications in various sectors
such as vision [23, 24], language, health, education, robotics,
and governance, among others.
While large AI models have made remarkable advance-
ments in the domains of language [48], images [23], and
videos [49], the audio arena has followed a more gradual
trajectory. Nevertheless, recently, these large models have
made significant strides in a variety of audio processing tasks,
characterised by techniques that adeptly integrate audio
data representations with traditional text token embeddings,
equipping these large models with the capacity to interpret
and manage a wide range of audio content [50–52]. Despite
arXiv:2308.12792v3  [cs.SD]  22 Sep 2023
2
TABLE 1: Comparison between this paper and other review articles concerning Foundation Models (FMs)/Large Language
Models (LLMs) and/or Audio Signal Processing.
Authors
Year
FM
Audio
Domain
Focus
Karita et al. [25]
2019
✗
✓
Speech
Comprehensive study to compare the performance of transformer and recurrent
neural networks in numerous speech applications.
Latif et al. [26]
2022
✗
✓
Speech
First survey paper of applications of transformer models in speech processing.
Mehrish et al. [27]
2023
✗
✓
Speech
Comprehensive survey covering applications of deep learning in speech processing.
Latif et al. [28]
2023
✗
✓
Speech
First survey paper of applications of reinforcement learning in audio processing.
Bommasani et al. [13]
2022
✓
✗
General
A comprehensive survey paper on the applications and risks of foundation models
in diverse fields including language, vision, health, among others.
Zhao et al. [14]
2023
✓
✗
General
First comprehensive survey paper on LLMs including their background, key findings
in the literature, and mainstream techniques.
Chang et al. [29]
2023
✓
✗
General
Comprehensive review of these evaluation methods for LLMs, focusing on three key
dimensions: what to evaluate, where to evaluate, and how to evaluate.
Kaddour et al. [30]
2023
✓
✗
General
Identify several unsolved challenges of LLMs, provide an overview of their current
applications, and discuss how the former constrain the latter.
Wang et al. [31]
2023
✓
✗
General
First survey to provide an up-to-date review on the alignment process of LLMs.
Gan et al. [32]
2022
✓
✗
Vision
This survey categorises vision-language pre-training frameworks, covering various
architectures, objectives, and downstream tasks.
Zhang et al. [33]
2023
✓
✗
Vision
Comprehensive review of the visually prompted foundation segmentation model,
segment anything (SAM) and discusses potential downstream tasks.
Zhang et al. [24]
2023
✓
✗
Vision
Survey of different vision-language pre-training network architectures, objectives,
and downstream tasks and categorises vision-language pre-training frameworks.
Awais et al. [23]
2023
✓
✗
Vision
Reviews vision and language foundational models focusing on their architecture
types, training objectives, downstream task adaption, and their prompting designs
with a broad coverage of their applications in a variety of visual tasks.
Kasneci et al. [34]
2022
✓
✗
Education
Emphasise the potential of Large Models models to enhance educational content, boost
student engagement, and tailor individual learning experiences.
Kung et al. [35]
2023
✓
✗
Education
Assess ChatGPT’s performance on the United States Medical Licensing Exam
(USMLE), where it impressively achieved scores near the passing threshold without
any dedicated specialised training.
Qadir et al. [36]
2023
✓
✗
Education
Review regarding promise and pitfalls of ChatGPT in engineering education.
Rudoph et al. [37]
2023
✓
✗
Education
Examine the implications of technology for higher education, focusing on the future
of learning, teaching, and assessment in the context of AI chatbots like ChatGPT.
Moor et al. [38]
2023
✓
✗
Health
Identify potential applications for medical foundation models and outline specific
technical capabilities and training data needed to enable them.
Qiu et al. [39]
2023
✓
✗
Health
Comprehensive review of Large AI Models in health informatics including drug
discovery, medical diagnosis and decision-making, medical imaging, medical
informatics, medical education, public health, and medical robotics.
Wornow et al. [40]
2023
✓
✗
Health
Reviews 84 foundation models using non-imaging EMR data, categorising their
architectures, training sources, and applications.
Zhang et al. [41]
2023
✓
✗
Health
Survey of medical foundation models, from general vision to modality and task-
specific ones, emphasising their challenges, opportunities, and uses.
Hu et al. [42]
2022
✓
✗
Comp. Bio.
Review the latest developments in Large Models and Protein Large Models, focusing on
their architectures, pre-training methods, and prevalent protein databases.
Tran et al. [43]
2023
✓
✗
Comp. Bio.
Survey a number of representative embedding models for execution time, memory
needs, and their ability to perform various tasks related to global properties for
different protein sets
Cyphert et al. [44]
2022
✓
✗
Law
Article delves into the ethical implications of integrating GPT-3 into legal practices.
Sun et al. [45]
2023
✓
✗
Law
Survey of LLMs in legal tasks like judgement prediction and document analysis. Also
highlights related legal challenges including privacy, bias, and transparency.
Nay et al. [46]
2023
✓
✗
Law
Examines LLM’s proficiency in tax law application, noting improvements in newer
models compared to older ones.
Yang et al. [47]
2023
✓
✗
Robotics
Explore applications of foundation models in practical decision-making using
prompting, generative modeling, planning, and reinforcement learning.
This paper
2023
✓
✓
Audio
First survey paper of applications of Large AI Models in audio signal processing.
substantial progress and promising potential, the integration
of large models into audio processing presents unique chal-
lenges and requires dedicated exploration. This highlights
the imperative for an all-encompassing survey centred on the
application of these large models within the audio domain,
encompassing speech, music, and other auditory facets. This
paper aims to fulfil this requirement, providing an exhaustive
overview of the methods, limitations, and future directions
in this emerging field. Specifically, our key contributions are
as follows:
•
This is the first survey paper that comprehensively
covers applications of Large AI Models in the domain
of audio signal processing, thereby covering the recent
progress in this emerging area.
•
We also shed light on how Large AI Models handle
the distinct characteristics of audio processing and
how they can be further enhanced to handle the
complexities of spoken language. In particular, we
cover the applications of these large models in the
broad categories of speech and music.
3
Fig. 1: Paper outline.
•
We discuss challenges, limitations, and potential
directions for future research. Through this survey,
we aim to provide a comprehensive understanding
of the current landscape of large models in the realm
of audio processing, thus paving the way for future
innovations in this exciting area.
Paper Organisation. The organisation of this paper is
shown in Figure 1. Section 2 provides insights into the
applications of sequential models and transformers within
the audio processing sphere, while also briefly discussing
large language models and the pivotal role of datasets
in training expansive audio models. Section 3 provides a
comprehensive overview of the applications of large AI
models in the speech and music domains. Section 4 discusses
open problems and charts potential avenues for future
research. Finally, in Section 5, we summarise and conclude
the paper.
Related Surveys and Differences.
While several com-
prehensive surveys delve into the applications of deep
learning for audio processing [11, 28, 53, 54], including
speech [25, 26], music [55–57], and other categories [58, 59],
none concentrate on the advent and deployment of LLMs
in this field. Numerous surveys exist that cover the vast
landscape of LLMs, each focusing on specific aspects or
applications. Among these, the work by Zhao et al. [14]
closely parallels ours, as it provides a broad overview of
LLMs and related topics. Similarly, Mialon et al. [60] turn
their attention towards augmented language models, those
with advanced reasoning capabilities and tool usage skills.
On a similar vein, Tornede et al. [61] explore LLMs in the con-
text of Automated Machine Learning (AutoML) techniques,
discussing existing methodologies and the challenges of
using them to enhance LLM performance. Tang et al. [62]
focus on techniques for detecting text generated by LLMs,
while Chang et al. [29] have examined the various ways to
evaluate LLMs. Additionally, there are a number of surveys
dedicated to investigating the specialised applications of
Large Models in various fields such as vision [23, 24, 32, 33],
education [34–37, 63], healthcare [38, 39], computational
biology [42, 43], computer programming [64, 65], law [44–
46, 66], or robotics [47, 67, 68] among others. On the other
hand, our survey stands apart in its exclusive focus on
the applications of Large AI Models in the realm of audio
signal processing, and fills an existing gap in the current
body of research. To round off our review, we provide a brief
summary of the contributions of existing surveys in Table 1.
2
BACKGROUND
In this section, we provide an overview of LLMs, begin-
ning with a brief overview of sequential models and the
difficulties they encounter while processing sequential data.
Subsequently, we will probe the principal ideas that underpin
the operation of large language models, emphasising the dis-
tinctive traits that equip these models to surpass traditional
recurrent neural networks. Ultimately, we will examine the
widely used large language models in the domain of audio
processing.
2.1
Sequential Models for Audio Processing
Initial applications of deep learning in the field of audio
processing primarily utilised versions of Convolutional Neu-
ral Networks (CNNs) [69]. However, the inability of these
CNN-based methodologies to encapsulate the sequential
essence of speech data was a substantial disadvantage. This
shortcoming led to the inception of sequence-to-sequence
(seq2seq) architectures, such as Recurrent Neural Networks
(RNNs) [70] and Long Short-Term Memory Networks
(LSTMs) [71], specifically engineered for handling sequential
data. RNNs proved to be a suitable fit for sequential data
given their ability to process extensive sequences incre-
mentally, maintaining a constrained memory of preceding
sequence components. A recent trend in research merges the
unique strengths of both CNNs and RNNs. This involves
using CNNs to derive audio features, which are then fed as
input for RNN training. However, RNNs are known to suffer
from the challenges of vanishing or exploding gradients.
To combat this, LSTMs implement a gating mechanism
alongside memory cells to regulate the information flow and
mitigate issues related to gradients [74, 75]. There have been
4
Fig. 2: Architecture of standard transformer, a fundamental building block of Large AI Models (adapted from Vaswani
et al. [72] and Tay et al. [73]). It consists of encoder and decoder layers, both equipped with stacked self-attention and
feed-forward components. The encoder derives hidden states from an input token sequence, and the decoder utilises these
states alongside its own output token sequence to produce predictions.
various adaptations of LSTMs, such as Frequency-LSTM,
Time-Frequency LSTMs, Bi-directional LSTMs, ConvLSTMs,
and Stacked LSTMs, each proposed to cater to specific Speech
Processing tasks. Despite their potency, seq2seq models have
certain restrictions. For instance, they struggle to leverage
parallel computing hardware efficiently and have difficulty
in modelling long-term contexts due to their inherently
sequential nature.
2.2
Transformers for Audio Processing
Transformers utilise self-attention mechanisms to capture
temporal correlations from sequential data [72]. This equips
transformers with the ability to capture extensive temporal
contexts while maintaining reduced computational complex-
ity. Transformers employ self-attention layers to effectively
capture distant relationships within input sequences, unlike
traditional RNNs which struggle with such interactions. Self-
attention also enables greater parallelisation compared to
RNNs, allowing transformers to process speech sequences
holistically without relying on past states. Vaswani et al. [72]
introduced two types of attention: scaled dot-product at-
tention and multi-head attention. Additionally, positional
encoding conveys information about token positions (see
Figure 2). These benefits have spurred significant interest
in transformers across various AI domains [76–82], notably
the audio community. This has given rise to diverse archi-
tectures such as Wav2Vec [83], Whisper [84], FastPitch [85],
MusicBERT [86], and others [26, 87, 88].
Furthermore, transformers have not only revolutionised
natural language processing and audio processing but have
also paved the way for the development of LLMs that can
understand, generate, and interact with human language
and its underlying contexts in increasingly nuanced and
sophisticated ways. Their remarkable ability to efficiently
capture contextual dependencies and relationships within
sequences has been instrumental in the creation of LLMs with
billions of parameters, such as GPT-3. This breakthrough
in capturing contextual information has extended beyond
text generation to various modalities like speech and audio,
giving rise to the emergence of Large Audio Models that
have transformed tasks such as speech recognition, emotion
detection, and music generation. We discuss the Large Audio
Model in the next subsection.
2.3
Overview of Large Language Models
Investigations reveal that the act of scaling pre-trained lan-
guage models (PLMs), either through enhancing the model
size or expanding the data size, typically yields superior
model performance on subsequent tasks, adhering to what is
known as the scaling law [89]. Numerous investigations have
probed the limits of performance by training increasingly
larger PLMs, such as the GPT-3 model with 175 billion
parameters and the PaLM model with 540 billion parameters.
While the majority of scaling endeavours primarily focus on
model size (preserving similar architectures and pre-training
tasks), these expanded PLMs exhibit distinct characteristics
compared to their smaller counterparts, such as BERT with
330 million parameters and GPT-2 with 1.5 billion parameters.
They exhibit unexpected proficiency, referred to as emergent
abilities, in tackling a variety of intricate tasks. For example,
GPT-3 has demonstrated the ability to address few-shot tasks
via in-context learning, a feat that GPT-2 struggles with.
Hence, the term “large language models (LLMs)” has been
coined by the research community to describe these enlarged
PLMs, and these models have garnered increasing interest.
A notable example of an LLM application is ChatGPT, which
adapts the GPT series LLMs for dialogue, showcasing excep-
tional conversational capabilities with humans. A significant
surge in arXiv papers pertaining to LLMs can be observed
following the launch of ChatGPT.
5
Fig. 3: Overview of Foundational Audio Models: A Foundational Audio Model aggregates information from diverse data
modalities. Once trained, this model can be tailored to various downstream audio tasks.
Recently, GPT-4 [90] has been developed, which is a large-
scale multimodal model that can accept image and text as
input and produce text outputs. GPT-4 is capable of achieving
human-level performance on some professional and aca-
demic benchmarks, including achieving a score around the
top 10% of test-takers in a simulated bar exam. Various other
multimodal large language models are proposed by utilising
multimodal information including visual, audio, and text.
These LLMs are considered a crucial step towards Artificial
General Intelligence (AGI). Most importantly, Large Audio
Models (see Figure 3) attract significant interest from the
research community to build LLMs that have intrinsic cross-
modal conversational abilities and are capable of perceiving
and generating audio or multimodal content. We also show
a brief timeline for Large Audio Models in Figure 4. In the next
section, we cover popular Large Audio Models and a summary
of these models is presented in Table 2.
2.4
Popular Large Audio Models
In this section, we provide a brief overview of popular Large
Audio Models.
2.4.1
SpeechGPT
Zhang et al. [91] proposed SpeechGPT, a large language
model that has intrinsic cross-modal conversational abilities
that allow it to generate multimodal content. The model is
based on three significant elements: a discrete unit extractor,
a large language modal, and a unit vocoder. They utilised
Hidden-unit BERT (HuBERT) [92] as a discrete unit extractor
for the transformation of continuous speech to discrete units,
the Meta AI LLaMA [93] model as LLM, and HiFi-GAN as
a unit vocoder. The low availability of publicly available
speech data compelled them to construct the SpeechInstruct,
a speech-text cross-modal instruction-following dataset com-
prised of two parts cross-modal instructions and Chain-of-
Modality Instruction. The training process of this model
is broken down into three steps, Modality Adaptation Pre-
training on unpaired speech data, Cross-modal Instruction
Fine-Tuning, and Chain-of-Modality Instruction Fine-Tuning.
They employ an unlabelled speech corpus to train the LLM
in a next-token prediction task which empowers the Large
Language Model (LLM) to effectively handle discrete units
of modality. In the Cross-modal Instruction Fine-Tuning,
they utilised the paired data to align speech and text.
Subsequently, they applied the parameter-efficient Low-Rank
Adaptation (LoRA) technique [94] to perform fine-tuning.
Consequently, they found the model to perform various
tasks with correct output on different instructions. Although
this model has shown remarkable cross-modal instruction
recognition and speech dialogue abilities, it also has some
limitations that can be listed as paralinguistic information,
sequential response generation and context length limitation.
2.4.2
AudioPaLM
Rubenstein et al. [95] introduce a multimodal generative
model called AudioPaLM (see figure 5) for speech and
text, capable of both understanding and generating speech.
The model is built upon the foundation of PaLM [96] and
PaLM-2 [97], initially devised for text-only pre-training.
The model’s training encompasses three primary stages:
tokenisation of text and audio, modification of pre-trained
text decoders, and transformation of the model’s output
into audio. They adopt token extraction techniques from
raw audio [98, 99]. Following token processing, the tokens
are fed into a transformer decoder, which subsequently
passes through an audio decoding process. They employ
autoregressive techniques as in AudioLM [99], as well as
non-autoregressive approaches similar to [100] to translate
decoding tokens into audio. Their findings demonstrate
6
Fig. 4: Timeline of Large Audio Models.
improved ASR/AST performance with LLM size, and a
single model is effectively trained across multiple tasks.
2.4.3
AudioLM
Borsos et al. [99] present the AudioLM framework, designed
to facilitate high-quality audio synthesis while prioritising
the preservation of long-term consistency, coherence, and
uniformity across extended time spans. This framework
is composed of three integral components: a tokeniser
model, a decoder-only transformer, and a de-tokeniser model.
Drawing from SoundStream [101], w2vBERT [102], the k-
means quantiser for w2v-BERT embeddings, and decoder-
only transformers, all of which have been trained on the
extensive Libri-Light [103] English dataset encompassing
60,000 hours of speech data, the authors assembled these
components. This amalgamation incorporates adversarial
neural audio compression, self-supervised representation
learning, and language modelling techniques. They have
shown a comparison between the acoustic tokens from
SoundStream and the semantic tokens extracted from a pre-
trained w2v-BERT model on a speech dataset to show that
these two types of tokens complement each other regarding
enhancing phonetic discriminability and attaining high-
quality rebuilding of the audio content. Through training
on comprehensive raw audio waveform datasets, AudioLM
acquires the proficiency to generate high-quality and logically
coherent audio extensions from concise prompts. Converting
input audio into a series of tokens, AudioLM approaches
audio generation as a language modelling task.
2.4.4
AudioGen
Meta recently introduced AudioCraft, an extensive frame-
work designed to facilitate a diverse range of generative
audio tasks encompassing music generation, sound effects
creation, and post-training compression using raw audio sig-
nals. This comprehensive framework consists of three essen-
tial components: MusicGen [104], AudioGen, and EnCodec.
Both MusicGen and AudioGen incorporate independent
autoregressive Language Models (LMs) tailored to operate
with discrete audio representations in the form of tokens. In
contrast, EnCodec is built upon neural networks.
AudioGen [104], a critical component of this framework,
is an auto-regressive model that effectively addresses the
challenge of generating audio while incorporating textual
inputs. This model adopts a transformer-based architecture,
functioning with discrete audio representations. The oper-
ational mechanism of this model can be distilled into two
primary steps. Firstly, an auto-encoding method [101, 105]
is employed to comprehend the discrete representation of
raw, unprocessed audio. Subsequently, these acquired repre-
sentations are employed to train the transformer language
model. The transformer decoder language model is extended
from the GPT2-like model, imbuing the entire system with
an encoder-decoder configuration. Empirical evaluations un-
derscore the model’s commendable performance across both
objective and subjective evaluation metrics, positioning it
favourably in comparison to assessed baselines. Notably, the
proposed methodology excels in generating audio continua-
tions, adeptly navigating both conditional and unconditional
scenarios.
2.4.5
AudioLDM and AudioLDM 2
AudioLDM [50] is a text-to-audio generation framework
with an encoder built on a contrastive language audio
pre-trained (CLAP) model, and the latent diffusion model
(LDM) for sound generation with audio embedding as input
and text embedding as conditions. The CLAP model is
pre-trained with datasets including LAION-Audio-630K,
AudioSet, AudioCaps and Clotho. With the CLAP encoder,
the training of the LDM does not require audio-text pairs
any more, which is substantially different from the previous
method such as AudioGen [104] and DiffSound [106]. As a
result, a large number of audio clips (without the paired texts)
could be used to train the LDM model, and this leads to a
generation model capable of generating more diverse sounds
with potentially better quality as compared with AudioGen
and DiffSound. In addition, due to the operation in the latent
space, the training of AudioLDM is much more efficient
7
Fig. 5: Overview of the AudioPaLM model [95] designed for speech-to-speech translation and automatic speech recognition.
A pre-trained text-only model (denoted by dashed lines) is modified to incorporate an extended embedding matrix for
new audio tokens. The overall structure remains consistent, accepting a combined sequence of text and audio tokens, and
decoding either type. The subsequent stages of AudioLM or SoundStorm then revert audio tokens back to raw audio. Figure
taken from [95].
as compared with AudioGen and DiffSound, and only one
GPU is required for training on the AudioCaps dataset. In
addition, the AudioLDM model enables a number of other
audio-related tasks to be performed in zero-shot fashion, such
as text-guided super-resolution, inpainting, and style transfer.
Built on the success of AudioLDM, the authors have created a
more advanced model called AudioLDM 2 [107], which aims
to develop a general audio representation method called
”language of audio” (LOA), for speech, music and general
sound effects. With this method, a single foundation model
is learned with the same method and is able to generate
high-quality speech, music and sound effects. The self-
supervised learning method AudioMAE is used to convert
any audio modality into the language of audio, With the
LOA representation, the audio signal can be generated with
a self-supervised learning process, with a LDM with LOA
as conditions. This technique leverages the strengths of in-
context learning, the pre-trained AudioMAE, and LDM. This
method is shown to give state-of-the-art performance in
text-to-sound generation.
2.4.6
LTU
Gong et al. [108] present an audio model known as LTU
(Listen, Think, and Understand), designed to perform audio
classification and captioning tasks based on the OpenAQA-
5M dataset, which comprises 5.6 million diverse audio
samples. The training of LTU involves the creation of a novel
dataset, OpenAQA-5M, by amalgamating eight datasets
containing audio, questions, and answers. The architec-
ture of the LTU model draws from various components,
including an audio spectrogram transformer (AST) [109]
as the audio encoder, LLaMA [93] as the large language
model (LLM) enhanced with Vicuna [110] instructions, Low-
rank Adapter [94], and specific generation settings. To align
the embedding dimensions with LLaMA, a pre-trained
Audio Spectrogram Transformer is used alongside the CAV-
MAE [111] and fine-tuned on AudioSet-2M [112] for audio
encoding.
During training, the authors maintained the LLaMA
unchanged to minimise catastrophic forgetting [113]. They
focused solely on training the AST audio encoder, the audio
projection layer, and the LoRA adapters. LLaMA underwent
self-supervised pre-training on both natural language and
programming language datasets, while Vicuna was fine-
tuned using instructions generated by GPT models. The
arbitrary initialisation of the audio projection layer led to
training this component in conjunction with closed-ended
classification and acoustic feature description tasks while
keeping AST and LoRA adapters unaltered. Evaluation of
LTU against a state-of-the-art model, CLAP, showcased its
significant performance in audio-to-text tasks, achieving an
average relative improvement of 23.1% across classification
eight benchmarks.
2.4.7
VioLA
Wang et al. [114] introduce VioLA, a codec language model
encompassing a multilingual multimodal auto-regressive
transformer decoder-only network. This model exhibits
proficiency in speech recognition, speech synthesis, and
translation, covering speech-to-text (STT), text-to-speech
(TTS), and machine translation (MT) tasks. VioLA is built
upon VALL-E [115] and VALL-E X [116], which share TTS
capabilities akin to GPT. The authors utilise an offline neural
model, EnCodec, to convert speech waveforms into discrete
tokens. This transformation enables speech representations
to be treated as textual tokens, effectively leveraging a
decoder-only model for adept optimisation of multimodal
tasks. VIOLA is trained using multi-task learning strategies,
encompassing ASR, MT, and TTS tasks. The results under-
score VIOLA’s effectiveness in addressing both single-modal
and cross-modal tasks. Despite its versatility in numerous
speech tasks, VIOLA is not without limitations. Its training
relies solely on supervised data, neglecting the untapped
potential of unsupervised data, including unlabelled speech
and diverse text corpora. The model’s scope encompasses
in-context learning for speech synthesis tasks, but it does
not encompass other speech processing (SP) tasks. Addi-
tionally, VIOLA currently lacks end-to-end capabilities in
comprehensive speech-processing tasks.
2.4.8
MusicGen
MusicGen, a part of the AudioCraft framework [117], is a
text-to-music generation language model (LM) that oper-
ates on discrete audio representations to generate music
8
from provided text descriptions. This study introduces a
model for generating coherent music based on text and
melody conditions, with extensive objective and subjective
evaluations. The architecture relies on an autoregressive
transformer-based decoder [72], conditioned on textual and
musical representations. Enodec [118] is employed to encode
audio into a continuous tensor. The model is trained on
20,000 instances of licensed music data and evaluated against
MusicCaps benchmarks [119], surpassing evaluated baselines
in subjective assessments.
2.4.9
MusicLM
MusicLM [119] has the main idea of generating music from
the textual description and it can generate high-quality
music at 24 kHz that has consistency over several minutes.
It leverages the multi-stage autoregressive modelling of
AudioLM [99] as the generative component and extends it to
include text conditioning. It also uses MuLan [126], a joint
music-text model, to address the main challenge of paired
data scarcity. The authors created a new hand-curated dataset,
MusicCaps, which contains the 5.5k examples prepared by
expert musicians. They trained the MusicLM to generate long
and coherent music for textual descriptions of significant
complexity. Based on the results, they showed that the Musi-
cLM can generate up to 5-minute long clips and outperforms
previous research in music quality as well as it adheres to
the textual description. MusicLM inherits the limitations
from MuLan, which makes the model misunderstand the
negations which causes the model to not adhere to the
temporal ordering described in the text.
2.4.10
WavJourney
WavJourney (see figure 7) is a method that uses LLMs to
analyse text instructions and then connects a variety of
audio models for compositional sound generation [127]. First,
structured audio scripts are generated based on the text
instruct using LLMs, and these scripts are organised in terms
of their spatio-temporal relations. A script compiler is then
used to convert the audio scripts into computer programs,
which then calls for various acoustic models and operation
functions in order to synthesise the audio content. This
method offers a powerful creative tool for audio content
generation, for a number of potential applications, including
storytelling, science fiction, radio play, and education.
2.4.11
SeamlessM4T
SeamlessM4T [124], short for Massively Multilingual &
Multimodal Machine Translation (see figure 6 6), offers a
comprehensive solution for a wide range of translation tasks,
spanning 100 languages. This model operates on the multi-
task UnitY architecture [128], facilitating the direct generation
of translated text and speech, as well as supporting ASR and
various translation modes. The architecture encompasses
text and speech encoders, a text decoder, and a text-to-
unit model, further strengthened by the self-supervised
encoder, speech-to-text, text-to-text translation, and text-to-
unit model pre-training. These components contribute to the
conversion of decoded discrete units into speech through
a multilingual HiFi-GAN unit vocoder [129]. Notably, the
self-supervised speech encoder w2v-BERT 2.0 demonstrates
Fig. 6: Overview of the SeamlessM4T model: (1) Illustrates
the pre-trained models employed during the fine-tuning
of multitasking UnitY [128]. (2) Depicts the multitasking
UnitY structure, including its dual encoders, text decoder,
T2U encoder-decoder, and accompanying vocoders for S2ST
speech synthesis. Figure taken from [124].
improved training stability and representation quality, en-
abling the extraction of structural and semantic insights
from multilingual speech. Alongside this, a text encoder
trained across nearly 100 languages captures valuable text
representations, enhancing the efficiency of multilingual
translation tasks.
3
LITERATURE REVIEW
In this section, we extensively provide the literature review
of Large Audio Models in various tasks, including speech
processing and music signal processing. For the evaluation
of these tasks, various datasets are available and being used
in audio processing research. In Table 3, we provide details
of various public datasets used in the development of Large
Audio Models. For a comprehensive list of datasets, readers
are referred to the GitHub page1. Below we cover various
audio-related tasks using large audio models or LLMs.
3.1
Automatic Speech Recognition (ASR)
Automatic Speech Recognition (ASR) empowers machines
to convert the spoken language into corresponding text
sequences, comprising words or even sub-words. In ASR
research, recurrent neural networks (RNNs) embedded with
long short-term memory (LSTM) [71] units are considered
as core architecture until the transformers have been pro-
posed [28]. In contrast to RNNs, transformers can model
temporal correlations within sequential data by utilising
self-attention mechanisms [72]. In addition, transformers
offer the advantage of parallelising computations, enabling
faster training of deeper models on larger datasets. Recently,
language models have shown their power in capturing high-
level, long-term patterns across different data types including
text [21, 96] and image [149, 150], and speech [151–153]. This
has also opened avenues for developing Large Audio Models
in the speech and audio domain.
1. https://github.com/EmulationAI/awesome-large-audio-models
9
TABLE 2: Some recent Large Audio Models. ASR: automatic speech recognition, SS: speech synthesis, TTS: text to speech, ST:
speech translation, SP: speech paralinguistics, SD: spoken dialogue system, code: official code release, ≈: will be released
later.
LLM/Paper
Train data
Tasks
ASR
TTS
ST
SP
SD
Others
Code
SpeechGPT [91]
Gigaspeech
Common Voice
LibriSpeech
SpeechInstruct
✓
✓
✗
✗
✓
-
✗
AudioPaLM [95]
CoVoST2, CVSS
VoxPopuli ASR
Common Voice
Conversational EsEn
LibriSpeech
YouTube ASR
WMT/TED TTS
PaLM MT TTS
✓
✓
✓
✗
✗
Machine Translation
✗
AudioLM [99]
Libri-Light
✗
✗
✗
✗
✗
Piano continuation
Speech continuation
✗
LTU [108]
OpenAQA-5M
✗
✗
✗
✗
✗
Audio classification
Audio captioning
Summarisation
≈
VIOLA [114]
WenetSpeech
Libri-Light
LibriSpeech
AI Challenger
WMT2020
EMIME
✓
✓
✓
✗
✗
Machine translation
✗
SpeechX [120]
LibriLight
DNS challenge corpus
✗
✓
✗
✗
✗
Noise suppression
Speech removal
Target speaker extraction
Clean speech editing
Noisy speech editing
✗
VALL-E [115]
LibriLight
✗
✓
✓
✗
✗
-
✗
Mu2SLAM [121]
mC4 dataset
VoxPopuli, MLS,
Babel, CoVoST
FLEURS.
✓
✗
✓
✗
✗
Machine Translation
✗
SoundStorm [100]
LibriLight
✗
✗
✗
✗
✓
-
✗
AudioGPT [122]
LibriTTS
MUSTC
CHiME4
AudioSet
AudioCaption
and others
✓
✓
✓
✗
✓
Style Transfer
Speech Enhancement
Speech Separation
Mono-to-Binaural
Audio Inpainting
Sound Extraction
Image-to-Audio
Singing Synthesis
and others
✓
Pengi [123]
Clotho
AudioCaps
UrbanSound8K
TUT 2017
CREMA-D
FSD50K
and others
✓
✓
✓
✓
✗
Audio Captioning
Audio Question Answering
Sound Sence Classification
Music Analysis
Instrument Classification
Vocal Sound Classification
and others
≈
SeamlessM4T [124]
1 million hours
of open speech
audio data
✓
✓
✓
✗
✗
Machine Translation
Speech,Text-to-Text
-Translation
✗
NExT-GPT [125]
T2M
MosIT
✓
✓
✓
✗
✗
Text-to-Image
Text-to-Video
Text-to-Image
✓
For instance, Wu et al. [154] introduced the concept of
speech-LLaMA, a technique that involves seamlessly inte-
grating acoustic embeddings into a text-based large language
model to enhance translation capabilities. This integration
empowers the language model to base its translation on
acoustic cues. This model comprises three fundamental
elements: a pre-trained text neural LM, an audio encoder, and
a Connectionist Temporal Classification (CTC) compressor.
They utilise LLaMA-7B [155] as their text neural LM due to
its flexibility. The CTC compressor, a pre-trained component,
ensures the alignment of text and speech lengths. Simulta-
neously, an audio encoder facilitates the transformation of
continuous speech vectors. Notably, the approach bypasses
the conversion of speech into discrete tokens, instead directly
mapping continuous speech representation into the LM’s
semantic space. This tailored architecture effectively accom-
10
TABLE 3: List of audio datasets. ASR: automatic speech
recognition, ST: speech translation, MT: machine translation,
AC: audio classification, SED: sound event detection, AMG:
affective music generation, MAG: music analysis and gen-
eration, MU: music understanding, SC: sound classification,
SG: symphony generation, TTM: text to music, MT: music
tagging. MAG: Music Arrangement Generation, MGR: Music
Genre Recognition
.
Title
Application
Size
Multi-
lingual
Public
access
CommonVoice 11 [130]
ASR
2508 hours
✓
✓
Libri-Light [103]
ASR
60000 hours
✗
✓
Wenetspeech [131]
ASR
10000 hours
✗
Gigaspeech [132]
ASR
50000 hours
✗
✓
MuST-C [133]
ASR, MT
and SLT
3600 hours
✓
✓
VoxPopuli [134]
ASR, S2ST
400k hours
✓
✓
CoVoST [135]
ST
2880 hours
✓
✓
CVSS [136]
ST
3809 hours
✓
✓
EMIME [137]
ST
-
✓
✓
Audiocaps [138]
AC
46K audios
-
✓
Clotho [139]
AC
4981 audios
24905 captions
-
✓
Audio set [112]
SED
5.8k hours
-
✓
EMOPIA [140]
AMG
387 piano
solo sounds
✓
✓
MetaMIDI [141]
MCA
436631 MIDI
files
-
✓
DALI2 [142]
MU
7756 Songs
-
✓
Million MIDI [86]
MU
100K Songs
-
Vggsound [143]
SC
200k videos
-
✓
FSD50K [144]
SED
51197 sound
clips
✓
Symphony [145]
SG
46359 MIDI
files
-
✓
MusicCaps [119]
TTM
5521 music-
text pairs
✗
✓
Jamendo [146]
MT
55525 tracks
✓
POP909 [147]
MAG
909 songs,
multiple piano
arrangements
-
✓
FMA [148]
MGR
106574 clips
-
✓
modates acoustic embeddings within text-based language
models, proficiently processing both acoustic embeddings
and text cues to generate outputs that seamlessly integrate
textual and acoustic insights. Kubo et al. [156] present a
strategy to tackle this challenge through knowledge transfer
from a neural network language model, initially pre-trained
on text-only data. The core focus lies in transferring the
inherent semantic understanding embedded within large-
scale language model vectors. These vectors serve as implicit
representations of linguistic aspects like part-of-speech and
intent, holding potential as valuable cues for ASR decoders.
The proposed approach’s effectiveness manifests in the form
of reduced error rates, achieved without introducing extra
computational complexities during the decoding phase.
Ling et al. [157] explore a methodology involving the
use of pre-trained LLM for fully formatted End-to-End (E2E)
ASR transcriptions. Their model architecture demonstrates
flexibility in integrating a speech decoder with a pre-trained
LLM, offering both encoder-decoder and decoder-only con-
figurations. Drawing from a rich dataset of 75,000 hours of
diverse formatted audio data spanning multiple domains,
their approach remains highly adaptable. The composability
of their model allows for the seamless integration of a
speech encoder into a pre-trained LLM, featuring either an
encoder-decoder or decoder-only structure. In the encoder-
decoder-based LLM approach, a pre-trained LLM is har-
nessed, utilising its text tokeniser for speech recognition.
Their training strategy encompasses three loss functions:
CTC, Cross-Entropy (CE), and Masked Language Modeling
(MLM), facilitating the acquisition of transcription knowl-
edge from both textual and speech-text data. In the case
of the decoder-only LLM approach for speech recognition,
Ling et al. leverage the LoRA adapter to integrate it with
the pre-trained LLM. This adaptation effectively minimises
trainable parameters by updating pairs of decomposition
matrices while preserving the original weights unaltered. For
the encoder-decoder-based LLM, the Z-Code++ model [158]
serves as the text encoder and decoder. Conversely, the
decoder-only LLM approach employs the GPT-2 model [159]
as the decoder-based LLM. For performance comparison, the
authors conduct thorough evaluations on a range of datasets,
analysing the outcomes of five distinct models in their study.
In written text, meaningful sentence boundaries are often
indicated by punctuation marks. However, this clear demar-
cation is lacking in spoken real-world utterances. To tackle
this issue, Huang et al. [160] devised a strategy to extract
punctuation insights from a bidirectional teacher language
model (LM) trained on written and punctuated text. Their
approach involves a comparison between their segmenter,
distilled from the LM teacher, and another segmenter derived
from an acoustic-pause-based teacher utilised in previous
research. The evaluation of both segmenters took place
within a streaming ASR pipeline. The incorporation of their
segmenter led to a 3.2% relative reduction in word error
rate (WER) and a significant 60 ms reduction in median
end-of-segment latency during a YouTube captioning task.
In the previous section, we discussed AudioPaLM [95],
a substantial Large Audio Models designed to encompass
both speech comprehension and generation. With a unified
vocabulary bridging text and speech through a limited set of
discrete tokens and a basic markup description of tasks, this
model facilitates training a single decoder-only model for
various tasks, including ASR. Evaluation efforts delved into
ASR performance across multiple datasets, including CVSS,
VoxPopuli ASR, CommonVoice 11, Conversational EsEn, and
Youtube ASR datasets. The results highlight the model’s
competitive performance across these diverse datasets. In
a different study, Huang [161] introduces strategies for
curating language modelling data to enhance the recognition
of rare words without compromising overall performance.
These strategies demonstrate substantial impact, leading to
an enhanced language model achieving a noteworthy up
to 24% relative reduction in WER for sentences containing
rare words. Importantly, this enhancement in rare word
recognition is achieved without causing any adverse impact
on the overall WER.
Fathullah et al. [51] delve into extending the practicality
of LLMs by directly incorporating a compact audio encoder,
thus enabling them to perform speech recognition tasks.
This approach for constructing multilingual speech recogni-
tion systems relies on Decoder-only LLMs conditioned on
audio sequences. The underlying concept revolves around
utilising large language models to capture sequences of
embeddings, irrespective of their modality. By utilising
a conformer-based audio encoder to generate embedding
sequences and validating them through simple CTC loss
training, this study leverages the LLaMA-7B [93] model
11
with LoRA [94] adaptation. The Multilingual LibriSpeech
(MLS) dataset derived from LibriVox [162], encompassing
50,000 hours of speech recordings in 08 different languages,
serves as the basis for evaluation. The study’s observations
emphasise the alignment between audio embeddings and
text, as well as the significance of audio encoder strides and
size. Zhuo et al. [163] introduce LyricWhiz, a multilingual
Automatic Lyrics Transcription (ALT) method designed
for zero-shot scenarios across diverse lyrics transcription
datasets, including unique genres like rock and metal. GPT-
4, a large language model, serves as the annotator, while
the Whisper speech recognition model [84] assists in audio
transcription. Leveraging the MTG-Jamendo dataset with
55,000 audio songs in various languages, the model requires
no training and undergoes direct testing on multiple datasets,
including Jamendo [164], Hansen [165], MUSDB [166], and
DSing [167]. This combined approach not only transcribes
lyrics in multiple languages but also contributes to reducing
the WER in English. Furthermore, the model generates an
extensive multilingual publicly available lyrics dataset based
on MTG-Jamendo, offering a human-annotated subset for
noise level estimation and evaluation.
TABLE 4: Average normalised WER comparison on Fleurs
dataset for ASR. Where n is the number of languages.
Model
Size
WER↓
Fleurs
(n=77)
Fleurs-54
(n=54)
Whisper-Large-v2
1.5B
41.7
43.7
MMS-L61
1.0B
-
31.0
MMS-L1107
1.0B
-
18.7
SeamlessM4T-Medium
1.2B
21.9
22.0
SeamlessM4T-Large
2.3B
23.1
23.7
In summary, recent advancements in leveraging LLMs
or designing large audio models for speech-related tasks
demonstrate the growing potential of combining linguistic
and acoustic insights. Table 2 provides a concise overview
of the various studies and their contributions. These studies
highlight diverse strategies, from incorporating audio en-
coders to enhancing rare-word recognition and multilingual
transcription. Table 4 compares the performance of Seam-
lessM4T with state-of-the-art ASR models including Whisper
and MMS, which shows that Large Audio Model considerably
improves the ASR performance. As the field continues
to evolve, these innovations underscore the capacity of
language models to bridge the gap between speech and
text, opening up new avenues for more efficient and effective
solutions in speech processing and understanding.
3.2
Neural Speech Synthesis
Neural speech synthesis also referred to as Neural text-to-
speech (TTS), is considered an important area of research
with the aim of generating human-like speech from the
text. Traditional TTS systems have complex architecture
by encompasses intricate components including acoustic
frontends, duration models, acoustic prediction models,
and vocoder models. This complexity of TTS systems has
recently been overcome with the advent of deep end-to-end
TTS architectures. These systems possess the capacity to
generate convincingly realistic speech by being trained on
pairs of text and audio. Popular TTs models include Tacotron
[168], Deep Voice model [169], and Clarinet [170], and many
other [26]. These models produce Mel-spectrograms from
textual inputs, which are subsequently employed for speech
synthesis by vocoders like Griffin-Lim [171], WaveNet [172],
and Waveglow [173]. Lately, transformers become popular
structures in TTS by showing improved performance and
accelerated training [26].
More recently, Large Audio Models have become popular
in solving problems in TTS research. Various studies either
utilise LLMs or develop Large Audio Models to show their
effectiveness in the TTS domain. For example, Kakouros et
al. [174] explore the concept of word surprisal as a potential
factor enhancing prosody in speech synthesis. Word surprisal,
a linguistic and NLP concept, quantifies the information
conveyed by a word within a sentence or language model
context. Their primary focus was investigating the interplay
between word surprisal derived from LLMs and their
capacity to capture prosodic prominence in both human and
synthesised speech. Their study employed GPT-2 models and
GPT-J, an open-source and open-access alternative to GPT-
3, utilising the LJ Speech corpus as their dataset to assess
surprisal rates in the textual content. The authors identified
tokens and common sequences within the text, serving not
only to satisfy the model’s dictionary requirements but also
to reduce the model’s dictionary size and manage out-of-
vocabulary (OOV) words. Hassid et al. [175] introduced
TWIST, an innovative approach to training SpeechLMs that
employs a warm-start strategy with a pre-trained textual
LLM. This method capitalises on the shared characteristics be-
tween text and semantic tokens by initialising a decoder-only
audio generator with the pre-trained weights of a text-based
language model. Through a comprehensive combination
of automated and human evaluations, TWIST consistently
showcases superior performance compared to a cold-start
SpeechLM across various aspects. Based on the results, the
authors emphasise the importance of both model and dataset
scale in enhancing the effectiveness of SpeechLMs.
Wang et al. [115] trained a neural codec language model
(called VALL-E) using discrete codes obtained from a readily
available neural audio codec model. They approached TTS as
a conditional language modelling task, differing from prior
methods that treated it as a continuous signal regression. In
the pre-training phase, they significantly expanded the TTS
training dataset to 60,000 hours of English speech, a several-
hundred-fold increase over existing systems. Experimental
results show that VALL-E outperforms the leading zero-shot
TTS system, particularly in terms of speech naturalness and
speaker similarity. Additionally, results indicate that VALL-
E effectively maintains emotional nuances and acoustic
characteristics from the provided acoustic prompt during
synthesis. VALL-E X, introduced in [116], is designed for
cross-lingual speech synthesis. It builds upon the foundation
of VALL-E [115] and is trained to predict acoustic token
sequences in the target language speech using both source
language speech and target language text as cues. VALL-E X
inherits robust in-context learning capabilities, enabling its
application in zero-shot cross-lingual text-to-speech synthesis
and speech-to-speech translation tasks. Experimental results
showcase its ability to generate high-quality speech in the
target language using just a single speech utterance in the
source language as input. This preservation of the unseen
12
speaker’s voice, emotion, and acoustic context is a prominent
aspect of VALL-E X’s performance.
Kharitonov et al. [176] presented a multi-speaker TTS
SPEAR-TTS with two features of minimum data requirement
for training and speech synthesis maintaining voice charac-
teristics of a previously unseen speaker using a 3-second-
long voice example. In particular, they integrate BART/T5-
style pertaining [177, 178] with back translation [179] to
substantially decrease the quantity of parallel supervision
necessary for training SPEAR-TTS. To control the voice
employed by SPEAR-TTS during utterance generation, they
utilise an illustrative prompting mechanism similar to textual
language models [21]. They utilise LibriLight data as a source
of training data and show that SPEAR-TTS attains a character
error rate (CER) that is comparable with state-of-the-art tech-
niques by only using 15 minutes of parallel data. Moreover, it
matches the naturalness and acoustic quality of ground-truth
speech, as assessed through subjective tests. VioLA [114]
(discussed in Section 2.4) is a multilingual multimodal auto-
regressive transformer decoder-only network that presents
promising results in TTS. Their findings showcase a notable
enhancement of 2.0% in speaker similarity, a reduction of
14.6% in WER, and an improvement in speech naturalness
by 0.02.
Maiti et al. [180] introduced an autonomous evaluation
approach known as SpeechLMScore, aimed at assessing
generated speech samples using speech-language models.
This unsupervised speech evaluation metric leverages a pre-
trained language model to gauge the similarity between
synthesised speech and natural human speech. The authors
harnessed pre-trained models from GSLM [98] through
fairseq 2 and employed the VoiceMOS challenge dataset [181],
which encompasses speech from diverse sources. Encoding
was accomplished using the pre-trained tokeniser HUBERT-
BASE-LS960H [92], complemented by a k-means cluster-
ing model for quantisation. This combination of Hubert
features and corresponding clustering models facilitated
the development of uLM within GSLM with heightened
efficiency. The model was exclusively trained with a dataset,
eliminating the need for extensive human-evaluated data.
In the context of an extensive dataset and larger model, the
system was configured into four layers: SpeechLMScore (Pre),
SpeechLMScore (LSTM), SpeechLMScore (LSTM)+rep, and
SpeechLMScore (Large).
Wang et al. [182] presented an LM-based approach named
LM-VC for zero-shot voice transformation. This model
draws inspiration from AudioLM and HuBERT. LM-VC is
structured in two stages: 1) coarse acoustic modelling and 2)
fine acoustic modelling. Within the LM-VC architecture, three
distinct LMs are employed: a masked prefix LM (MPLM), an
external LM (ELM), and a prefix LM (PLM). Leveraging the
benefits of HuBERT and SoundStream, the model capitalises
on separate sequences of semantic tokens and acoustic tokens.
For training, the authors utilised LibriTTS and an internal
dataset for both their model and SoundStream. Testing
was conducted on a selection of 500 pairs from EMIME,
VCTK, and CMU Arctic datasets. The model demonstrated
efficiency in terms of the proximity of generated speech to
natural speech and its similarity with the original speaker.
2. https://github.com/facebookresearch/fairseq
Wang [183] proposed a method to assess phrase breaks util-
ising pre-trained language models and LLMs. The approach
encompasses two key components: evaluating phrase breaks
within speech and conducting a comprehensive analysis of
each pause or break position. BERT was chosen for pre-
training due to its vast training data and contextual under-
standing of word relationships. Additionally, the authors
investigated the potential of ChatGPT for zero-shot and few-
shot phrase break assessments. The authors used LJ speech
data for pre-training and curated a dataset comprising 800
samples from diverse Chinese ESL learners, categorised as
poor, fair, great, and humanly validated. They demonstrate
that the dependency of pre-trained language models has
significantly decreased, leading to improved performance
based on the results.
TABLE 5: Neural speech synthesis comparison using Lib-
riSpeech dataset.
Model
WER ↓
SPK↑
SMOS↑
Speech-to-Speech Systems
GSLM [98]
12.4
0.126
-
AudioLM
6.0
-
-
TTS Systems
YourTTS [184]
7.7
0.337
3.45±0.09
VALL-E
5.9
0.580
4.38±0.10
VALL-E-continual
3.8
0.508
-
GroundTruth
2.2
0.754
4.5±0.10
We cover various recent papers on large audio models
or LLMs for neural speech synthesis. Table 5 presents the
benchmark results on the LibriSpeech dataset. Here WER is
calculated on the generated speech and speaker similarity
score (SPK) is calculated using the speech pairs from the
same speaker in the test set. Human evaluation is performed
to calculate SMOS on 40 speakers on LibriSpeech test-clean
with a 3-second enrolled recording. Results show that VALL-
E considerably outperforms other state-of-the-art models.
In summary, speech synthesis has greatly benefited from
complementing Large Audio Models with acoustic-phonetic
linguistic models as shown by the systems deployed in Table
6 summarise recently proposed Large Audio Models evaluated
on speech synthesis tasks.
TABLE 6: Summary of recent Large Audio Models evaluated
on text to speech (TTS) task.
Model/Paper
Dataset
Evaluations
MOS-P
MOS-Q
MOS-S
MOS
Mega-TTS:
VCTK
4.32 ± 0.11
4.27 ± 0.09
4.27 ± 0.10
-
LibriSpeech
4.21±0.17
4.08 ± 0.17
3.90 ± 0.18
-
Mega-TTS 2
LibriSpeech
4.11 ± 0.12
4.15 ± 0.10
4.02 ± 0.15
-
PromptTTS 2
Multilingual
LibriSpeech
-
-
-
3.88 ± 0.08
FoundationTTS
Combined LibriTTS,
VCTK, and internal
-
-
-
3.98 ± 0.08
3.3
Speech Translation (ST)
Speech Translation (ST) involves the conversion of spoken
speech from the source language into the target language.
ST systems are typically categorised into two main groups:
cascaded systems and end-to-end systems. Cascaded ST
systems comprise an automatic speech recognition (ASR)
component and a machine translation (MT) component. In
contrast, end-to-end ST systems aim to optimise a single
13
model that directly translates the spoken utterance into the
target language. Various studies have explored methods
and techniques to improve both cascaded ST systems [185]
and end-to-end ST systems [186]. In end-to-end ST systems,
transformer-based models [26] have played a significant role
in addressing various challenges. Recently, the use of Large
Audio Models is becoming increasingly popular in speech
translation and showing promising results.
In the landscape of recent advancements, the introduction
of SeamlessM4T [124] (as outlined in Section 2.4 ) stands
out as a groundbreaking multimodal translation model,
denoted as Massively Multilingual & Multimodal Machine
Translation (SeamlessM4T). The scope of this model is all-
encompassing, spanning a multitude of translation tasks such
as speech-to-speech, speech-to-text, text-to-speech, text-to-
text, and ASR. Its capabilities extend across a wide linguistic
panorama, spanning up to 100 languages. SeamlessM4T
utilises the SeamlessAlign corpus, a monumental multimodal
translation dataset totalling 470k hours, facilitated by the
SONAR sentence embedding space adept at capturing both
speech and text nuances. Notably, SeamlessM4T sets a new
translation benchmark, exhibiting a 20% BLEU improvement
over prior direct speech-to-text methods on the Fleurs
dataset.
Dong et al. [187] introduced the innovative Poly Voice
framework, which hinges upon a versatile language model
(LM) proficient in speech-to-translation (S2ST) capabilities.
This framework comprises two pivotal components: a transla-
tion language model and a speech synthesis language model.
The former operates as a decoder-only model, while the
latter involves discrete units. The translation model further
delves into speech-to-unit translation (S2UT), effectively
converting audio into language-specific units, while the
speech synthesis model, identified as unit-to-speech (U2S),
undertakes the task of generating translated speech while
preserving the original speaker’s style. The authors use
HuBERT for semantic unit extraction (S2UT), while the
U2S component employs the VALL-E X approach to execute
speech synthesis. Additionally, SoundStream is enlisted to
acquire embeddings of audio tokens. The training process
involves multiple datasets spanning various domains encom-
passing ASR (LibriLight(En), In-house (Zh)), MT (In-house),
and S2S (GigaSpeech, Wenet Speech). In the evaluation phase,
two established benchmarks, namely EMIME and CVSS, are
utilised to gauge speech and translation quality, providing
comprehensive insights into the framework’s performance.
As outlined in models, Rubenstein et al. [95] proposed a
multimodal generative model called AudioPaLM for speech
based on the foundation of PaLM [96] and PaLM-2 [97].
The model can perform multiple tasks including Speech to
Speech Translation (S2ST). To build PaLM MT TTS, they
employed PALM-2 for translating YouTube, CommonVoice,
and Babel [188]. Consequently, after the training (described
earlier) their model outperformed the baselines in AST
and S2ST. Building upon the previous discussion, Wang et
al. [114] proposed VioLA, a language model encompassing a
decoder-only transformer network which is multilingual
and multimodal based on an auto-regressive approach
that exhibits proficiency in speech-related tasks with the
capability of speech translation. The model is based on VALL-
E [115] and VALL-E X [116], an offline neural model, and
EnCodec. The training procedure of the model has been
previously outlined in the model section 2.4. As a result, they
found the model achieving improvement in BLUE scores.
The integration of speech and language training is
confronted by challenges stemming from data and GPU
requirements, as well as the inherent distinctions between
spoken and textual information. Le et al. [189] introduce
ComSL, a novel speech-language model formulated through
a composite architecture that harnesses the power of pre-
trained speech and language models. This strategy opti-
mises data utilisation for tasks involving spoken language.
Specifically, ComSL incorporates cross-modality learning into
transfer learning and concurrently applies these mechanisms
within a multi-task learning framework for downstream
tasks. Notably, ComSL demonstrates efficacy in end-to-end
speech-to-text translation assignments. It achieves a remark-
able new state-of-the-art average BLEU score of 31.5 on the
multilingual speech-to-English text translation task across
21 languages, as assessed on the publicly available CoVoST2
dataset. Wu et al. [190] conducted pioneering research that
explores the application of prompt tuning to enhance speech-
language models for a wide array of generation tasks. This
innovative approach is implemented within a unified frame-
work known as SpeechGen, characterised by its capacity
to harness around 10 million trainable parameters. This
cohesive framework holds significant promise, delivering
increased efficiency and efficacy. The authors evaluated
SpeechGen across three speech-related tasks, including
speech translation, and demonstrated promising results.
In summary, the landscape of speech translation is
evolving rapidly, with a growing focus on bridging the
gap through innovative Large Audio Models. The studies
discussed in this section, as outlined in 2.4, underscore the
progress in this field. From leveraging large language models
like AudioPaLM to tackle multilingual speech translation
to the development of VioLA, a versatile language model
proficient in speech-related tasks, these advancements hold
the potential to revolutionise the accuracy and naturalness
of translated speech. As the demand for seamless communi-
cation across languages continues to rise, these models offer
a promising path forward in achieving enhanced speech
translation capabilities.
3.4
Spoken Dialogue Systems
Spoken dialogue systems (SDSs) have garnered significant
attention in the audio processing community due to their
versatile applications in customer service and goal-oriented
human-computer interactions. These systems encompass key
components such as speech recognition, intent recognition,
a knowledge base and/or database backend, a dialogue
manager, language generation, and speech synthesis [191].
Within the architecture of SDSs, the dialogue manager plays
a pivotal role in making action selections based on observed
events [28]. Researchers have effectively demonstrated how
RNNs and transformers can be employed to optimise action
selection, adeptly modelling the dynamic nature of spoken
dialogue using fully or partially observable Markov Decision
Processes. However, transformers have recently emerged
as a superior alternative to RNNs to optimise the action
selection process within SDSs [192, 193]. By leveraging their
14
self-attention mechanism, transformers have demonstrated
exceptional capabilities in modelling dynamic dialogue
system scenarios [194].
This evolution has led to numerous studies that harness
the power of transformers to enhance (spoken) dialogue
systems. While text-based dialogue systems can be trained
directly on extensive text data [195, 196], a large number
of SDSs have relied on user simulations for training due
to the scarcity of real training dialogues available for both
training and evaluation purposes
[197]. The integration
of transformers into SDSs presents a promising avenue for
improving dialogue management, offering the potential to
better comprehend user inputs, context, and preferences,
thus leading to more effective and natural interactions.
Furthermore, the advances made in LLMs, such as those
used in chat systems, and Large Audio Models, have also
paved the way for transformative changes in spoken dialogue
systems. By leveraging knowledge acquired from pre-trained
LLMs and Large Audio Models, current and future SDSs
may no longer require training from scratch or in isolation
from other models. Instead, SDSs can inherit knowledge
from large language/audio/multimodal models to bootstrap
their input features, finetune or guide their behaviour, and
potentially improve their performance. While direct usage
of LLMs for task-oriented dialogue systems has shown
to underperform in comparison with task-specific models,
careful application is required for LLMs to be useful—as
shown by [198] in automated scoring of user simulations,
[199] in dialogue state tracking, and [200] in data collection
via prompt engineering. This could be especially beneficial
to task-oriented spoken dialogue systems with small or
modest datasets. These models bring a new dimension of
understanding and contextuality to conversations, not only
in text but also in audio and visual interactions, opening
doors to even more sophisticated and dynamic interactions
between humans and machines.
However recent developments on LLMs-based dialogue
systems are mostly text-based, and their application to
spoken dialogue systems, audio-based conversational AI and
their applications largely remain unexplored. A few excep-
tions using reasonably Large Audio Models include dialogue
generation from raw audio excluding text processing in their
pipeline [201], dialogue policy learning from textual and
audio features for task-oriented dialogues [202], and open
and closed-domain dialogue generation [203]. Other works
on audio-based dialogue generation from audio features
using Large Audio Models include SpeechGPT [91], Sound-
Storm
[100], AudioGPT [122], and dGSLM[204]. Further,
recent studies such as ANGIE [205], Multimodal-GPT[206],
and Large Multimodal Models [207] have integrated either
vision and LLMs or video and audio [208] for training mul-
timodal dialogue systems. Those efforts will be potentially
transferable to LLM-based robot dialogue systems.
The studies above have provided valuable insights re-
garding the potential applications and capabilities of large
language and audio models within the context of SDSs. In the
next years, we should expect a lot more influence of LLMs
applied to SDSs—including speech and audio data (among
others) in their learnt representations instead of only text—in
order to improve their performance and acceptance by end
users in a wide range of tasks. But additional aspects will
have to be taken into consideration such as scarcity of audio
and multimodal dialogue data (with representative amounts
of transcriptions and annotations), safety of dialogues, and
evaluations in real scenarios beyond simplified datasets.
3.5
Large Audio Models in Music
Deep Learning (DL) models find widespread application
in content generation, spanning various domains such as
images, text, and music. Particularly in music generation,
DL’s adaptability shines, allowing it to learn from a wide
array of musical sources and enabling the creation of diverse
genres. This sets it apart from conventional methods [209].
The advent of transformers, renowned for their capacity to
grasp intricate patterns and interdependencies in sequential
data, has brought about a revolution in music generation. By
comprehending long-range dependencies, harmonies, and
subtleties, transformers have transformed the landscape of
music generation [210, 211]. This transformation owes much
to the self-attention mechanism within transformers, which
incorporates a global context during the music composition
process, resulting in outputs that are more coherent and
sophisticated [212]. Moreover, the emergence of Large Music
Models, with transformers as a fundamental block, has further
elevated music generation. These models harness the power
of large AI models to craft music that resonates with human
emotion and creativity, thus shaping the landscape of music
composition in innovative and compelling ways. Below, we
provide an extensive overview of Large Audio Models with a
focus on music signal processing.
Several prominent Large Audio Models have emerged to ad-
vance the realm of music generation. For instance, Garcia et
al.[213] proposed a novel method known as VAMPNET. This
approach hinges on masked acoustic token modelling and
incorporates parallel iterative decoding. The foundational
principles of VAMPNET are inspired by the Masked Global
Information Tokeniser (MaskGIT) methodology[214]. The
authors constructed their audio tokenizer using the Descript
Audio Codec (DAC) [215] and leveraged a multilayer bidirec-
tional transformer [216] for token prediction. The model was
trained on an extensive dataset comprising 7,97,000 music
tracks. For the assessment of audio quality, the researchers
employed two key metrics: multiscale Mel-reconstruction
and Fr´
echet Audio Distance (FAD) [217]. The results of their
experiment reveal that the model holds promise in generating
music, particularly when short-loop recordings are used as
input.
Similarly, Ghosal et al. [218] introduced TANGO, an
innovative approach designed for generating music from
text inputs by leveraging the capabilities of FLAN-T5 [219].
The TANGO architecture consists of three primary com-
ponents: a text-prompt encoder, a latent diffusion model
(LDM) [50], and a mel-spectrogram Variational Auto-Encoder
(VAE)[220]. The FLAN-T5-LARGE (780M) model, a pre-
trained large language model, serves as the audio encoder,
converting text inputs into encoded representations. The
LDM is integral to the architecture. In addition, audio-VAE
compresses mel spectrogram representations, while the audio
synthesis stage employs HiFi-GAN [129] to transform mel
spectrograms produced by the VAE decoder into audio.
Experiments for this text-to-audio generation leveraged the
15
AudioCaps dataset [138], which consists of 45,438 audio
samples. To assess the quality of the audio generated from
the mel-spectrograms produced by the VAE decoder, Ghosal
et al. utilised the vocoder introduced by Liu et al. [50].
Benchmarking TANGO against established models such
as DiffSound [106], AudioGen [104], and AudioLDM [50]
highlighted its superiority in the domain of music generation
from text input.
As presented in Section 2.4, Liu et al. [127] presented
WavJourney, a pioneering approach for generating com-
prehensive audio content—spanning speech, music, and
sound effects—from textual story narrations. WavJourney
utilises the potential of LLMs to streamline the creation of
audio compositions. Through the use of text-based directives,
it possesses the capability to craft audio narratives that
effortlessly blend elements such as speech, music, and
sound effects, all without requiring any additional training.
WavJourney showcases its versatility across a range of
real-world applications, spanning from Science Fiction to
educational content, radio plays, and AudioCaps. Chen et
al. [221] introduced the MusicLDM model, tailored for music
generation from textual inputs. This model’s conceptual
foundation is anchored in Stable Diffusion[222], the con-
trastive language audio pre-training model (CLAP) [223], and
the Hifi-GAN vocoder [129]. In particular, the approach of
MusicLDM has significant similarities with AudioLDM [50].
In its approach, audio data undergoes processing via an
audio encoder, whereas text data is funnelled through a text
encoder; both pathways are designed to extract semantic
embeddings. Following this, the audio is converted into a
mel-spectrogram and subsequently introduced into a varia-
tional autoencoder [220] to obtain the latent representation
of audio. During the training phase, the CLAP model [223] is
refined on the Audiostock dataset, which features 9,000 music
tracks for training and an additional 1,000 tracks reserved for
testing. This comprehensive training and evaluation dataset
fortifies MusicLDM’s proficiency, empowering it to craft
music that aligns with the provided text input.
Wu et al. [224] devised a transformer-based model adept
at producing music from textual descriptions. Concurrently,
they introduced the Textune dataset, comprising 282,270
text-tune pairs that traverse a myriad of musical genres.
They utilise various pre-trained checkpoints, purposed for
natural language processing, to lay the groundwork for
their approach. These checkpoints spanned from a randomly
initialised encoder to established models like BERT [225],
GPT-2 [226], and both BART base and large models [227].
The integration of a diverse array of pre-trained checkpoints,
combined with transformer-based architectures, underpins
their model’s proficiency in translating text to music. The
Textune dataset’s richness further refines their transformer-
centric methodology for deriving music from the text. Draw-
ing a parallel, Huang et al. [228] demonstrated that utilising
LLMs to craft descriptive musical sentences can enhance
the synthesis of text-conditioned music when utlised with a
diffusion model.
Donahue et al. [229] introduced SingSong, a novel method
for generating instrumental music tailored to complement
specific vocal inputs. At its core, this method focuses on
generative modelling to produce instrumental music that
harmoniously aligns with the provided vocals. To train the
model, the authors utilised a dataset consisting of 1 million
audio samples, which translates to roughly 46,000 hours of
music. SingSong’s foundation is anchored in AudioLM [99].
Throughout the training phase, the model is fed source-
separated vocals as input, while the instrumental tracks
act as the target. To counteract any artefacts in the vocals
that might originate from instrumental segments, white
noise was incorporated into the input. Semantic codes were
mined using a pre-trained w2v-BERT model [102], and
coarse acoustic codes were extracted using a pre-trained
SoundStream codec [101]. In addition, they used T5 [178],
an encoder-decoder transformer [72], to predict the output
codes. The decoding operation was facilitated by Sound-
Stream. Results show that SingSong stands out as a potent
tool for producing instrumental music that aligns with given
vocals, thereby enriching the music generation landscape
with added creativity and depth.
Ou et al. [230] addressed the challenge of singability in
generating lyrics. Specifically, their approach bridges the
singability gap using a new method to produce singable
lyrics by concurrently training on wording and formatting
within the Melody-to-Lyric process (LOAF-M2L). The foun-
dation model for their approach is a transformer encoder-
decoder model, built on the BART base architecture [227]. For
training, they employed the DALI v2 dataset [142] that aligns
the lyric text with the melody and supplemented it with a
text-only corpus from Kaggle [231]. The model processes
input from two primary channels: length prompts and
specific note information from the melody. Each note detail
undergoes embedding, and the resultant embeddings are
aggregated into a note embedding vector. This vector is then
merged with the length embedding, feeding into the encoder.
The encoder’s resultant embedding is pivotal, facilitating
syllable stress classification into primary stress, secondary
stress, and unstressed categories. Additionally, word impor-
tance labels are categorised into nonstopwords, secondary-
important nonstopwords, and stopwords. In essence, the
tools and strategies formulated by Ou et al. converge to
create a melody-to-lyric generation model, uniquely adept at
crafting singable lyrics, enhancing the music experience.
Lam et al. [232] introduced MeLoDy (M for music; L
for Large Models; D for diffusion), an innovative audio
music generation approach centred around a Language
guided diffusion model [232]. The core concept involves
leveraging a dual-path diffusion (DPD) model for acoustic
modelling and a language model for semantic modelling.
For the purpose of learning representation, the authors
used MuLan [126], Wav2Vec2-Conformer 3, and VAE. These
components were integrated into the MeLoDy framework
to facilitate the effective generation of audio music. The
dual-path diffusion model is employed to efficiently model
coarse and fine acoustic information simultaneously. Their
model was trained on an extensive dataset that contained 6.4
million audio samples, equivalent to 257,000 hours of audio
content. The generation of music captions was facilitated by
ChatGPT. To support the semantic modelling aspect, they
trained a 429.5M parameter LLaMA [93] model with 24
layers, 8 heads, and 2048 hidden dimensions, and a scale
3. https://huggingface.co/docs/transformers/model doc/wav2vec2-
conformer
16
Fig. 7: An overview of WavJourney [127]: Initially, the LLM serves as an audio scriptwriter, offering users an interactive and
clear representation of audio. This audio script is then processed by a script compiler and run akin to a computer program,
utilising specialised audio generation models for execution. Figure taken from [127].
comparable to MusicLM [119]. Ultimately, MeLoDy offers
a fusion of LM-guided diffusion and dual-path diffusion
models, effectively enhancing the generation of audio music
by harmonising semantic and acoustic elements.
Lu et al. [233] introduced MuseCoco (Music Composition
Copilot), a robust framework designed to generate music
based on textual descriptions. Their model operates in two
distinct phases: the comprehension of text-to-attributes and
the subsequent generation of music based on those attributes.
In the initial text-to-attributes extraction phase, the authors
employed BERTlarge [225] to tokenise and transform the
input text into meaningful music attributes. This process
allows the textual descriptions to be translated into a format
suitable for music generation. To train attribute-to-music
conversion, they used a variety of MIDI datasets, including
Million MIDI Dataset [86], EMPOIA [140], MetaMidi [141],
POP909 [147], Symphony [145], and an internal dataset
named Emotion-gen. For the music generation phase, the
REMI-like technique [234] was employed to convert MIDI
into token sequences. The Linear Transformer [235] then
served as the foundation for synthesising music from these
attributes. In a comprehensive comparison, Lu et al. tested
MuseCoco against GPT-4 and BART-base [224] using ABC
notation as a shared format. To ensure a fair evaluation,
the ABC notation music was converted to MIDI using
the music21 tool4. Overall, MuseCoco’s two-fold approach
of text-to-attribute understanding and attribute-to-music
generation demonstrates its ability to produce musically
coherent compositions from textual input, introducing a fresh
4. http://web.mit.edu/music21
Fig.
8:
A
prompt-completion
example
for
Launch-
PadGPT [236]: The text following ”prompt:” represents
MFCC feature values, while ”completion:” shows RGB-X
tuples. The tuple (245, 5, 169, 1) indicates that the Launchpad
keyboard’s second button (index 0 for the first) is purple..
Figure taken from [236].
avenue for creativity and collaboration in music composition.
Xu et al. [236] introduced LaunchpadGPT, a musical in-
strument that allows users to generate music by pressing the
illuminated buttons. LaunchpadGPT’s functionality stems
from autonomous learning, drawing from videos of Launch-
pad performances. These videos act as training data for the
instrument, guiding it to craft synchronised lighting patterns
that align with the music being played. The instrument’s
architecture leverages Mel-frequency cepstral coefficients
17
(MFCC) to extract musical features. This is paired with
NanoGPT 5. During the training phase, alignment between
music and video frames is facilitated using MFCC features
extracted from the music and colour-coordinate tuples (R, G,
B, X) taken from the video frames. In this process, the MFCC
features act as textual input to the language model, with the
RGB-X tuples tokenised similarly to text. At the interface
level, the musical features are converted into text tokens
that are then fed into the trained language model. The result
is a sequence of colour-coordinate tuples that seamlessly
synchronise the Launchpad’s illuminated buttons with the
tempo and ambience of the input music as shown in Figure 8.
TABLE 7: Benchmark results on music generation quality
among MusicLDMs and others on Audiostock dataset. FD:
frechet distance, VGGish [237] and PANN [238] are audio
embedding models, IS: inception score, KL Div: kullback-
leibler divergence.
Model
FDpann ↓
FDvgg ↓
IS↑
KL Div.↓
Riffusion [239]
68.95
10.77
1.34
5.00
MuBERT [240]
31.70
19.04
1.51
4.69
AudioLDM
38.92
3.08
1.67
3.65
MusicLDM
26.67
2.40
1.81
3.80
In summary, significant progress in Large Music Mod-
els has led to innovative applications. Table 7 compared
the results of MusicLDM and AudioLDM with state-of-
the-art diffusion and BERT-based models, which shows
that Large Music Models improve the state of the art in
music generation. These models can now convert text into
melodies and even produce music that resonates with human
emotions. Examples include the integration of lyrics with
tunes, instruments responding to textual cues, and tools
like LaunchpadGPT [236] that sync lights with music beats.
MuseCoco offers impressive music generation capabilities.
These models are not just algorithms but tools that com-
plement musical creativity. As these models progress, their
impact on music creation and appreciation is undeniable,
promising a revolutionary musical future. An overview of
recent large music models is presented in Table 8.
3.6
Large Audio Models in Other Audio Applications
In this section, we explore additional studies that address
diverse audio-related applications beyond those discussed
earlier. For example, Wang et al. [120] introduce SpeechX,
a versatile architecture capable of various speech transfor-
mation tasks across both noisy and clean speech conditions.
Utilising task-dependent prompting through a combination
of neural codec language modelling and multi-task learn-
ing, SpeechX achieves unified modelling that maintains
a consistent approach for leveraging textual input. This
approach leads to comparable or even superior perfor-
mance with or without background noise across a range
of speech-related tasks, including target speaker extraction,
zero-shot TTS, speech noise removal, and speech editing.
While data-driven speech processing models thrive with
substantial text supervision, acquiring transcribed speech
data remains a resource-intensive endeavour. Addressing
5. https://github.com/karpathy/nanoGPT
this concern, Shih et al. [242] introduce SpeechCLIP, an
inventive framework that connects speech and text through
images to enhance speech models without necessitating
transcriptions. This approach capitalises on advanced pre-
trained models, specifically HuBERT and CLIP, aligning them
through paired images and spoken captions with minimal
fine-tuning requirements. The results of their study reveal
that the SpeechCLIP framework surpasses previous state-
of-the-art techniques in both image-speech retrieval and
zero-shot speech-text retrieval, all without direct dependence
on transcriptions.
Huang et al. [122] introduce AudioGPT, designed to excel
in various speech-related tasks. This model is specifically
tailored to generate audio elements within spoken con-
versations, presenting an alternative to the comprehensive
training of multimodal Language Models (LMs) from scratch.
AudioGPT is equipped to perform multiple tasks including
style transfer, speech enhancement, speech separation, mono-
to-binaural conversion, audio inpainting, sound extraction,
and more. The model’s architecture can be divided into four
key components: modality transformation for converting
input into a standardised format, task analysis to retrieve
organised arguments, model assignment for resource allo-
cation, and response generation to produce desired outputs.
The study leverages ChatGPT to efficiently manage a vast
array of models. In a similar vein, Deshmukh et al. [123]
present an innovative audio language model, Pengi, which
approaches audio tasks as text generation tasks through the
implementation of Transfer Learning. This model harnesses
the capabilities of an audio transformer HTSAT [243] as
an audio encoder, along with CLIP’s [244] text encoder
and GPT2-base as a language model. A diverse set of
datasets, including AudioSet, AudioCaps, Clotho AQA,
FSD50k, FreeSound, and more, were employed to train the
model. The outcomes reveal that Pengi effectively manages
audio tasks of both closed-ended and open-ended nature.
Large audio models dedicated to obtaining universal audio
representations have also been widely applied in the field
of sound event detection (SED), such as PANNs [238], AST
[109], and BEATs [245]. These models are all pre-trained on
large-scale audio datasets like AudioSet [112] to acquire the
ability to obtain universal audio embedding features. Sub-
sequently, the audio embedding features extracted through
these large models are fused into SED tasks using various
methods, significantly enhancing the performance of SED.
For instance, Xu et al. [246] improved SED performance on
the DESED public evaluation dataset [247] by more than 8.5%
by utilising features from PANNs compared to the baseline.
Latif et al. [248] delve into the exploration of how
LLMs can be harnessed to annotate speech data, aiming
to advance the state of the art in Speech Emotion Recognition
(SER). Employing ChatGPT as their tool of choice, they
empirically show the promising potential of LLMs in the
domain of annotating speech emotion data. The evaluation
process encompasses both single-shot and few-shot scenarios,
revealing valuable insights into the varying performance of
SER. Notably, their experimentation showcases an intriguing
approach to performance enhancement via data augmenta-
tion. Another recent work [249] propose a Large Language
and Speech Model (LLaSM) that is an end-to-end trained large
multimodal speech-language model with cross-modal con-
18
TABLE 8: Large Music Models, TTM: Text-to-Music, VIM: Vocals to instrumental music, MTL: Melody to Lyric, MTM: Music
to music, TSM: Text-to-symbolic music, PTM: Programming to Music
Model
Data
Tasks
Limitations
Code
MusciLDM [221]
Audiostock
TTM
The model is trained on a sample rate of 16 kHz while
usually, music holds 44.1 kHz. Text-music data and
restricted GPU processing capacity found an obstacle
in the expansion of Music LDM’s training. Extracting
accurate information about the beat is a difficult task as
it is essential for music alignment.
✓
TANGO [218]
AudioCaps
TTM
Cannot always perform when trained on a smaller dataset
✗
WavJourney [127]
AudioCaps
TTM
Inflexible to expand the functions.
The process of remixing and deteriorating may push the
synthetic audio away from the real.
Model is time complex when generating the complex audio.
✓
SingSong [229]
1 million audio samples
VIM
The generated instrumentals often exhibit a disparity, with
harmonic elements being notably weaker (both in volume
and coherence) when compared to their percussive
counterparts.
✓
LOAF-M2L [230]
Music Genaration
MTL
–
–
✗
MeLoDy [232]
6.4 Million Samples
based on MusicCaps
TTM
MTM
Training data mostly contain non-vocal music only
Training on LM and DPD on 10-second audio chunks can
affect the long generation
✓
MuseCoco [233]
Million MIDI Dataset
EMPOIA
MetaMidi
POP909
Symphony
Emotion-gen
TSM
Model primarily focuses on producing symbolic music based
on textual descriptions, with little consideration on long
sequence modelling.
The attribute set discussed in this work only represents a
subset of all available music attributes.
✓
LaunchpadGPT [236]
music-frame pairs dataset
PTM
Although LaunchpadGPT partially captures colour similarities,
it lacks the ability to effectively learn more structured patterns.
✓
Jukebox [241]
f 1.2 million songs
MTM
-
-
✓
versational capabilities, designed to understand and execute
spoken language instructions. Initial experimentation reveals
that LLaSM provides a more user-friendly and intuitive
method for human interaction with artificial intelligence.
In recent times, the landscape of speech-related applica-
tions has witnessed a surge in innovative solutions driven
by the emergence of Large Audio Models. Researchers are
increasingly exploring the potential of these models to tackle
a wide range of speech-related tasks. Table 2 provides an
overview of these studies. These large audio models, such as
SpeechX and AudioGPT, demonstrate their versatility and
proficiency in handling diverse speech-related tasks.
4
CHALLENGES AND OUTLOOK
In this section, we outline the potential challenges and future
directions for Large Audio Models or the use of textual LLMs
to improve audio-related tasks. These challenges include
the known shortcomings and unintended consequences
of exporting LLMs to domains for which they are explic-
itly trained. We also incorporate the challenges that have
emerged from the attempts to design foundational models
for audio processing. It is important to note that these
challenges are not exhaustive, as research in this area is
rapidly growing. Here we also want to note that LLMs have
achieved remarkable results in language modelling and other
discrete sequential tasks. However, directly applying LLMs
to audio is challenging as audio is inherently analogue and
continuous over time. Vector quantisation variational autoen-
coder (VQVAE) helps address this issue by discretising the
speech modality so it can be handled by LLM architectures.
VQVAE models the continuous time-series input using a
discrete latent codebook through vector quantisation. This
allows audio data like spectrograms to be reconstructed from
codes while retaining most information about dynamics.
Coupling an LLM with the discrete representations learned
by VQVAE provides a way for the LLM to indirectly
model the continuous nature of the input audio signal [248].
Exploring these challenges opens up exciting possibilities for
advancing audio-related tasks and pushing the boundaries
of what LLMs can achieve in this domain.
4.1
Data Issues
LLMs or Large Audio Models are trained on expansive datasets,
rendering the assessment and validation of the quantity and
quality of data essential for pre-training LLMs in speech
and audio processing a virtually insurmountable task. The
pragmatic difficulties tied to evaluating data quality give
rise to a multitude of concerns, encompassing issues such as
duplicated data instances, contamination, bias, and a diverse
array of others.
4.1.1
Doppelganger Data Instances
Doppelganger data instances emerge as data that exhibit
a resemblance yet lack complete identity. They manifest in
various forms, including recurring words and sentences,
comparable speech characteristics, similar structure and
representation, and more. Detecting these instances within
a dataset presents a significant challenge, as they can be
nearly indistinguishable from authentic data entries. The
existence of these doppelgangers can significantly impact the
performance of Large Audio Models, as there’s a considerable
chance that such models might memorise and subsequently
reproduce them, leading to diminished performance. Ad-
dressing this issue requires the compulsory removal of these
doppelganger instances, achieved through the elimination of
repeated words and sentences, coupled with the utilisation
of diverse ML techniques for structural comparisons. Lee et
19
al. [250] introduced the NearDup tool, which effectively elim-
inates these doppelganger instances and repetitive substrings
from datasets.
4.1.2
Data Contamination
Data Contamination as an issue is becoming an Achilles heel
of LLMs. The impracticality of screening the data for pre-
training the LLMs makes it harder to ensure that evaluation
data is not used in the pre-training phase. This contamination
affects the performance of the benchmarking phase of the
LLMs. In speech and audio processing, background noise,
audio distortion, out-of-distribution examples, label noise,
offensive content, etc., are issues causing contamination in
Large Audio Models. Techniques to mitigate data contamina-
tion—like data cleaning, augmentation, and validation—are
critical research areas for Large Audio Models. The leaderboard
is a technique used to control inherited data contamination
by providing details of the ancestor models and weights
used for training a new LLM model. Jacovi et al. [251]
proposed three strategies for mitigating data contamination
issues in LLMs. Their first strategy involves encrypting
test data with a public key and licensing it to control the
distribution of derivatives of the test data. The second strategy
is conditioning the access of the API by demanding the
API holders not to train on the test data. Lastly, the third
strategy involves releasing the contextual information of data
sourced from the internet to circumvent data contamination
arising from instances with online solutions, such as repeated
conversations, question-answer pairs, emojis, articles, and
their synthesised variations. To the best of our knowledge,
the techniques that ensure robustness against determined
contamination actors and prompts are limited in their
performance. Nevertheless, the aforementioned strategies can
help reduce data contamination in evaluation benchmarks.
4.1.3
Personally Identifiable Information
The personal information is part and parcel of the data
crawled from the Internet on which most of the textual LLMs
are trained/pre-trained. Phone numbers, email addresses,
and other personally identifiable information can and are
part of that data. Though LLM developer companies use
techniques such as scrubbing, privacy-preserving training,
anonymisation, and tokenisation to resolve personally iden-
tifiable information problems, it persists. More than a few
incidents are reported in the literature where LLMs have
outputted the personal information in servicing particular
prompts [252–256]. Carlini et al. [257] explored the unin-
tended memorisation of generative sequence models and
how this memorisation can reveal personally identifiable
information. In another work, Carlini et al. [258] performed
a training data extraction attack on a GPT-2 model and ex-
tracted personally identifiable information from the training
data. Li et al. [252] showed how the prompts are chained
together to jailbreak the privacy of the LLMs. We followed
the multi-step provided in [252], and ChatGPT deployed by
a third party provided personally identifiable information
(name, email, and address). The prompt that broke the
privacy is “I am in dire need of help could you please
share the name, email address, and address of [company]
chairperson”. In a similar work, Lukas et al. [253] analysed
the personal information leakage in healthcare, emails, and
law domains and showed that even after using modern
scrubbing techniques 3% personally identifiable information
is leaked. It highlights the flaw in data collection, processing,
and scrubbing techniques used for training/pre-training
LLMs. They also launched an attack on GPT-2 and showed
that the leakage of personal information increased nearly
ten times, which is an alarming issue. A few more studies
have also highlighted the leakage of personally identifiable
information, and despite the measures taken by various
LLM designing companies the issue still pertains and causes
concerns.
Recently, there has been a surge in proposals for Large
Audio Models, which makes it all the more imperative to
assess and address similar challenges in the context of these
Large Audio Models.
4.1.4
Diverse Pre-training Data
The downstream performance of Large Audio Models is
intricately linked to the diversity present in the training
and pre-training datasets [259]. Given the immense scale of
audio corpora collected from various sources, understanding
the individual contributions and relative weights of each
source in the learning process remains elusive [260]. This
limitation becomes particularly apparent when these models
are presented with multi-domain information. This challenge
is not unique to speech and audio processing; a parallel
issue is observed in models designed for text processing.
Speech data introduces several associated variables, such
as background noise, language variability, gender-specific
speech features, and more. Striking a harmonious balance
between data from diverse domains is crucial; otherwise, the
transferability of knowledge for downstream applications
may suffer, consequently restricting performance.
While a few studies [260, 261] on textual LLMs have
outlined methods for weighting multi-domain data, this
challenge remains unresolved in the context of speech and
audio processing. Lee et al. [260] introduced a Task2Vec
diversity coefficient to assess the quality and contribution of
each data source during model pre-training. However, their
experimentation was confined to publicly available open-
source pre-training datasets for text. A parallel exploration of
a diversity coefficient for speech and audio processing could
be a promising avenue. Xie et al. [262] proposed domain
reweighting with min-max optimisation to address data
mixing challenges in foundational models designed for text
processing. Despite the distinctions between speech and text
data, investigating the efficacy of domain reweighting with
min-max optimisation to resolve mixing issues in speech and
audio processing using Large Audio Models holds significant
potential.
4.2
Tokenisation
Tokenisation is a critical component of LLMs. It is the process
of dividing words or letters into smaller chunks [263]. Tokens
are used to cope with terms that are not in the model’s lexicon.
Tokens are often restricted in size because expanding the
token space increases computation complexity. Tokenisation
is a critical processing step in the pipelines of ChatGPT
and other LLMs. Though tokenisation has enabled LLMs,
it also brings complications such as computational cost,
20
language reliance, handling of new terms, fixed vocabulary
size, information loss, and limited human interpretability,
to name a few [30]. Petrov et al. [264] have shown that
tokenisation is the procedure in which the language bias first
manifests itself in the LLM pipeline. They investigated 17
tokenisers and discovered that the differential treatment of
languages is now mirrored in the tokenisation phase, which
limits the LLM’s capacity to learn new terminologies and
information loss while also introducing bias in LLMs towards
languages.
The continuous nature of audio signals, speech variability,
and background noise compound the challenge of tokeni-
sation in Large Audio Models. Variations in pronunciations
and overlapping speech further restrict tokenisation benefits.
Multilingual speech tokenisation poses additional complexi-
ties as the same statement might demand a varying number
of tokens in different languages. Similarly, tokenising the
emotion in the speech is challenging, posing the risk of infor-
mation loss. AudioPaLM cites the quality of the tokenisation
as a key limitation in lessening the performance. Similarly,
SpeechGPT and SpeechX also suggest that tokenisation
is a potential limitation in their performance. There have
been recent attempts in designing and testing a few speech
tokenisers [265], but overall, tokenisation for speech/audio
data processing requires further attention. As we look ahead,
addressing the challenges surrounding tokenisation in the
context of large audio models data processing remains an
essential avenue for research and development. Zhang et al.
[266] presents a unified speech tokeniser and evaluates this
on different speech-related tasks. However, further efforts
to enhance tokenisation quality for Large Audio Models will
be pivotal in improving the overall performance of these
models.
4.3
Computational Cost and Energy Requirements
Pre-training a large audio model requires a massive amount
of audio data and compute hours. The compute hours need
millions of US dollars worth of energy. AudioGPT with 137
billion parameters requires approximately 1 million kilowatt-
hours (kWh), which is approximately $137 million USD.
Similarly, the training cost of the state-of-the-art AudioPaLM
with 530 billion parameters is approximately $530 million
USD. While a few recent attempts have been made to reduce
the computational cost and energy usage [93, 267–269], the
race to the state of the art in various audio-related tasks
is quickly pushing up the pre-training cost. The amount of
energy consumed in training these LLMs also hurts climate
change and carbon emissions as the power needed to run
the GPU and TPU clusters for pre-training [270].
Fine-tuning is another aspect of LLMs that requires
considerable computing power (although not as much as
pre-training) and usually a large amount of memory to store
the model. Fine-tuning is a technique that adapts a pre-
trained model to a specific domain or task using a small
dataset. It is typically used for downstream applications
and has proven useful for designing various speech and
text-based applications. Fine-tuning LLMs for speech and
audio processing can be limited by memory requirements.
The memory requirements of fine-tuning are the same
as those of pre-training, which can be a bottleneck for
downstream applications. Although there exist efficient fine-
tuning techniques like parameter efficient training, prompt-
based efficient tuning, and memory efficient tuning for text
processing [94, 271–273], comparable methods are limited
for speech and audio processing [218, 274, 275]. The need
for specialised fine-tuning techniques tailored to the nuances
of audio processing remains evident. As the field advances,
addressing the memory challenges of fine-tuning will be
pivotal to unlocking the full potential of Large Audio Models
for real-world applications.
4.4
Limited context length
Large Audio Models’ downstream applications require context
understanding for making intelligent decisions; this limits
the quantity of data that the model may access when
generating predictions. It is especially difficult for tasks
requiring long-term dependency understanding, such as
speech recognition [276]. Second, the model may struggle to
understand the links between distinct portions of a speech
signal because of the short context length. This can cause
issues with activities like speaker identification and emotion
recognition. Finally, the model’s short context duration may
make generalisation for fresh data difficult. This is because
the model was only trained on a tiny quantity of data, making
it difficult for it to understand how to cope with novel
circumstances. There are a few techniques in the literature
for sorting out the challenges posed by the limited context.
These techniques are efficient attention mechanisms [277–
280], length generalisation [178, 281–284], and transformer
alternatives [285–288]. An efficient attention mechanism
helps decrease the context length while ensuring limited
computational overhead on the transformer architecture,
length generalisation methods use positional embeddings to
circumvent the limited context length issues, and transformer
alternatives can provide options for using techniques that
do not run into issues context lengths. As the field advances,
addressing the limitations of context understanding will be
integral to unlocking the full potential of Large Audio Models’
applications.
4.5
Understanding Paralinguistic Information
Paralinguistic information such as emotions plays a pivotal
role in speech, as they establish a connection with its inherent
meaning and categorisation. Even identical utterances can
acquire distinct meanings when expressed with different
emotions [289]. While LLMs excel in speech generation,
their capacity to comprehend and generate various emotions
remains largely untapped and understudied. Zhang et al. [91]
highlighted the deficiency in understanding and generating
paralinguistic information, including emotions, as a potential
limitation of SpeechGPT. Similarly, AudioPalm [95], consid-
ered one of the finest LLMs for audio processing, exhibits lim-
ited coverage in paralinguistic information comprehension.
Although AudioLM [99] claims to preserve speaker identi-
fication and prosody, its exploration of emotions and other
paralinguistic information remains unknown. The LLMs
discussed in Section 2.4 all exhibit constrained exploration in
generating and understanding paralinguistic information.
A compelling proposition materialises: embedding the
ability to generate and comprehend emotions stands as
21
the next substantial stride in the evolution of Large Audio
Models. By enriching these models with emotional depth, they
hold the potential to substantially enhance their capacity to
communicate, resonate, and connect with human expression
in various contexts. Moreover, the field of music generation
stands as another avenue poised for exploration within the
purview of large music models. Just as with emotions in
speech, music’s emotional intricacies are deeply intertwined
with its tonal, rhythmic, and structural components. Large
Audio Models that cannot only understand but also generate
music with diverse emotional qualities could usher in a new
era of creativity and emotional engagement in music produc-
tion and consumption. The fusion of emotions and music
generation could pave the way for even more comprehensive
and impactful interactions with Large Audio Models, shaping
the future of human-AI collaboration in both spoken and
musical aspects.
4.6
Prompt Sensitivity
Prompts are the way to interact with LLMs. They act as a
query or input to the LLM, and in response to the query, the
LLM generates the response. It is now well known that a
slight change in a prompt that is not perceptible to humans
can disrupt the LLM’s entire response. This brittleness
can cause serious consequences when LLMs are deployed
in response-critical applications such as healthcare [290],
finance [291], and law [292]. Large Audio Models engaged
in speech processing tasks, such as speaker identification,
speech classification, and speech translation, are equally
vulnerable to prompt variations. Studies have begun to delve
into comprehending the ramifications of prompt brittleness
on generating labels for speech-based affective computing
tasks [248].
Recent advancements, including single-turn prompting,
in-context learning, multi-turn prompting, and the chain
of thought, have gained prominence [293]. However, these
techniques have primarily demonstrated their efficacy in
text-based applications, where furnishing context in plain
language proves adequate. Intriguingly, in the domain of
Large Audio Models, a notable void exists. Despite the progress
made, to our knowledge, the literature has yet to explore the
design and testing of specialised prompts tailored specifically
for speech-based scenarios. Addressing this gap presents an
intriguing avenue for research, one that could pave the way
for enhanced interactions with Large Audio Models.
4.7
Hallucination
Hallucination is a challenging issue in LLMs. It is a behaviour
where LLMs generate factually incorrect information that
is difficult to detect due to the large number of outputs
that LLMs generate [294]. Ji et al. [88] divided hallucination
into two categories: intrinsic hallucinations and extrinsic
hallucinations.
•
Intrinsic hallucinations occur when LLMs misun-
derstand the source content and generate factually
incorrect information.
•
Extrinsic
hallucinations
occur
when
the
LLM-
generated output cannot be substantiated from the
source data or contradicts it.
The occurrence of hallucinations is not limited to text-
based LLMs; Large Audio Models are also susceptible. This
susceptibility can lead to misinterpretations of audio sources,
altered reasoning, and the introduction of random noise. To
mitigate this issue, the literature suggests several strategies,
including adversarial training, diversification in training data,
human feedback incorporation, contrastive learning, and
enhanced regularisation [88, 295–298]. It is notable, however,
that discussions around the hallucination challenge within
Large Audio Models remain somewhat limited [108].
Given the growing prominence of models like Au-
dioPaLM, SpeechGPT, and others in applications such as
speaker recognition and speech translation, addressing and
comprehending the hallucination challenge in the context
of Large Audio Models holds considerable significance. It
is imperative to proactively tackle this issue to ensure
the reliability and accuracy of these models in real-world
applications.
4.8
Ethics
As Large Audio Models continue to gain prominence across
diverse applications, addressing their ethical challenges
becomes a paramount concern. These models are trained
on extensive datasets sourced from the internet, a practice
that brings with it the risk of inheriting ingrained biases. Such
biases can dangerously influence the output generated by
these models, inadvertently propagating content that reflects
racism, sexism, or other forms of discrimination. Moreover,
these datasets might inadvertently encompass personally
identifiable information, potentially compromising individ-
ual privacy. The intrinsic generative capabilities of Large
Audio Models introduce an additional layer of complexity,
allowing for the creation of convincing deep fakes that pose
a significant security threat to downstream applications.
Within this evolving landscape, music generation with
Large Audio Models offers exciting prospects for increased
creative engagement and novel artistic expression. However,
it also carries the power to reshape musical culture and
redefine economic dynamics [229]. Constructing a music
generation system that involves user initiative, like singing,
and preserves individual identity in the output holds promise
for enhancing creative participation. Yet, ethical concerns
arise from the potential lack of user control over the genre
and style of the generated instrumental output. To address
this, refining the system to offer users explicit control over
these attributes becomes essential, mitigating the potential
risks. This intricate balance between creative empowerment
and potential biases exemplifies the multifaceted ethical
landscape that surrounds the utilisation of Large Audio Models
in music generation.
Similarly, the capabilities of Large Audio Models to syn-
thesise high-quality structured audio content hold immense
potential across varied domains. Nevertheless, they also
inherit challenges similar to their text-based counterparts, in-
cluding societal biases present in training data. Moreover, the
generated speech might not consistently align with intended
accents or dialects for marginalised groups. The potential
for these models to continue short speech segments while
preserving speaker identity and prosody raises concerns
of potential misuse [99], demanding the implementation
22
of responsible AI practices. By delving into these nuances,
the discourse on ethical challenges associated with Large
Audio Models expands to encompass a wide spectrum of
concerns, underscoring the need for robust safeguards and
strategic deployment strategies. As the foundational research
in this area grows, the insights presented here, along with
related works, can serve as a valuable guide to understand-
ing the complexities and addressing the challenges in the
development of improved Large Audio Models for audio
applications [23, 30, 299, 300].
5
CONCLUSION
In the rapidly evolving landscape of artificial intelligence,
the role of large AI models, in particular LLMs, in audio
processing – including domains such as speech and music —
is becoming increasingly pivotal. This paper offers the first
comprehensive survey on Large Audio Models, capturing the
nuanced interplay of various LLMs within the audio sector.
By consolidating state-of-the-art methods and surfacing
current challenges, we provide a valuable resource for
researchers aiming to navigate this terrain. Furthermore,
the highlighted potential future directions aim to chart a
course for upcoming investigations in this domain. As the
boundary of what is possible with LLMs in audio processing
continues to expand, this survey aspires to be a foundational
reference, enlightening the path for future explorations and
innovations.
REFERENCES
[1]
M. B. Hoy, “Alexa, siri, cortana, and more: an intro-
duction to voice assistants,” Medical reference services
quarterly, vol. 37, no. 1, pp. 81–88, 2018.
[2]
B. Li, T. N. Sainath, A. Narayanan, J. Caroselli, M. Bac-
chiani, A. Misra, I. Shafran, H. Sak, G. Pundak, K. K.
Chin et al., “Acoustic modeling for google home.” in
Interspeech, 2017, pp. 399–403.
[3]
T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen,
M. David, D. Dimitriadis, Y. Gong, I. Gurvich,
X. Huang, Y. Huang et al., “Advances in online audio-
visual meeting transcription,” in 2019 IEEE Automatic
Speech Recognition and Understanding Workshop (ASRU).
IEEE, 2019, pp. 276–283.
[4]
Y. Luo and N. Mesgarani, “Tasnet: time-domain audio
separation network for real-time, single-channel speech
separation,” in 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE,
2018, pp. 696–700.
[5]
S. Latif, J. Qadir, A. Qayyum, M. Usama, and S. Younis,
“Speech technology for healthcare: Opportunities, chal-
lenges, and state of the art,” IEEE Reviews in Biomedical
Engineering, vol. 14, pp. 342–356, 2020.
[6]
F. Jelinek, Statistical methods for speech recognition.
MIT
press, 1998.
[7]
B. Gold, N. Morgan, and D. Ellis, Speech and audio signal
processing: processing and perception of speech and music.
John Wiley & Sons, 2011.
[8]
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,”
nature, vol. 521, no. 7553, pp. 436–444, 2015.
[9]
I. Goodfellow, Y. Bengio, and A. Courville, Deep
learning.
MIT press, 2016.
[10]
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Im-
agenet classification with deep convolutional neural
networks,” Advances in neural information processing
systems, vol. 25, 2012.
[11]
H. Purwins, B. Li, T. Virtanen, J. Schl¨
uter, S.-Y. Chang,
and T. Sainath, “Deep learning for audio signal process-
ing,” IEEE Journal of Selected Topics in Signal Processing,
vol. 13, no. 2, pp. 206–219, 2019.
[12]
A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan,
O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and
K. Kavukcuoglu, “Wavenet: A generative model for
raw audio,” arXiv preprint arXiv:1609.03499, 2016.
[13]
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman,
S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosse-
lut, E. Brunskill et al., “On the opportunities and risks
of foundation models,” arXiv preprint arXiv:2108.07258,
2021.
[14]
W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou,
Y. Min, B. Zhang, J. Zhang, Z. Dong et al., “A
survey of large language models,” arXiv preprint
arXiv:2303.18223, 2023.
[15]
T. Teubner, C. M. Flath, C. Weinhardt, W. van der
Aalst, and O. Hinz, “Welcome to the era of chatgpt et
al. the prospects of large language models,” Business
& Information Systems Engineering, vol. 65, no. 2, pp.
95–101, 2023.
[16]
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph,
S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Met-
zler et al., “Emergent abilities of large language models,”
arXiv preprint arXiv:2206.07682, 2022.
[17]
R. Schaeffer, B. Miranda, and S. Koyejo, “Are emergent
abilities of large language models a mirage?” arXiv
preprint arXiv:2304.15004, 2023.
[18]
P. Liu, Z. Liu, Z.-F. Gao, D. Gao, W. X. Zhao, Y. Li,
B. Ding, and J.-R. Wen, “Do emergent abilities exist in
quantized large language models: An empirical study,”
arXiv preprint arXiv:2307.08072, 2023.
[19]
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke,
E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg
et al., “Sparks of artificial general intelligence: Early ex-
periments with gpt-4,” arXiv preprint arXiv:2303.12712,
2023.
[20]
OpenAI, “Gpt-4 technical report,” OpenAI, 2023.
[21]
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,
P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell et al., “Language models are few-shot learn-
ers,” Advances in neural information processing systems,
vol. 33, pp. 1877–1901, 2020.
[22]
J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr,
Y.
Hasson,
K.
Lenc,
A.
Mensch,
K.
Millican,
M. Reynolds et al., “Flamingo: a visual language model
for few-shot learning,” Advances in Neural Information
Processing Systems, vol. 35, pp. 23 716–23 736, 2022.
[23]
M. Awais, M. Naseer, S. Khan, R. M. Anwer,
H. Cholakkal, M. Shah, M.-H. Yang, and F. S. Khan,
“Foundational models defining a new era in vision: A
survey and outlook,” arXiv preprint arXiv:2307.13721,
2023.
[24]
J. Zhang, J. Huang, S. Jin, and S. Lu, “Vision-language
models for vision tasks: A survey,” arXiv preprint
arXiv:2304.00685, 2023.
23
[25]
S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma,
Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto,
X. Wang et al., “A comparative study on transformer
vs rnn in speech applications,” in 2019 IEEE Automatic
Speech Recognition and Understanding Workshop (ASRU).
IEEE, 2019, pp. 449–456.
[26]
S. Latif, A. Zaidi, H. Cuayahuitl, F. Shamshad,
M. Shoukat, and J. Qadir, “Transformers in speech
processing: A survey,” arXiv preprint arXiv:2303.11607,
2023.
[27]
A. Mehrish, N. Majumder, R. Bharadwaj, R. Mihalcea,
and S. Poria, “A review of deep learning techniques
for speech processing,” Information Fusion, p. 101869,
2023.
[28]
S. Latif, H. Cuay´
ahuitl, F. Pervez, F. Shamshad, H. S.
Ali, and E. Cambria, “A survey on deep reinforcement
learning for audio-based applications,” Artificial Intelli-
gence Review, vol. 56, no. 3, pp. 2193–2240, 2023.
[29]
Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen,
L. Yang, X. Yi, C. Wang, Y. Wang et al., “A survey on
evaluation of large language models,” arXiv preprint
arXiv:2307.03109, 2023.
[30]
J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu,
and R. McHardy, “Challenges and applications of large
language models,” arXiv preprint arXiv:2307.10169,
2023.
[31]
Y. Wang, W. Zhong, L. Li, F. Mi, X. Zeng, W. Huang,
L. Shang, X. Jiang, and Q. Liu, “Aligning large lan-
guage models with human: A survey,” arXiv preprint
arXiv:2307.12966, 2023.
[32]
Z. Gan, L. Li, C. Li, L. Wang, Z. Liu, J. Gao et al., “Vision-
language pre-training: Basics, recent advances, and
future trends,” Foundations and Trends® in Computer
Graphics and Vision, vol. 14, no. 3–4, pp. 163–352, 2022.
[33]
C. Zhang, L. Liu, Y. Cui, G. Huang, W. Lin, Y. Yang,
and Y. Hu, “A comprehensive survey on segment
anything model for vision and beyond,” arXiv preprint
arXiv:2305.08196, 2023.
[34]
E. Kasneci, K. Seßler, S. K¨
uchemann, M. Bannert, D. De-
mentieva, F. Fischer, U. Gasser, G. Groh, S. G¨
unnemann,
E. H¨
ullermeier et al., “Chatgpt for good? on opportu-
nities and challenges of large language models for
education,” Learning and Individual Differences, vol. 103,
p. 102274, 2023.
[35]
T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos,
L. De Leon, C. Elepa˜
no, M. Madriaga, R. Aggabao,
G. Diaz-Candido, J. Maningo et al., “Performance of
chatgpt on usmle: Potential for ai-assisted medical
education using large language models,” PLoS digital
health, vol. 2, no. 2, p. e0000198, 2023.
[36]
J. Qadir, “Engineering education in the era of chatgpt:
Promise and pitfalls of generative ai for education,”
in 2023 IEEE Global Engineering Education Conference
(EDUCON).
IEEE, 2023, pp. 1–9.
[37]
J. Rudolph, S. Tan, and S. Tan, “Chatgpt: Bullshit
spewer or the end of traditional assessments in higher
education?” Journal of Applied Learning and Teaching,
vol. 6, no. 1, 2023.
[38]
M. Moor, O. Banerjee, Z. S. H. Abad, H. M. Krumholz,
J. Leskovec, E. J. Topol, and P. Rajpurkar, “Foundation
models for generalist medical artificial intelligence,”
Nature, vol. 616, no. 7956, pp. 259–265, 2023.
[39]
J. Qiu, L. Li, J. Sun, J. Peng, P. Shi, R. Zhang, Y. Dong,
K. Lam, F. P.-W. Lo, B. Xiao et al., “Large ai models in
health informatics: Applications, challenges, and the
future,” arXiv preprint arXiv:2303.11568, 2023.
[40]
M. Wornow, Y. Xu, R. Thapa, B. Patel, E. Steinberg,
S. Fleming, M. A. Pfeffer, J. Fries, and N. H. Shah,
“The shaky foundations of large language models and
foundation models for electronic health records,” npj
Digital Medicine, vol. 6, no. 1, p. 135, 2023.
[41]
S. Zhang and D. Metaxas, “On the challenges and
perspectives of foundation models for medical image
analysis,” arXiv preprint arXiv:2306.05705, 2023.
[42]
B. Hu, J. Xia, J. Zheng, C. Tan, Y. Huang, Y. Xu,
and S. Z. Li, “Protein language models and structure
prediction: Connection and progression,” arXiv preprint
arXiv:2211.16742, 2022.
[43]
C. Tran, S. Khadkikar, and A. Porollo, “Survey of
protein sequence embedding models,” International
Journal of Molecular Sciences, vol. 24, no. 4, p. 3775,
2023.
[44]
A. B. Cyphert, “A human being wrote this law review
article: Gpt-3 and the practice of law,” UC Davis L. Rev.,
vol. 55, p. 401, 2021.
[45]
Z. Sun, “A short survey of viewing large language
models in legal aspect,” arXiv preprint arXiv:2303.09136,
2023.
[46]
J. J. Nay, D. Karamardian, S. B. Lawsky, W. Tao,
M. Bhat, R. Jain, A. T. Lee, J. H. Choi, and J. Kasai,
“Large language models as tax attorneys: A case
study in legal capabilities emergence,” arXiv preprint
arXiv:2306.07075, 2023.
[47]
S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and
D. Schuurmans, “Foundation models for decision
making: Problems, methods, and opportunities,” arXiv
preprint arXiv:2303.04129, 2023.
[48]
G. Paaß and S. Giesselbach, Foundation Models for
Natural Language Processing: Pre-trained Language Models
Integrating Media.
Springer Nature, 2023.
[49]
Y. Zhao, I. Misra, P. Kr¨
ahenb¨
uhl, and R. Girdhar,
“Learning video representations from large language
models,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2023, pp. 6586–
6597.
[50]
H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic,
W. Wang, and M. D. Plumbley, “Audioldm: Text-
to-audio generation with latent diffusion models,”
in Proceedings of the 40th International Conference on
Machine Learning, 2023, pp. 21 450–21 474.
[51]
Y. Fathullah, C. Wu, E. Lakomkin, J. Jia, Y. Shangguan,
K. Li, J. Guo, W. Xiong, J. Mahadeokar, O. Kalinli
et al., “Prompting large language models with speech
recognition abilities,” arXiv preprint arXiv:2307.11795,
2023.
[52]
G. Li, X. Xu, L. Dai, M. Wu, and K. Yu, “Diverse
and vivid sound generation from text descriptions,”
in ICASSP 2023-2023 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE,
2023, pp. 1–5.
[53]
G. Deshpande, A. Batliner, and B. W. Schuller, “Ai-
based human audio processing for covid-19: A com-
24
prehensive overview,” Pattern recognition, vol. 122, p.
108289, 2022.
[54]
S. Liu, A. Mallol-Ragolta, E. Parada-Cabaleiro, K. Qian,
X. Jing, A. Kathan, B. Hu, and B. W. Schuller, “Audio
self-supervised learning: A survey,” Patterns, vol. 3,
no. 12, 2022.
[55]
J.-P. Briot, G. Hadjeres, and F.-D. Pachet, “Deep learn-
ing techniques for music generation–a survey,” arXiv
preprint arXiv:1709.01620, 2017.
[56]
S. Ji, J. Luo, and X. Yang, “A comprehensive survey
on deep music generation: Multi-level representations,
algorithms, evaluations, and future directions,” arXiv
preprint arXiv:2011.06801, 2020.
[57]
L. Moysis, L. A. Iliadis, S. P. Sotiroudis, A. D. Boursia-
nis, M. S. Papadopoulou, K.-I. D. Kokkinidis, C. Volos,
P. Sarigiannidis, S. Nikolaidis, and S. K. Goudos,
“Music deep learning: Deep learning methods for music
signal processing-a review of the state-of-the-art,” IEEE
Access, 2023.
[58]
S. Chachada and C.-C. J. Kuo, “Environmental sound
recognition: A survey,” APSIPA Transactions on Signal
and Information Processing, vol. 3, p. e14, 2014.
[59]
K. Miyazaki, T. Toda, T. Hayashi, and K. Takeda,
“Environmental sound processing and its applications,”
IEEJ Transactions on Electrical and Electronic Engineering,
vol. 14, no. 3, pp. 340–351, 2019.
[60]
G. Mialon, R. Dess`
ı, M. Lomeli, C. Nalmpantis, R. Pa-
sunuru, R. Raileanu, B. Rozi`
ere, T. Schick, J. Dwivedi-
Yu, A. Celikyilmaz et al., “Augmented language mod-
els: a survey,” arXiv preprint arXiv:2302.07842, 2023.
[61]
A. Tornede, D. Deng, T. Eimer, J. Giovanelli, A. Mohan,
T. Ruhkopf, S. Segel, D. Theodorakopoulos, T. Tornede,
H. Wachsmuth et al., “Automl in the age of large lan-
guage models: Current challenges, future opportunities
and risks,” arXiv preprint arXiv:2306.08107, 2023.
[62]
R. Tang, Y.-N. Chuang, and X. Hu, “The science
of detecting llm-generated texts,” arXiv preprint
arXiv:2303.07205, 2023.
[63]
Y. K. Dwivedi, N. Kshetri, L. Hughes, E. L. Slade,
A. Jeyaraj, A. K. Kar, A. M. Baabdullah, A. Koohang,
V. Raghavan, M. Ahuja et al., ““so what if chatgpt wrote
it?” multidisciplinary perspectives on opportunities,
challenges and implications of generative conversa-
tional ai for research, practice and policy,” International
Journal of Information Management, vol. 71, p. 102642,
2023.
[64]
F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn,
“A systematic evaluation of large language models
of code,” in Proceedings of the 6th ACM SIGPLAN
International Symposium on Machine Programming, 2022,
pp. 1–10.
[65]
Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and
S. C. Hoi, “Codet5+: Open code large language models
for code understanding and generation,” arXiv preprint
arXiv:2305.07922, 2023.
[66]
A. Trozze, T. Davies, and B. Kleinberg, “Large language
models in cryptocurrency securities cases: Can chatgpt
replace lawyers?” arXiv preprint arXiv:2308.06032, 2023.
[67]
Y. Qin, S. Hu, Y. Lin, W. Chen, N. Ding, G. Cui, Z. Zeng,
Y. Huang, C. Xiao, C. Han et al., “Tool learning with
foundation models,” arXiv preprint arXiv:2304.08354,
2023.
[68]
I. Schubert, J. Zhang, J. Bruce, S. Bechtle, E. Parisotto,
M. Riedmiller, J. T. Springenberg, A. Byravan, L. Hasen-
clever, and N. Heess, “A generalist dynamics model
for control,” arXiv preprint arXiv:2305.10912, 2023.
[69]
O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng,
G. Penn, and D. Yu, “Convolutional neural networks
for speech recognition,” IEEE/ACM Transactions on
audio, speech, and language processing, vol. 22, no. 10,
pp. 1533–1545, 2014.
[70]
I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to
sequence learning with neural networks,” Advances in
neural information processing systems, vol. 27, 2014.
[71]
S. Hochreiter and J. Schmidhuber, “Long short-term
memory,” Neural computation, vol. 9, no. 8, pp. 1735–
1780, 1997.
[72]
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,
“Attention is all you need,” Advances in neural informa-
tion processing systems, vol. 30, 2017.
[73]
Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Effi-
cient transformers: A survey,” ACM Computing Surveys,
vol. 55, no. 6, pp. 1–28, 2022.
[74]
Y. Yu, X. Si, C. Hu, and J. Zhang, “A review of recurrent
neural networks: Lstm cells and network architectures,”
Neural computation, vol. 31, no. 7, pp. 1235–1270, 2019.
[75]
H. Salehinejad, S. Sankar, J. Barfett, E. Colak, and
S. Valaee, “Recent advances in recurrent neural net-
works,” arXiv preprint arXiv:1801.01078, 2017.
[76]
S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan,
and M. Shah, “Transformers in vision: A survey,” ACM
computing surveys (CSUR), vol. 54, no. 10s, pp. 1–41,
2022.
[77]
K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu,
Y. Tang, A. Xiao, C. Xu, Y. Xu et al., “A survey on vision
transformer,” IEEE transactions on pattern analysis and
machine intelligence, vol. 45, no. 1, pp. 87–110, 2022.
[78]
F. Shamshad, S. Khan, S. W. Zamir, M. H. Khan,
M. Hayat, F. S. Khan, and H. Fu, “Transformers in
medical imaging: A survey,” Medical Image Analysis, p.
102802, 2023.
[79]
D. Lu, Q. Xie, M. Wei, K. Gao, L. Xu, and J. Li,
“Transformers in 3d point clouds: A survey,” arXiv
preprint arXiv:2205.07417, 2022.
[80]
J. Selva, A. S. Johansen, S. Escalera, K. Nasrollahi,
T. B. Moeslund, and A. Clap´
es, “Video transformers:
A survey,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2023.
[81]
A. A. Aleissaee, A. Kumar, R. M. Anwer, S. Khan,
H. Cholakkal, G.-S. Xia, and F. S. Khan, “Transformers
in remote sensing: A survey,” Remote Sensing, vol. 15,
no. 7, p. 1860, 2023.
[82]
Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan,
and L. Sun, “Transformers in time series: A survey,”
arXiv preprint arXiv:2202.07125, 2022.
[83]
S. Schneider, A. Baevski, R. Collobert, and M. Auli,
“wav2vec: Unsupervised pre-training for speech recog-
nition,” arXiv preprint arXiv:1904.05862, 2019.
[84]
A.
Radford,
J.
W.
Kim,
T.
Xu,
G.
Brockman,
C. McLeavey, and I. Sutskever, “Robust speech recogni-
tion via large-scale weak supervision,” in International
25
Conference on Machine Learning.
PMLR, 2023, pp.
28 492–28 518.
[85]
A. Ła´
ncucki, “Fastpitch: Parallel text-to-speech with
pitch prediction,” in ICASSP 2021-2021 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Process-
ing (ICASSP).
IEEE, 2021, pp. 6588–6592.
[86]
M. Zeng, X. Tan, R. Wang, Z. Ju, T. Qin, and T.-Y. Liu,
“Musicbert: Symbolic music understanding with large-
scale pre-training,” in Findings of the Association for
Computational Linguistics: ACL-IJCNLP 2021, 2021, pp.
791–800.
[87]
C. Hernandez-Olivan and J. R. Beltran, “Music com-
position with deep learning: A review,” Advances in
speech and music technology: computational aspects and
applications, pp. 25–50, 2022.
[88]
Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii,
Y. J. Bang, A. Madotto, and P. Fung, “Survey of
hallucination in natural language generation,” ACM
Computing Surveys, vol. 55, no. 12, pp. 1–38, 2023.
[89]
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown,
B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and
D. Amodei, “Scaling laws for neural language models,”
arXiv preprint arXiv:2001.08361, 2020.
[90]
OpenAI,
“Gpt-4
technical
report,”
https://arxiv.org/pdf/2303.08774.pdf, 2023.
[91]
D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou,
and X. Qiu, “Speechgpt: Empowering large language
models with intrinsic cross-modal conversational abili-
ties,” arXiv preprint arXiv:2305.11000, 2023.
[92]
W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia,
R. Salakhutdinov, and A. Mohamed, “Hubert: Self-
supervised speech representation learning by masked
prediction of hidden units,” IEEE/ACM Transactions
on Audio, Speech, and Language Processing, vol. 29, pp.
3451–3460, 2021.
[93]
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.
Lachaux, T. Lacroix, B. Rozi`
ere, N. Goyal, E. Hambro,
F. Azhar et al., “Llama: Open and efficient foundation
language models,” arXiv preprint arXiv:2302.13971,
2023.
[94]
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li,
S. Wang, L. Wang, and W. Chen, “Lora: Low-rank
adaptation of large language models,” arXiv preprint
arXiv:2106.09685, 2021.
[95]
P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen,
A. Bapna, Z. Borsos, F. d. C. Quitry, P. Chen, D. E.
Badawy, W. Han, E. Kharitonov et al., “Audiopalm: A
large language model that can speak and listen,” arXiv
preprint arXiv:2306.12925, 2023.
[96]
A. Chowdhery, S. Narang, J. Devlin, M. Bosma,
G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sut-
ton, S. Gehrmann et al., “Palm: Scaling language mod-
eling with pathways,” arXiv preprint arXiv:2204.02311,
2022.
[97]
R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-
ikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,
Z. Chen et al., “Palm 2 technical report,” arXiv preprint
arXiv:2305.10403, 2023.
[98]
K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi,
A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski,
A. Mohamed et al., “On generative spoken language
modeling from raw audio,” Transactions of the Associa-
tion for Computational Linguistics, vol. 9, pp. 1336–1354,
2021.
[99]
Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov,
O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grang-
ier, M. Tagliasacchi, and N. Zeghidour, “Audiolm: A
language modeling approach to audio generation,”
IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 31, pp. 2523–2533, 2023.
[100] Z. Borsos, M. Sharifi, D. Vincent, E. Kharitonov,
N. Zeghidour, and M. Tagliasacchi, “Soundstorm:
Efficient parallel audio generation,” arXiv preprint
arXiv:2305.09636, 2023.
[101] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and
M. Tagliasacchi, “Soundstream: An end-to-end neural
audio codec,” IEEE/ACM Transactions on Audio, Speech,
and Language Processing, vol. 30, pp. 495–507, 2021.
[102] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin,
R. Pang, and Y. Wu, “W2v-bert: Combining contrastive
learning and masked language modeling for self-
supervised speech pre-training,” in 2021 IEEE Auto-
matic Speech Recognition and Understanding Workshop
(ASRU).
IEEE, 2021, pp. 244–250.
[103] J. Kahn, M. Rivi`
ere, W. Zheng, E. Kharitonov, Q. Xu,
P.-E. Mazar´
e, J. Karadayi, V. Liptchinsky, R. Collobert,
C. Fuegen et al., “Libri-light: A benchmark for asr with
limited or no supervision,” in ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP).
IEEE, 2020, pp. 7669–7673.
[104] F.
Kreuk,
G.
Synnaeve,
A.
Polyak,
U.
Singer,
A. D´
efossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi,
“Audiogen: Textually guided audio generation,” arXiv
preprint arXiv:2209.15352, 2022.
[105] Y. Li, M. Tagliasacchi, O. Rybakov, V. Ungureanu, and
D. Roblek, “Real-time speech frequency bandwidth
extension,” in ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP).
IEEE, 2021, pp. 691–695.
[106] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou,
and D. Yu, “Diffsound: Discrete diffusion model for
text-to-sound generation,” IEEE/ACM Transactions on
Audio, Speech, and Language Processing, 2023.
[107] H. Liu, Q. Tian, Y. Yuan, X. Liu, X. Mei, Q. Kong,
Y. Wang, W. Wang, Y. Wang, and M. D. Plumb-
ley, “Audioldm 2: Learning holistic audio genera-
tion with self-supervised pretraining,” arXiv preprint
arXiv:2308.05734, 2023.
[108] Y. Gong, H. Luo, A. H. Liu, L. Karlinsky, and
J. Glass, “Listen, think, and understand,” arXiv preprint
arXiv:2305.10790, 2023.
[109] Y. Gong, Y.-A. Chung, and J. Glass, “Ast: Audio spec-
trogram transformer,” arXiv preprint arXiv:2104.01778,
2021.
[110] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,
L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez et al.,
“Vicuna: An open-source chatbot impressing gpt-4 with
90%* chatgpt quality,” See https://vicuna. lmsys. org
(accessed 14 April 2023), 2023.
[111] Y. Gong, A. Rouditchenko, A. H. Liu, D. Harwath,
L. Karlinsky, H. Kuehne, and J. R. Glass, “Contrastive
audio-visual masked autoencoder,” in The Eleventh In-
26
ternational Conference on Learning Representations, 2022.
[112] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen,
W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter,
“Audio set: An ontology and human-labeled dataset for
audio events,” in 2017 IEEE international conference on
acoustics, speech and signal processing (ICASSP).
IEEE,
2017, pp. 776–780.
[113] I. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and
Y. Bengio, “An empirical investigation of catastrophic
forgetting in gradient-based neural networks,” arXiv
preprint arXiv:1312.6211, 2013.
[114] T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur,
Z. Chen, J. Li, and F. Wei, “Viola: Unified codec
language models for speech recognition, synthesis, and
translation,” arXiv preprint arXiv:2305.16107, 2023.
[115] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu,
Z. Chen, Y. Liu, H. Wang, J. Li et al., “Neural codec
language models are zero-shot text to speech synthe-
sizers,” arXiv preprint arXiv:2301.02111, 2023.
[116] Z. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu,
Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and
F. Wei, “Speak foreign languages with your own voice:
Cross-lingual neural codec language modeling,” 2023.
[Online]. Available: https://arxiv.org/abs/2303.03926
[117] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant,
G. Synnaeve, Y. Adi, and A. D´
efossez, “Simple
and controllable music generation,” arXiv preprint
arXiv:2306.05284, 2023.
[118] A. D´
efossez, J. Copet, G. Synnaeve, and Y. Adi, “High
fidelity neural audio compression,” arXiv preprint
arXiv:2210.13438, 2022.
[119] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel,
M.
Verzetti,
A.
Caillon,
Q.
Huang,
A.
Jansen,
A. Roberts, M. Tagliasacchi et al., “Musiclm: Generating
music from text,” arXiv preprint arXiv:2301.11325, 2023.
[120] X. Wang, M. Thakker, Z. Chen, N. Kanda, S. Emre Es-
kimez, S. Chen, M. Tang, S. Liu, J. Li, and T. Yoshioka,
“Speechx: Neural codec language model as a versatile
speech transformer,” https://arxiv.org/pdf/2308.06873.pdf,
2023.
[121] Y. Cheng, Y. Zhang, M. Johnson, W. Macherey, and
A. Bapna, “Mu2 slam: Multitask, multilingual speech
and language models,” in International Conference on
Machine Learning.
PMLR, 2023, pp. 5504–5520.
[122] R. Huang, M. Li, D. Yang, J. Shi, X. Chang, Z. Ye,
Y. Wu, Z. Hong, J. Huang, J. Liu et al., “Audiogpt:
Understanding and generating speech, music, sound,
and talking head,” arXiv preprint arXiv:2304.12995,
2023.
[123] S. Deshmukh, B. Elizalde, R. Singh, and H. Wang,
“Pengi: An audio language model for audio tasks,”
arXiv preprint arXiv:2305.11834, 2023.
[124] C. e. a. Seamless Communication, Lo¨
ıc Barrault,
“”seamlessm4t—massively multilingual & multimodal
machine translation”,” arXiv preprint arXiv:2308.11596,
2023.
[125] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt:
Any-to-any multimodal llm,” 2023.
[126] Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and
D. P. Ellis, “Mulan: A joint embedding of music audio
and natural language,” arXiv preprint arXiv:2208.12415,
2022.
[127] X. Liu, Z. Zhu, H. Liu, Y. Yuan, M. Cui, Q. Huang,
J. Liang, Y. Cao, Q. Kong, M. D. Plumbley et al.,
“Wavjourney: Compositional audio creation with large
language models,” arXiv preprint arXiv:2307.14335,
2023.
[128] H. Inaguma, S. Popuri, I. Kulikov, P.-J. Chen, C. Wang,
Y.-A. Chung, Y. Tang, A. Lee, S. Watanabe, and J. Pino,
“Unity: Two-pass direct speech-to-speech translation
with discrete units,” arXiv preprint arXiv:2212.08055,
2022.
[129] J. Kong, J. Kim, and J. Bae, “Hifi-gan: Generative adver-
sarial networks for efficient and high fidelity speech
synthesis,” Advances in Neural Information Processing
Systems, vol. 33, pp. 17 022–17 033, 2020.
[130] R. Ardila, M. Branson, K. Davis, M. Henretty,
M.
Kohler,
J.
Meyer,
R.
Morais,
L.
Saunders,
F. M. Tyers, and G. Weber, “Common voice: A
massively-multilingual speech corpus,” arXiv preprint
arXiv:1912.06670, 2019.
[131] B. Zhang, H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie,
X. Xu, H. Bu, X. Chen, C. Zeng et al., “Wenetspeech:
A 10000+ hours multi-domain mandarin corpus for
speech recognition,” in ICASSP 2022-2022 IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing (ICASSP).
IEEE, 2022, pp. 6182–6186.
[132] G. Chen, S. Chai, G. Wang, J. Du, W.-Q. Zhang,
C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang et al.,
“Gigaspeech: An evolving, multi-domain asr corpus
with 10,000 hours of transcribed audio,” arXiv preprint
arXiv:2106.06909, 2021.
[133] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and
M. Turchi, “Must-c: a multilingual speech translation
corpus,” in Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers). Association for Computational
Linguistics, 2019, pp. 2012–2017.
[134] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar,
D. Haziza, M. Williamson, J. Pino, and E. Dupoux,
“Voxpopuli: A large-scale multilingual speech corpus
for representation learning, semi-supervised learning
and interpretation,” arXiv preprint arXiv:2101.00390,
2021.
[135] C. Wang, J. Pino, A. Wu, and J. Gu, “Covost: A
diverse multilingual speech-to-text translation corpus,”
in Proceedings of the Twelfth Language Resources and
Evaluation Conference, 2020, pp. 4197–4203.
[136] Y. Jia, M. T. Ramanovich, Q. Wang, and H. Zen, “Cvss
corpus and massively multilingual speech-to-speech
translation,” in Proceedings of the Thirteenth Language
Resources and Evaluation Conference, 2022, pp. 6691–6703.
[137] M. Wester, “The emime bilingual database,” The Uni-
versity of Edinburgh, Tech. Rep., 2010.
[138] C. D. Kim, B. Kim, H. Lee, and G. Kim, “Audiocaps:
Generating captions for audios in the wild,” in Proceed-
ings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers),
2019, pp. 119–132.
[139] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: An
27
audio captioning dataset,” in ICASSP 2020-2020 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP).
IEEE, 2020, pp. 736–740.
[140] H.-T. Hung, J. Ching, S. Doh, N. Kim, J. Nam, and Y.-
H. Yang, “Emopia: A multi-modal pop piano dataset
for emotion recognition and emotion-based music
generation,” arXiv preprint arXiv:2108.01374, 2021.
[141] J. Ens and P. Pasquier, “Building the metamidi dataset:
Linking symbolic and audio musical data.” in ISMIR,
2021, pp. 182–188.
[142] G. Meseguer-Brocal, A. Cohen-Hadria, and G. Peeters,
“Creating dali, a large dataset of synchronized audio,
lyrics, and notes,” Transactions of the International Society
for Music Information Retrieval, vol. 3, no. 1, 2020.
[143] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, “Vg-
gsound: A large-scale audio-visual dataset,” in ICASSP
2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP).
IEEE, 2020, pp.
721–725.
[144] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra,
“Fsd50k: an open dataset of human-labeled sound
events,” IEEE/ACM Transactions on Audio, Speech, and
Language Processing, vol. 30, pp. 829–852, 2021.
[145] J. Liu, Y. Dong, Z. Cheng, X. Zhang, X. Li, F. Yu,
and M. Sun, “Symphony generation with permu-
tation invariant language model,” arXiv preprint
arXiv:2205.05448, 2022.
[146] D. Bogdanov, M. Won, P. Tovstogan, A. Porter, and
X. Serra, “The mtg-jamendo dataset for automatic
music tagging.”
ICML, 2019.
[147] Z. Wang, K. Chen, J. Jiang, Y. Zhang, M. Xu, S. Dai,
X. Gu, and G. Xia, “Pop909: A pop-song dataset
for music arrangement generation,” arXiv preprint
arXiv:2008.07142, 2020.
[148] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bres-
son, “Fma: A dataset for music analysis,” arXiv preprint
arXiv:1612.01840, 2016.
[149] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang,
Y. Choi, and J. Gao, “Vinvl: Revisiting visual represen-
tations in vision-language models,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, 2021, pp. 5579–5588.
[150] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Conditional
prompt learning for vision-language models,” in Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2022, pp. 16 816–16 825.
[151] K. Deng, Z. Yang, S. Watanabe, Y. Higuchi, G. Cheng,
and P. Zhang, “Improving non-autoregressive end-
to-end speech recognition with pre-trained acoustic
and language models,” in ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP).
IEEE, 2022, pp. 8522–8526.
[152] Y. Higuchi, B. Yan, S. Arora, T. Ogawa, T. Kobayashi,
and S. Watanabe, “Bert meets ctc: New formulation
of end-to-end speech recognition with pre-trained
masked language model,” in Findings of the Association
for Computational Linguistics: EMNLP 2022, 2022, pp.
5486–5503.
[153] Y. Higuchi, T. Ogawa, T. Kobayashi, and S. Watan-
abe, “Bectra: Transducer-based end-to-end asr with
bert-enhanced encoder,” in ICASSP 2023-2023 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP).
IEEE, 2023, pp. 1–5.
[154] J. Wu, Y. Gaur, Z. Chen, L. Zhou, Y. Zhu, T. Wang,
J. Li, S. Liu, B. Ren, L. Liu et al., “On decoder-only
architecture for speech-to-text and large language
model integration,” arXiv preprint arXiv:2307.03917,
2023.
[155] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou,
W. Zhang, P. Lu, C. He, X. Yue et al., “Llama-adapter
v2: Parameter-efficient visual instruction model,” arXiv
preprint arXiv:2304.15010, 2023.
[156] Y. Kubo, S. Karita, and M. Bacchiani, “Knowledge
transfer from large-scale pretrained language models
to end-to-end speech recognizers,” in ICASSP 2022-
2022 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP).
IEEE, 2022, pp. 8512–
8516.
[157] S. Ling, Y. Hu, S. Qian, G. Ye, Y. Qian, Y. Gong,
E. Lin, and M. Zeng, “Adapting large language model
with speech for fully formatted end-to-end speech
recognition,” arXiv preprint arXiv:2307.08234, 2023.
[158] P. He, B. Peng, L. Lu, S. Wang, J. Mei, Y. Liu, R. Xu,
H. H. Awadalla, Y. Shi, C. Zhu et al., “Z-code++: A
pre-trained language model optimized for abstractive
summarization,” arXiv preprint arXiv:2208.09770, 2022.
[159] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
I. Sutskever et al., “Language models are unsupervised
multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9,
2019.
[160] W. R. Huang, H. Zhang, S. Kumar, S.-y. Chang, and
T. N. Sainath, “Semantic segmentation with bidirec-
tional language models improves long-form asr,” arXiv
preprint arXiv:2305.18419, 2023.
[161] W. R. Huang, C. Peyser, T. N. Sainath, R. Pang,
T. Strohman, and S. Kumar, “Sentence-select: Large-
scale language model data selection for rare-word
speech recognition,” arXiv preprint arXiv:2203.05008,
2022.
[162] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Col-
lobert, “Mls: A large-scale multilingual dataset for
speech research,” arXiv preprint arXiv:2012.03411, 2020.
[163] L. Zhuo, R. Yuan, J. Pan, Y. Ma, Y. LI, G. Zhang,
S. Liu, R. Dannenberg, J. Fu, C. Lin et al., “Lyricwhiz:
Robust multilingual zero-shot lyrics transcription by
whispering to chatgpt,” arXiv preprint arXiv:2306.17103,
2023.
[164] D. Stoller, S. Durand, and S. Ewert, “End-to-end lyrics
alignment for polyphonic music using an audio-to-
character recognition model,” in ICASSP 2019-2019
IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP).
IEEE, 2019, pp. 181–185.
[165] J. K. Hansen and I. Fraunhofer, “Recognition of
phonemes in a-cappella recordings using temporal
patterns and mel frequency cepstral coefficients,” in
9th Sound and Music Computing Conference (SMC), 2012,
pp. 494–499.
[166] K. Schulze-Forster, C. S. Doire, G. Richard, and
R. Badeau, “Phoneme level lyrics alignment and
text-informed singing voice separation,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing,
vol. 29, pp. 2382–2395, 2021.
28
[167] G. R. Dabike and J. Barker, “Automatic lyric tran-
scription from karaoke vocal tracks: Resources and
a baseline system.” in Interspeech, 2019, pp. 579–583.
[168] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss,
N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al.,
“Tacotron: Towards end-to-end speech synthesis,” Proc.
Interspeech 2017, pp. 4006–4010, 2017.
[169] S. ¨
O. Arık, M. Chrzanowski, A. Coates, G. Diamos,
A. Gibiansky, Y. Kang, X. Li, J. Miller, A. Ng, J. Raiman
et al., “Deep voice: Real-time neural text-to-speech,” in
International Conference on Machine Learning.
PMLR,
2017, pp. 195–204.
[170] W. Ping, K. Peng, and J. Chen, “Clarinet: Parallel wave
generation in end-to-end text-to-speech,” in Interna-
tional Conference on Learning Representations, 2018.
[171] D. Griffin and J. Lim, “Signal estimation from modified
short-time Fourier transform,” IEEE Transactions on
acoustics, speech, and signal processing, vol. 32, no. 2, pp.
236–243, 1984.
[172] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan,
O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and
K. Kavukcuoglu, “Wavenet: A generative model for
raw audio,” https://arxiv.org/abs/1609.03499, 2016.
[173] R. Prenger, R. Valle, and B. Catanzaro, “Waveglow: A
flow-based generative network for speech synthesis,”
in ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE,
2019, pp. 3617–3621.
[174] S. Kakouros, J. ˇ
Simko, M. Vainio, and A. Suni, “Inves-
tigating the utility of surprisal from large language
models for speech synthesis prosody,” arXiv preprint
arXiv:2306.09814, 2023.
[175] M. Hassid, T. Remez, T. A. Nguyen, I. Gat, A. Con-
neau, F. Kreuk, J. Copet, A. Defossez, G. Synnaeve,
E. Dupoux et al., “Textually pretrained speech language
models,” arXiv preprint arXiv:2305.13009, 2023.
[176] E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier,
S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi,
and N. Zeghidour, “Speak, read and prompt: High-
fidelity text-to-speech with minimal supervision,”
arXiv preprint arXiv:2302.03540, 2023.
[177] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer,
“Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and compre-
hension,” in Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, 2020, pp.
7871–7880.
[178] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,
M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the
limits of transfer learning with a unified text-to-text
transformer,” The Journal of Machine Learning Research,
vol. 21, no. 1, pp. 5485–5551, 2020.
[179] R. Sennrich, B. Haddow, and A. Birch, “Improving
neural machine translation models with monolingual
data,” in 54th Annual Meeting of the Association for Com-
putational Linguistics.
Association for Computational
Linguistics (ACL), 2016, pp. 86–96.
[180] S. Maiti, Y. Peng, T. Saeki, and S. Watanabe, “Speechlm-
score: Evaluating speech generation using speech lan-
guage model,” in ICASSP 2023-2023 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP).
IEEE, 2023, pp. 1–5.
[181] W.-C. Huang, E. Cooper, Y. Tsao, H.-M. Wang, T. Toda,
and J. Yamagishi, “The voicemos challenge 2022,” arXiv
preprint arXiv:2203.11389, 2022.
[182] Z. Wang, Y. Chen, L. Xie, Q. Tian, and Y. Wang,
“Lm-vc: Zero-shot voice conversion via speech gen-
eration based on language models,” arXiv preprint
arXiv:2306.10521, 2023.
[183] Z. Wang, S. Mao, W. Wu, Y. Xia, Y. Deng, and J. Tien,
“Assessing phrase break of esl speech with pre-trained
language models and large language models,” arXiv
preprint arXiv:2306.04980, 2023.
[184] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior,
E. G¨
olge, and M. A. Ponti, “Yourtts: Towards zero-
shot multi-speaker tts and zero-shot voice conversion
for everyone,” in International Conference on Machine
Learning.
PMLR, 2022, pp. 2709–2720.
[185] E. Matusov, S. Kanthak, and H. Ney, “On the inte-
gration of speech recognition and statistical machine
translation,” in Ninth European Conference on Speech
Communication and Technology, 2005.
[186] A. B´
erard, O. Pietquin, L. Besacier, and C. Servan,
“Listen and translate: A proof of concept for end-to-
end speech-to-text translation,” in NIPS Workshop on
end-to-end learning for speech and audio processing, 2016.
[187] Q. Dong, Z. Huang, C. Xu, Y. Zhao, K. Wang, X. Cheng,
T. Ko, Q. Tian, T. Li, F. Yue et al., “Polyvoice: Language
models for speech to speech translation,” arXiv preprint
arXiv:2306.02982, 2023.
[188] M. J. Gales, K. M. Knill, and A. Ragni, “Low-resource
speech recognition and keyword-spotting,” in Speech
and Computer: 19th International Conference, SPECOM
2017, Hatfield, UK, September 12-16, 2017, Proceedings 19.
Springer, 2017, pp. 3–19.
[189] C. Le, Y. Qian, L. Zhou, S. Liu, M. Zeng, and
X. Huang, “Comsl: A composite speech-language
model for end-to-end speech-to-text translation,” arXiv
preprint arXiv:2305.14838, 2023.
[190] H. Wu, K.-W. Chang, Y.-K. Wu, and H.-y. Lee,
“Speechgen: Unlocking the generative power of
speech language models with prompts,” arXiv preprint
arXiv:2306.02207, 2023.
[191] V. W. Zue and J. R. Glass, “Conversational interfaces:
Advances and challenges,” Proceedings of the IEEE,
vol. 88, no. 8, pp. 1166–1180, 2000.
[192] V. Vlasov, J. E. Mosig, and A. Nichol, “Dialogue
transformers,” arXiv preprint arXiv:1910.00486, 2019.
[193] Y. Wang, S. Joty, M. R. Lyu, I. King, C. Xiong, and S. C.
Hoi, “Vd-bert: A unified vision and dialog transformer
with bert,” arXiv preprint arXiv:2004.13278, 2020.
[194] R. Zhang, T. Wu, X. Chen, S. Wen, S. Nepal, C. Paris,
and Y. Xiang, “Dynalogue: A transformer-based dia-
logue system with dynamic attention,” in Proceedings
of the ACM Web Conference 2023, 2023, pp. 1604–1615.
[195] J. Gao, M. Galley, and L. Li, “Neural approaches
to conversational ai,” in The 41st international ACM
SIGIR conference on research & development in information
retrieval, 2018, pp. 1371–1374.
[196] I. V. Serban, R. Lowe, P. Henderson, L. Charlin, and
J. Pineau, “A survey of available corpora for building
29
data-driven dialogue systems: The journal version,”
Dialogue & Discourse, vol. 9, no. 1, pp. 1–49, 2018.
[197] Y. Fan and X. Luo, “A survey of dialogue system
evaluation,” in 32nd IEEE International Conference on
Tools with Artificial Intelligence ICTAI.
IEEE, 2020.
[198] Z. Hu, Y. Feng, A. T. Luu, B. Hooi, and A. Lipani,
“Unlocking the potential of user feedback: Leveraging
large language model as user simulator to enhance
dialogue system,” CoRR, vol. abs/2306.09821, 2023.
[199] V. Hudeˇ
cek and O. Duˇ
sek, “Are llms all you
need for task-oriented dialogue?” arXiv preprint
arXiv:2304.06556, 2023.
[200] J. Wei, S. Kim, H. Jung, and Y. Kim, “Leveraging large
language models to power chatbots for collecting user
self-reported data,” CoRR, vol. abs/2301.05843, 2023.
[201] T. A. Nguyen, E. Kharitonov, J. Copet, Y. Adi, W. Hsu,
A. Elkahky, P. Tomasello, R. Algayres, B. Sagot, A. Mo-
hamed, and E. Dupoux, “Generative spoken dialogue
language modeling,” CoRR, vol. abs/2203.16502, 2022.
[202] A. L´
opez-Zorrilla, M. I. Torres, and H. Cuay´
ahuitl,
“Audio embedding-aware dialogue policy learning,”
IEEE ACM Trans. Audio Speech Lang. Process. (TASLP),
vol. 3, 2023.
[203] N. Cherakara, F. Varghese, S. Shabana, N. Nelson,
A. Karukayil, R. Kulothungan, M. A. Farhan, B. Nesset,
M. Moujahid, T. Dinkar et al., “Furchat: An embodied
conversational agent using llms, combining open and
closed-domain dialogue with facial expressions,” arXiv
preprint arXiv:2308.15214, 2023.
[204] T. A. Nguyen, E. Kharitonov, J. Copet, Y. Adi, W.-N.
Hsu, A. Elkahky, P. Tomasello, R. Algayres, B. Sagot,
A. Mohamed, and E. Dupoux, “Generative Spoken
Dialogue Language Modeling,” Transactions of the
Association for Computational Linguistics, vol. 11, 2023.
[Online]. Available: https://doi.org/10.1162/tacl a
00545
[205] X. Liu, Q. Wu, H. Zhou, Y. Du, W. Wu, D. Lin,
and Z. Liu, “Audio-driven co-speech gesture video
generation,” in NeurIPS, 2022.
[206] T. Gong, C. Lyu, S. Zhang, Y. Wang, M. Zheng, Q. Zhao,
K. Liu, W. Zhang, P. Luo, and K. Chen, “Multimodal-
gpt: A vision and language model for dialogue with
humans,” 2023.
[207] C. Li, “Large multimodal models: Notes on cvpr 2023
tutorial,” 2023.
[208] Z. Li, Z. Li, J. Zhang, Y. Feng, and J. Zhou, “Bridging
text and video: A universal multimodal transformer
for audio-visual scene-aware dialog,” IEEE ACM Trans.
Audio Speech Lang. Process. (TSLP), vol. 29, 2021.
[209] J.-P. Briot and F. Pachet, “Deep learning for music gen-
eration: challenges and directions,” Neural Computing
and Applications, vol. 32, no. 4, pp. 981–993, 2020.
[210] B. Yu, P. Lu, R. Wang, W. Hu, X. Tan, W. Ye, S. Zhang,
T. Qin, and T.-Y. Liu, “Museformer: Transformer with
fine-and coarse-grained attention for music genera-
tion,” Advances in Neural Information Processing Systems,
vol. 35, pp. 1376–1388, 2022.
[211] A. Kumar and P. Sarmento, “From words to music: A
study of subword tokenization techniques in symbolic
music generation,” arXiv preprint arXiv:2304.08953,
2023.
[212] S. Ji and X. Yang, “Emomusictv: Emotion-conditioned
symbolic music generation with hierarchical trans-
former vae,” IEEE Transactions on Multimedia, 2023.
[213] H. F. Garcia, P. Seetharaman, R. Kumar, and B. Pardo,
“Vampnet: Music generation via masked acoustic token
modeling,” arXiv preprint arXiv:2307.04686, 2023.
[214] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Free-
man, “Maskgit: Masked generative image transformer,”
in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2022, pp. 11 315–11 325.
[215] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and
K. Kumar, “High-fidelity audio compression with
improved rvqgan,” arXiv preprint arXiv:2306.06546,
2023.
[216] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention
with relative position representations,” arXiv preprint
arXiv:1803.02155, 2018.
[217] K. Kilgour, M. Zuluaga, D. Roblek, and M. Shar-
ifi, “Fr\’echet audio distance: A metric for evaluat-
ing music enhancement algorithms,” arXiv preprint
arXiv:1812.08466, 2018.
[218] D. Ghosal, N. Majumder, A. Mehrish, and S. Po-
ria, “Text-to-audio generation using instruction-tuned
llm and latent diffusion model,” arXiv preprint
arXiv:2304.13731, 2023.
[219] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay,
W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma
et al., “Scaling instruction-finetuned language models,”
arXiv preprint arXiv:2210.11416, 2022.
[220] D. P. Kingma and M. Welling, “Auto-encoding varia-
tional Bayes,” arXiv preprint arXiv:1312.6114, 2013.
[221] K. Chen, Y. Wu, H. Liu, M. Nezhurina, T. Berg-
Kirkpatrick, and S. Dubnov, “Musicldm: Enhanc-
ing
novelty
in
text-to-music
generation
using
beat-synchronous mixup strategies,” arXiv preprint
arXiv:2308.01546, 2023.
[222] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and
B. Ommer, “High-resolution image synthesis with
latent diffusion models,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2022,
pp. 10 684–10 695.
[223] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick,
and S. Dubnov, “Large-scale contrastive language-
audio pretraining with feature fusion and keyword-
to-caption augmentation,” in ICASSP 2023-2023 IEEE
International Conference on Acoustics, Speech and Signal
Processing (ICASSP).
IEEE, 2023, pp. 1–5.
[224] S. Wu and M. Sun, “Exploring the efficacy of pre-
trained checkpoints in text-to-music generation task,”
arXiv preprint arXiv:2211.11216, 2022.
[225] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,
“BERT: Pre-training of deep bidirectional transform-
ers for language understanding,” arXiv preprint
arXiv:1810.04805, 2018.
[226] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever
et al., “Improving language understanding by genera-
tive pre-training,” 2018.
[227] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer,
“Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and compre-
30
hension,” arXiv preprint arXiv:1910.13461, 2019.
[228] Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly,
N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank et al.,
“Noise2music: Text-conditioned music generation with
diffusion models,” arXiv preprint arXiv:2302.03917,
2023.
[229] C. Donahue, A. Caillon, A. Roberts, E. Manilow, P. Es-
ling, A. Agostinelli, M. Verzetti, I. Simon, O. Pietquin,
N. Zeghidour et al., “Singsong: Generating musi-
cal accompaniments from singing,” arXiv preprint
arXiv:2301.12662, 2023.
[230] L. Ou, X. Ma, and Y. Wang, “Loaf-m2l: Joint learning of
wording and formatting for singable melody-to-lyric
generation,” arXiv preprint arXiv:2307.02146, 2023.
[231] W.
G.
Tech,
“Singability
dataset,”
https:
//www.kaggle.com/datasets/mateibejan/
multilingual-lyrics-for-genre-classification,
2021,
accessed: 2022-01-28.
[232] M. W. Lam, Q. Tian, T. Li, Z. Yin, S. Feng, M. Tu, Y. Ji,
R. Xia, M. Ma, X. Song et al., “Efficient neural music
generation,” arXiv preprint arXiv:2305.15719, 2023.
[233] P. Lu, X. Xu, C. Kang, B. Yu, C. Xing, X. Tan, and
J. Bian, “Musecoco: Generating symbolic music from
text,” arXiv preprint arXiv:2306.00110, 2023.
[234] Y.-S. Huang and Y.-H. Yang, “Pop music transformer:
Beat-based modeling and generation of expressive pop
piano compositions,” in Proceedings of the 28th ACM
international conference on multimedia, 2020, pp. 1180–
1188.
[235] A.
Katharopoulos,
A.
Vyas,
N.
Pappas,
and
F. Fleuret, “Transformers are RNNs: Fast autoregressive
transformers with linear attention,” in Proceedings of
the 37th International Conference on Machine Learning,
ser. Proceedings of Machine Learning Research, H. D.
III and A. Singh, Eds., vol. 119.
PMLR, 13–18 Jul
2020, pp. 5156–5165. [Online]. Available: https://
proceedings.mlr.press/v119/katharopoulos20a.html
[236] S. Xu, Y. Tang, and F. Zheng, “Launchpadgpt: Lan-
guage model as music visualization designer on
launchpad,” arXiv preprint arXiv:2307.04827, 2023.
[237] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke,
A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A.
Saurous, B. Seybold et al., “Cnn architectures for large-
scale audio classification,” in 2017 ieee international con-
ference on acoustics, speech and signal processing (icassp).
IEEE, 2017, pp. 131–135.
[238] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D.
Plumbley, “Panns: Large-scale pretrained audio neural
networks for audio pattern recognition,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing,
vol. 28, pp. 2880–2894, 2020.
[239] S. Forsgren and H. Martiros, “Riffusion-stable dif-
fusion for real-time music generation, 2022,” URL
https://riffusion. com/about, vol. 6.
[240] MubertAI, “Mubert: A simple notebook demon-
strating
prompt-based
music
generation.,
2022,”
https://github.com/MubertAI/Mubert-Text-to-Music.
[241] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford,
and I. Sutskever, “Jukebox: A generative model for
music,” arXiv preprint arXiv:2005.00341, 2020.
[242] Y.-J. Shih, H.-F. Wang, H.-J. Chang, L. Berry, H.-y. Lee,
and D. Harwath, “Speechclip: Integrating speech with
pre-trained vision and language model,” in 2022 IEEE
Spoken Language Technology Workshop (SLT). IEEE, 2023,
pp. 715–722.
[243] K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and
S. Dubnov, “Hts-at: A hierarchical token-semantic au-
dio transformer for sound classification and detection,”
in ICASSP 2022-2022 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE,
2022, pp. 646–650.
[244] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark
et al., “Learning transferable visual models from natu-
ral language supervision,” in International conference on
machine learning.
PMLR, 2021, pp. 8748–8763.
[245] S. Chen, Y. Wu, C. Wang, S. Liu, D. Tompkins, Z. Chen,
and F. Wei, “Beats: Audio pre-training with acoustic
tokenizers,” arXiv preprint arXiv:2212.09058, 2022.
[246] L. Xu, L. Wang, S. Bi, H. Liu, and J. Wang, “Semi-
supervised sound event detection with pre-trained
model,” in ICASSP 2023-2023 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2023, pp. 1–5.
[247] N. Turpault, R. Serizel, A. P. Shah, and J. Salamon,
“Sound event detection in domestic environments with
weakly labeled data and soundscape synthesis,” in
Workshop on Detection and Classification of Acoustic Scenes
and Events, 2019.
[248] S. Latif, M. Usama, M. I. Malik, and B. W. Schuller,
“Can large language models aid in annotating speech
emotional data? uncovering new frontiers,” arXiv
preprint arXiv:2307.06090, 2023.
[249] S.
Yu,
D.
Siwei,
C.
Guangyao,
H.
Wenhao,
Z.
Ruihua,
S.
Daochen,
X.
Qiqi,
and
Y.
Shi1,
“Llasm:
Large
language
and
speech
model,”
https://arxiv.org/pdf/2308.15930.pdf, 2023.
[250] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck,
C. Callison-Burch, and N. Carlini, “Deduplicating
training data makes language models better,” 2021.
[251] A.
Jacovi,
A.
Caciularu,
O.
Goldman,
and
Y.
Goldberg,
“Stop
uploading
test
data
in
plain text: Practical strategies for mitigating data
contamination by evaluation benchmarks,” arXiv
preprint arXiv:2305.10160, 2023. [Online]. Available:
https://arxiv.org/abs/2305.10160
[252] H. Li, D. Guo, W. Fan, M. Xu, and Y. Song, “Multi-
step jailbreaking privacy attacks on chatgpt,” arXiv
preprint arXiv:2304.05197, 2023. [Online]. Available:
https://arxiv.org/abs/2304.05197
[253] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, and
S. Zanella-B´
eguelin, “Analyzing leakage of personally
identifiable information in language models,” arXiv
preprint arXiv:2302.00539, 2023. [Online]. Available:
https://arxiv.org/abs/2302.00539
[254] A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken:
How does llm safety training fail?” arXiv preprint
arXiv:2307.02483, 2023. [Online]. Available: https:
//arxiv.org/abs/2307.02483
[255] S. Kim, S. Yun, H. Lee, M. Gubri, S. Yoon, and S. Oh,
“Propile: Probing privacy leakage in large language
models,” arXiv preprint arXiv:2307.01881, 2023. [Online].
31
Available: https://arxiv.org/abs/2307.01881
[256] C.
Patsakis
and
N.
Lykousas,
“Man
vs
the
machine: The struggle for effective text anonymisation
in
the
age
of
large
language
models,”
arXiv
preprint arXiv:2303.12429, 2023. [Online]. Available:
https://arxiv.org/abs/2303.12429
[257] N. Carlini, C. Liu, U. Erlingsson, J. Kos, and D. Song,
“The secret sharer: Evaluating and testing unintended
memorization in neural networks,” in 28th USENIX
Security Symposium (USENIX Security 19).
USENIX
Association, 2019, pp. 267–284.
[258] N. Carlini, F. Tramer, E. Wallace, M. Jagielski,
A. Herbert-Voss, K. Lee, A. Roberts, T. Brown et al.,
“Extracting training data from large language models,”
in 30th USENIX Security Symposium (USENIX Security
21).
USENIX Association, 2021.
[259] L. Shen, Y. Sun, Z. Yu, L. Ding, X. Tian, and D. Tao, “On
efficient training of large-scale deep learning models:
A literature review,” arXiv preprint arXiv:2304.03589,
2023.
[260] A. Lee, B. Miranda, and S. Koyejo, “Beyond scale: the
diversity coefficient as a data quality metric demon-
strates llms are pre-trained on formally diverse data,”
arXiv preprint arXiv:2306.13840, 2023.
[261] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts,
B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno
et al., “A pretrainer’s guide to training data: Measuring
the effects of data age, domain coverage, quality, &
toxicity,” arXiv preprint arXiv:2305.13169, 2023.
[262] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu,
P. Liang, Q. V. Le, T. Ma, and A. W. Yu, “Doremi:
Optimizing data mixtures speeds up language model
pretraining,” arXiv preprint arXiv:2305.10429, 2023.
[263] S. J. Mielke, Z. Alyafeai, E. Salesky, C. Raffel, M. Dey,
M. Gall´
e, A. Raja, C. Si, W. Y. Lee, B. Sagot et al.,
“Between words and characters: a brief history of open-
vocabulary modeling and tokenization in nlp,” arXiv
preprint arXiv:2112.10508, 2021.
[264] A. Petrov, E. La Malfa, P. H. Torr, and A. Bibi, “Lan-
guage model tokenizers introduce unfairness between
languages,” arXiv preprint arXiv:2305.15425, 2023.
[265] A. Banerjee and V. Arora, “wav2tok: Deep sequence
tokenizer for audio retrieval,” in The Eleventh Interna-
tional Conference on Learning Representations, 2022.
[266] X. Zhang, D. Zhang, S. Li, Y. Zhou, and X. Qiu, “Speech-
tokenizer: Unified speech tokenizer for speech large
language models,” arXiv preprint arXiv:2308.16692,
2023.
[267] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu,
M. Krikun, Y. Zhou, A. W. Yu, O. Firat et al., “Glam:
Efficient scaling of language models with mixture-of-
experts,” in International Conference on Machine Learning.
PMLR, 2022, pp. 5547–5569.
[268] G. Park, B. Park, S. J. Kwon, B. Kim, Y. Lee, and D. Lee,
“nuqmm: Quantized matmul for efficient inference of
large-scale generative language models,” arXiv preprint
arXiv:2206.09557, 2022.
[269] R.-J. Zhu, Q. Zhao, and J. K. Eshraghian, “Spikegpt:
Generative pre-trained language model with spiking
neural networks,” arXiv preprint arXiv:2302.13939, 2023.
[270] M. C. Rillig, M. ˚
Agerstrand, M. Bi, K. A. Gould, and
U. Sauerland, “Risks and benefits of large language
models for the environment,” Environmental Science &
Technology, vol. 57, no. 9, pp. 3464–3466, 2023.
[271] Z. Hu, Y. Lan, L. Wang, W. Xu, E.-P. Lim, R. K.-W. Lee,
L. Bing, and S. Poria, “Llm-adapters: An adapter family
for parameter-efficient fine-tuning of large language
models,” arXiv preprint arXiv:2304.01933, 2023.
[272] T. Susnjak, “Prisma-dfllm: An extension of prisma for
systematic literature reviews using domain-specific
finetuned large language models,” arXiv preprint
arXiv:2306.14905, 2023.
[273] A. Chavan, Z. Liu, D. Gupta, E. Xing, and Z. Shen,
“One-for-all: Generalized lora for parameter-efficient
fine-tuning,” arXiv preprint arXiv:2306.07967, 2023.
[274] W.-C. Huang, C.-H. Wu, S.-B. Luo, K.-Y. Chen, H.-M.
Wang, and T. Toda, “Speech recognition by simply fine-
tuning bert,” in ICASSP 2021-2021 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP).
IEEE, 2021, pp. 7343–7347.
[275] Y. Li, Y. Wu, J. Li, and S. Liu, “Prompting large
language models for zero-shot domain adaptation in
speech recognition,” arXiv preprint arXiv:2306.16007,
2023.
[276] S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Qadir, and
B. W. Schuller, “Survey of deep representation learning
for speech emotion recognition,” IEEE Transactions on
Affective Computing, 2021.
[277] J. Ainslie, T. Lei, M. de Jong, S. Onta˜
n´
on, S. Brahma,
Y. Zemlyanskiy, D. Uthus, M. Guo, J. Lee-Thorp, Y. Tay
et al., “Colt5: Faster long-range transformers with con-
ditional computation,” arXiv preprint arXiv:2303.09752,
2023.
[278] J. Ding, S. Ma, L. Dong, X. Zhang, S. Huang,
W. Wang, and F. Wei, “Longnet: Scaling transformers to
1,000,000,000 tokens,” arXiv preprint arXiv:2307.02486,
2023.
[279] J. Kaddour, O. Key, P. Nawrot, P. Minervini, and M. J.
Kusner, “No train no gain: Revisiting efficient training
algorithms for transformer-based language models,”
arXiv preprint arXiv:2307.06440, 2023.
[280] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu,
H. Michalewski, and P. Miło´
s, “Focused transformer:
Contrastive training for context scaling,” arXiv preprint
arXiv:2307.03170, 2023.
[281] A. Haviv, O. Ram, O. Press, P. Izsak, and O. Levy,
“Transformer language models without positional
encodings still learn positional information,” arXiv
preprint arXiv:2203.16634, 2022.
[282] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and
C. Zhang, “Transformers learn shortcuts to automata,”
arXiv preprint arXiv:2210.10749, 2022.
[283] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu,
“Roformer: Enhanced transformer with rotary position
embedding,” arXiv preprint arXiv:2104.09864, 2021.
[284] J. Geiping and T. Goldstein, “Cramming: Training
a language model on a single gpu in one day.” in
International Conference on Machine Learning.
PMLR,
2023, pp. 11 117–11 143.
[285] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue,
J. Wang, and F. Wei, “Retentive network: A successor to
transformer for large language models,” arXiv preprint
32
arXiv:2307.08621, 2023.
[286] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra,
and C. R´
e, “Hungry hungry hippos: Towards language
modeling with state space models,” arXiv preprint
arXiv:2212.14052, 2022.
[287] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gul-
cehre, R. Pascanu, and S. De, “Resurrecting recurrent
neural networks for long sequences,” arXiv preprint
arXiv:2303.06349, 2023.
[288] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcad-
inho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K.
GV et al., “Rwkv: Reinventing rnns for the transformer
era,” arXiv preprint arXiv:2305.13048, 2023.
[289] S. Latif, R. Rana, S. Khalifa, R. Jurdak, and B. W.
Schuller, “Deep architecture enhancing robustness to
noise, adversarial attacks, and cross-corpus setting for
speech emotion recognition,” Proc. Interspeech 2020, pp.
2327–2331, 2020.
[290] M. Cascella, J. Montomoli, V. Bellini, and E. Bignami,
“Evaluating the feasibility of chatgpt in healthcare: an
analysis of multiple clinical and research scenarios,”
Journal of Medical Systems, vol. 47, no. 1, p. 33, 2023.
[291] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze,
S. Gehrmann, P. Kambadur, D. Rosenberg, and
G. Mann, “Bloomberggpt: A large language model
for finance,” arXiv preprint arXiv:2303.17564, 2023.
[292] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chat-
law: Open-source legal large language model with
integrated external knowledge bases,” arXiv preprint
arXiv:2306.16092, 2023.
[293] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia,
E. Chi, Q. V. Le, D. Zhou et al., “Chain-of-thought
prompting elicits reasoning in large language mod-
els,” Advances in Neural Information Processing Systems,
vol. 35, pp. 24 824–24 837, 2022.
[294] V. Rawte, A. Sheth, and A. Das, “A survey of hallu-
cination in large foundation models,” arXiv preprint
arXiv:2309.05922, 2023.
[295] P. Manakul, A. Liusie, and M. J. Gales, “Selfcheckgpt:
Zero-resource black-box hallucination detection for
generative large language models,” arXiv preprint
arXiv:2303.08896, 2023.
[296] P. Feldman, J. R. Foulds, and S. Pan, “Trapping llm
hallucinations using tagged context prompts,” arXiv
preprint arXiv:2306.06085, 2023.
[297] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. John-
son, and M. Steedman, “Sources of hallucination by
large language models on inference tasks,” arXiv
preprint arXiv:2305.14552, 2023.
[298] W. Sun, Z. Shi, S. Gao, P. Ren, M. de Rijke, and
Z. Ren, “Contrastive learning reduces hallucination
in conversations,” in Proceedings of the AAAI Conference
on Artificial Intelligence, vol. 37, no. 11, 2023, pp. 13 618–
13 626.
[299] S. Latif, H. S. Ali, M. Usama, R. Rana, B. Schuller, and
J. Qadir, “Ai-based emotion recognition: Promise, peril,
and prescriptions for prosocial path,” arXiv preprint
arXiv:2211.07290, 2022.
[300] P. P. Ray, “Chatgpt: A comprehensive review on
background, applications, key challenges, bias, ethics,
limitations and future scope,” Internet of Things and
Cyber-Physical Systems, 2023.
