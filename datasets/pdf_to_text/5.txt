Adversarial Machine Learning Attack on
Modulation Classiﬁcation
Muhammad Usama∗, Muhammad Asim∗, Junaid Qadir∗, Ala Al-Fuqaha†, Muhammad Ali Imran‡
∗Information Technology University, Lahore, Punjab, Pakistan.
†Hamad Bin Khalifa University, Qatar
‡University of Glasgow, Scotland, UK
Email: ∗(muhammad.usama, msee16001, junaid.qadir)@itu.edu.pk, †aalfuqaha@hbku.edu.qa, ‡muhammad.imran@glasgow.ac.uk
Abstract—Modulation classiﬁcation is an important component
of cognitive self-driving networks. Recently many ML-based
modulation classiﬁcation methods have been proposed. We have
evaluated the robustness of 9 ML-based modulation classiﬁers
against the powerful Carlini & Wagner (C-W) attack and showed
that the current ML-based modulation classiﬁers do not provide
any deterrence against adversarial ML examples. To the best
of our knowledge, we are the ﬁrst to report the results of the
application of the C-W attack for creating adversarial examples
against various ML models for modulation classiﬁcation.
Index Terms—Adversarial machine learning, Modulation clas-
siﬁcation.
I. INTRODUCTION
The success of machine learning (ML) in computer vi-
sion and speech processing has motivated the networking
community to consider deploying ML for the automation of
networking operations. Recently, new networking paradigms
like cognitive self-driving networks [5] and most recently
knowledge deﬁned networking [10] have also emerged that de-
pend on and facilitate the extensive utilization of ML schemes
for conducting networking tasks. Recently ML has success-
fully applied on different cognitive self-driving networking
tasks such as modulation classiﬁcation [14] and representation
learning of radio signals [12].
Although, ML schemes especially deep neural networks
(DNN) have outperformed traditional networking schemes in
many networking tasks, it has been shown recently that DNN
and other ML schemes lacks robustness against “adversarial
examples”, which are deﬁned as inputs to the ML model
specially crafted by an adversary to cause a malfunction in
the performance of the ML model. These adversarial examples
are generated by adding small typically-imperceptible pertur-
bations to the legitimate examples for the express purpose of
misleading the ML model towards the production of wrong
results and to increase the prediction error of the model.
Based on the adversary’s knowledge, adversarial attacks are
classiﬁed into two major categories: white-box attacks and
black-box attacks. In white-box attacks, it is assumed that
the adversary has perfect knowledge about the victim model,
whereas in black-box attacks it is assumed that adversary has
no information about the victim model and the adversary can
only query the deployed ML model for a response and to later
use this information for crafting adversarial examples.
More formally, an adversarial example x∗is crafted by
adding a small imperceptible perturbation δ to the test example
x of the deployed trained classiﬁer f(.). The perturbation
δ is computed by approximating iteratively the nonlinear
optimization problem given in equation 1 until the crafted
adversarial example gets classiﬁed by the trained ML classiﬁer
f(.) in a wrong class t.
x∗= x + arg min
ηx {∥η∥: f(x + η) = t}
(1)
Adversarial examples are a direct consequence of an unsafe
assumption in ML that distribution encountered by the ML
model in training phase will also be encountered in the test
phase of the ML model.
The effects of adversarial ML examples in cognitive self-
driving networks have not been explored properly in the
literature. In this paper, we have performed an adversarial
attack on ML classiﬁers performing the task of modulation
classiﬁcation, which is an important application in cogni-
tive self-driving networks. Our results clearly highlight that
a small optimally-calculated adversarial perturbation for the
test example can cause a serious drop in performance of
the classiﬁcation output of the ML model. This paper also
highlights the vulnerability and brittleness associated with the
ML models used in the cognitive self-driving networks.
The major contributions of this work are:
• We have performed an adversarial ML attack on 9 ML-
based modulation classiﬁers to highlight the vulnerability
of these modulation classiﬁers to adversarial perturbation;
• We demonstrate the transferability phenomenon in the
setting of modulation classiﬁers by showing that an
adversarial example compromising one ML scheme will
also be to evade other ML schemes with high probability;
• To this best of our knowledge, this is the ﬁrst experiment
where the Carlini & Wagner (C-W) attack [3] has been
used to attack the modulation classiﬁcation task.
The rest of the paper is organized as follows. In the next sec-
tion, we will provide a brief review of the related research that
focuses on ML-based modulation classiﬁcation and adversarial
attacks on modulation classiﬁcation. Section III describes the
methodology where we have discussed the assumed threat
model, ML-models used for modulation classiﬁcation, and the
utilized adversarial attack for crafting adversarial examples.
Section IV provides the performance evaluation of the ad-
versarial attack on the modulation classiﬁcation. Section V
concludes the study and provides future directions.
arXiv:1909.12167v1  [cs.CR]  26 Sep 2019
II. RELATED WORK
A. Modulation classiﬁcation using ML schemes
The recent success of ML in computer vision and cyber-
physical systems has inspired a surge in the utilization of ML
schemes in wireless and data networks. It is conceived that ML
will be the backbone of future cognitive self-driving networks.
Modulation classiﬁcation is an important problem in dynamic
spectrum allocation of cognitive self-driving networks. There
are few ML-based modulation classiﬁcation schemes available
in the literature. Wong et al. [17] used a combination of
genetic algorithm and multi-layer perceptron for digital modu-
lation recognition. Aslam et al. [1] used genetic programming
with K-nearest neighbor (KNN) for modulation classiﬁcation.
Although genetic algorithms provides a good heuristic-based
solution but these algorithms do not scale efﬁciently with the
increase of the sample population.
Muller et al. [11] employed a combination of discrimina-
tive learning and support vector machines (SVM) for mod-
ulation classiﬁcation. Mendis et al. [9] utilized deep belief
networks (DBN) for modulation classiﬁcation, although DBN
has produced very impressive results but they are known
to be very difﬁcult to train and scale. O’Shea at al. [14]
used convolutional neural network (CNN), VGG, and ResNet
for performing modulation classiﬁcation schemes where they
have compared the deep ML-based modulation classiﬁcation
with the conventional modulation schemes under different
conﬁguration and noise levels and showed that ML-based
schemes performed better even in low signal to noise ratio
(SNR). Using ML schemes have produced very good results
but they are vulnerable to adversarial examples crafted by the
adversary to fool the ML-based classiﬁer to perform incorrect
classiﬁcation.
B. Adversarial attacks on ML-based modulation classiﬁcation
There has not been much work available on exploring the
threat of adversarial ML examples on modulation classiﬁca-
tion. Sadeghi et al. [16] used a variant of fast gradient sign
method (FGSM) [7] to perform an adversarial ML attack on
CNN-based modulation classiﬁcation and successfully showed
a considerable drop in classiﬁcation accuracy. FGSM is a
technique for crafting adversarial example where one step
gradient update is performed in the direction of the sign
associated with the gradient at each feature in the test example.
The FGSM perturbation (η) is given as:
η = ϵsign(∇xjθ(x, l))
(2)
Flowers et al. [6] provided an evaluation framework for
testing modulation classiﬁers against adversarial ML attacks,
they have tested the modulation classiﬁer against FGSM and
Gaussian random noise base adversarial attacks and showed
that FGSM causes more destruction than the random Gaussian
noise. Similarly, Kokalj et al. [8] used the FGSM attack
to demonstrate the vulnerability of modulation classiﬁcation
against adversarial examples. Bair et al. [2] highlighted the
limitations of the targeted adversarial attack on modulation
classiﬁcation in white-box settings.
Most of the results of the adversarial attacks reported on
modulation classiﬁcation have used FGSM attack without con-
sidering that FGSM was not designed to generate the optimal
amount of adversarial perturbation, it was only designed with
an absolute motivation of generating adversarial perturbations
quickly rather than optimally [3]. In this paper, we have
performed C-W attack [3] on modulation classiﬁcation to
compute the optimal adversarial perturbation.
III. METHODOLOGY
In this section, we describe our procedure for performing an
adversarial attack on modulation classiﬁcation. To the best of
our knowledge, there is no standardized ML-based solution for
modulation classiﬁcation in the cognitive self-driving networks
available yet in the literature, so for completeness we have
used both conventional and deep ML schemes for modulation
classiﬁcation. Before delving deep into the details of the
ML-models used for modulation classiﬁcation and adversarial
attack on it, we describe the threat model and a few related
assumptions.
A. Threat model
This subsection describes the major assumptions considered
for performing adversarial attack on modulation classiﬁer.
1) Adversary Knowledge: We have assumed a white-box
settings for performing an adversarial attack on DNN based
modulation classiﬁcation, which means adversary has the
complete knowledge about the model architecture, related
hyperparameters, and the test data. This assumption is fairly
standard in the adversarial ML domain. We have transferred
the adversarial examples for DNN-based modulation classiﬁer
to other conventional ML-based modulation classiﬁers. In this
paper, we have only assumed test time adversarial attacks.
Poisoning attacks are left for future considerations.
2) Adversarial Goals: Our goal in this experiment is to
compromise the integrity of the modulation classiﬁer through
adversarial examples and the success of the adversarial attack
in this paper will be measured by the comparison of the
accuracy before and after the adversarial attack.
B. Modulation Classiﬁcation Models
We have used DNN, KNN, SVM, Na¨
ıve Bayes (NB), linear
discriminant analysis (LDA), Decision Tree (DT), random
forest (RF), and ensemble methods for modulation classiﬁ-
cation. To the best of our knowledge, this is the ﬁrst paper
that uses almost all the famous ML schemes for modulation
classiﬁcation and then performs adversarial ML attack on these
schemes to highlight that conventional ML, deep ML, and
ensemble methods do not provide robustness against small
carefully-crafted perturbations.
For the DNN classiﬁer, we have used four dense hidden
layers network with rectiﬁed linear units as a nonlinear
activation in hidden layers and softmax for calculating the
classiﬁcation probabilities of each class. Stochastic gradient
descent (SGD) has been used as an optimizer and categorical
cross-entropy as the associated loss function for training the
DNN based modulation classiﬁer. For the KNN classiﬁer, we
have used 15 neighbors as an optimal number of neighbors for
performing the classiﬁcation. We have used radial basis func-
tion (RBF) kernel for performing the SVM based modulation
classiﬁcation. For the NB classiﬁer, we have assumed Gaussian
distribution as the underlying modulation data distribution. For
the LDA-based classiﬁcation, we have used singular value
decomposition (SVD) solver as an SVD solver can better
handle a large number of modulation data features. For the
DT classiﬁer, we have used maximum unfolding depth of 12
for achieving good classiﬁcation result. For the RF classiﬁer,
we have a maximum of 10 trees forest for estimating the
classiﬁcation results. For ensemble methods for modulation
classiﬁcation, we have employed AdaBoost and Gradient-
Boosting algorithms. The obtained classiﬁcation results are
provided in section IV.
C. Adversarial attack
We have performed C-W [3] attack on ML-based mod-
ulation classiﬁers to demonstrate the lack of robustness of
the ML-based modulation classiﬁcation scheme in cognitive
self-driving networks. Carlini et al. [3] proposed three very
powerful adversarial ML perturbation crafting techniques by
using three distance matrices (L0, L2, and L∞) and these
attacks have successfully evaded the defensive distillation
method [15] (a popular early scheme for defending against
adversarial examples).
We have used L2-based C-W attack for crafting adversarial
examples. Instead of formulating the adversarial ML problem
as in equation 1 (which is highly nonlinear formulation that
is difﬁcult to optimize), an alternative formulation (provided
in equation 3 where g(x∗) is the new objective function such
that g(x∗) ≤0 iff g(x∗) = t here t can be any label but the
true label) is used by the the C-W attack that can be solved
by gradient descent. The best performing objective function
g(.) used for crafting adversarial examples for modulation
classiﬁcation is provided in equation 4, where Z denotes the
softmax function.
minimize
η
∥η∥P + c.g(x∗)
such that
(x∗) ∈[0, 1]n
(3)
g(x∗) = max{0, max
i̸=t Z(x∗)i −Z(x∗)t}
(4)
We have only opted to use an L2-based adversarial attack
because we want to keep the perturbation η to a minimum
while minimizing the squared error between adversarial mod-
ulation example and the original modulation example.
Many defenses against adversarial examples have been
proposed in literature but this powerful attack has beaten all
of them [4] and to the best of our knowledge there does not
exist any defense that ensures robustness against L2-based C-
W adversarial attack.
In our experiments, we wish to achieve the following
objectives:
• Objective 1: Do the ML schemes used for modulation
classiﬁcation in the literature provide necessary robust-
ness against adversarial perturbations?
• Objective 2: We want to experimentally verify that the
adversarial examples breaching one ML schemes will
breach other ML models with high probability even if
the deployed ML model is unknown.
Before explaining how we have met these objectives through
our experiment in the next section, we provide a detailed
description of the dataset used for performing the experiments.
D. Dataset
We have used highly cited GNU radio ML RML2016.10a
dataset [13] for our experimentation, the reason for selecting
this dataset is its public availability and utilization in the
literature. Dataset consists of 220000 input examples, where
each example is associated with a modulation scheme at a
speciﬁc SNR. Dataset has 11 modulation schemes namely:
AM-DSB, AM-SSB, WBFM, PAM4, BPSK, QPSK, 8PSK,
QAM16, QAM64, CPFSK, and GFSK. Out of these 11 mod-
ulation schemes, 8 are digital modulations and 3 are analog
modulation schemes.
For this experiment, we have used eight digital modulation
schemes and excluded three analog modulation schemes. The
excluded schemes are AM-DSB, AM-SSB, and WBFM. The
total number of examples used in these experiments is 160000.
Each example is a 256 size vector with 128 in-phase and 128
quadrature-phase components. This dataset was generated for
20 different SNR levels from -20 dB to 18 dB. More details
of the dataset preparations can be found in [13].
IV. PERFORMANCE EVALUATION
In this section, we have provided a detailed evaluation
of the ML-based modulation classiﬁers against adversarial
perturbations.
A. Performance Impact
We have evaluated the ML-based modulation classiﬁcation
before and after the adversarial attack. We have used the
accuracy as the performance metric the decay in the modu-
lation classiﬁcation describes the adversarial attack success.
Figure 1 provides a detailed comparison of accuracy and SNR
before and after the adversarial attack. The clear drop in the
accuracy of the classiﬁers with increasing SNR fulﬁlls the
ﬁrst objective of this experiment where we set out to show
that the ML-schemes proposed in the literature for modulation
classiﬁcation does not provide the necessary robustness against
adversarial examples.
B. Transferability of Adversarial Examples
Here we want to note that adversarial examples were only
crafted for DNN-based modulation classiﬁer under white-
box assumptions. The adversarial examples compromising
the integrity of the DNN classiﬁers were transferred to the
rest of ML classiﬁers under black-box assumptions and it
turns out that modulation classiﬁers based on conventional
ML techniques are also equally vulnerable to the adversarial
examples which fulﬁll the second objective we wanted to
achieve through this experiment.
V. CONCLUSIONS
In this paper, we have highlighted the lack of robustness
of ML-models utilized in modulation classiﬁcation by suc-
cessfully evading 9 different ML-based modulation classiﬁers.
We have also successfully shown that transferability of the
adversarial examples from one model to another model for
performing the adversarial attack. This work has also provided
a glimpse of the security and robustness issues associated
with the utilization of ML models in cognitive self-organizing
networks. Designing new defenses against adversarial attacks
on cognitive self-driving networks are left as a future direction.
Figure 1. The accuracy of ML models used for modulation classiﬁcation before and after adversarial ML attack is provided in the ﬁgure. A clear drop in the
accuracy with improving SNR after the adversarial attack clearly indicates the lack of deterrence against small carefully crafted adversarial perturbations.
REFERENCES
[1] M. W. Aslam, Z. Zhu, and A. K. Nandi, “Automatic modulation
classiﬁcation using combination of genetic programming and knn,” IEEE
Transactions on wireless communications, vol. 11, no. 8, pp. 2742–2750,
2012.
[2] S. Bair, M. DelVecchio, B. Flowers, A. J. Michaels, and W. C. Headley,
“On the limitations of targeted adversarial evasion attacks against deep
learning enabled modulation recognition,” in Proceedings of the ACM
Workshop on Wireless Security and Machine Learning.
ACM, 2019,
pp. 25–30.
[3] Carlini and Wagner, “Towards evaluating the robustness of neural
networks,” in Security and Privacy (SP), 2017 IEEE Symposium on.
IEEE, 2017, pp. 39–57.
[4] N. Carlini and D. Wagner, “Adversarial examples are not easily detected:
Bypassing ten detection methods,” in Proceedings of the 10th ACM
Workshop on Artiﬁcial Intelligence and Security.
ACM, 2017, pp.
3–14.
[5] N. Feamster and J. Rexford, “Why (and how) networks should run
themselves,” arXiv preprint arXiv:1710.11583, 2017.
[6] B. Flowers, R. M. Buehrer, and W. C. Headley, “Evaluating adversarial
evasion attacks in the context of wireless communications,” arXiv
preprint arXiv:1903.01563, 2019.
[7] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[8] S. Kokalj-Filipovic and R. Miller, “Adversarial examples in rf deep
learning: Detection of the attack and its physical robustness,” arXiv
preprint arXiv:1902.06044, 2019.
[9] G. J. Mendis, J. Wei, and A. Madanayake, “Deep learning-based
automated modulation classiﬁcation for cognitive radio,” in 2016 IEEE
International Conference on Communication Systems (ICCS).
IEEE,
2016, pp. 1–6.
[10] A. Mestres, A. Rodriguez-Natal, J. Carner, P. Barlet-Ros, E. Alarc´
on,
M. Sol´
e, V. Munt´
es-Mulero, D. Meyer, S. Barkai, M. J. Hibbett
et al., “Knowledge-deﬁned networking,” ACM SIGCOMM Computer
Communication Review, vol. 47, no. 3, pp. 2–10, 2017.
[11] F. C. Muller, C. Cardoso, and A. Klautau, “A front end for discriminative
learning in automatic modulation classiﬁcation,” IEEE Communications
Letters, vol. 15, no. 4, pp. 443–445, 2011.
[12] T. J. O’Shea, J. Corgan, and T. C. Clancy, “Unsupervised representation
learning of structured radio communication signals,” in 2016 First Inter-
national Workshop on Sensing, Processing and Learning for Intelligent
Machines (SPLINE).
IEEE, 2016, pp. 1–5.
[13] T. J. O’shea and N. West, “Radio machine learning dataset generation
with gnu radio,” in Proceedings of the GNU Radio Conference, vol. 1,
no. 1, 2016.
[14] T. J. O’Shea, J. Corgan, and T. C. Clancy, “Convolutional radio modula-
tion recognition networks,” in International conference on engineering
applications of neural networks.
Springer, 2016, pp. 213–226.
[15] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami, “Distillation
as a defense to adversarial perturbations against deep neural networks,”
in Security and Privacy (SP), 2016 IEEE Symposium on.
IEEE, 2016,
pp. 582–597.
[16] M. Sadeghi and E. G. Larsson, “Adversarial attacks on deep-learning
based radio signal classiﬁcation,” IEEE Wireless Communications Let-
ters, vol. 8, no. 1, pp. 213–216, 2018.
[17] M. D. Wong and A. K. Nandi, “Automatic digital modulation recognition
using artiﬁcial neural network and genetic algorithm,” Signal Processing,
vol. 84, no. 2, pp. 351–365, 2004.
