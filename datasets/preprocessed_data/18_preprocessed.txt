
ieee transactions on artificial intelligence vol  no  april 
challenges and countermeasures for adversarial
attacks on deep reinforcement learning
inaam ilahi
 muhammad usama
 junaid qadir
 senior member ieee muhammad umar janjua
ala alfuqaha
 senior member ieee dinh thai hoang
 member ieee and dusit niyato
 fellow ieee
abstractdeep reinforcement learning drl has numerous
applications in the real world thanks to its ability to achieve
high performance in a range of environments with little man
ual oversight despite its great advantages drl is susceptible
to adversarial attacks which precludes its use in reallife critical
systems and applications eg smart grids trafﬁc controls and
autonomous vehicles unless its vulnerabilities are addressed and
mitigated to address this problem we provide a comprehensive
survey that discusses emerging attacks on drlbased systems and
the potential countermeasures to defend against these attacks we
ﬁrst review the fundamental background on drl and present
emerging adversarial attacks on machine learning techniques we
then investigate the vulnerabilities that an adversary can exploit to
attack drl along with stateoftheart countermeasures to prevent
such attacks finally we highlight open issues and research chal
lenges for developing solutions to deal with attacks on drlbased
intelligent systems
impact statementdeep reinforcement learning drl has nu
merous reallife applications ranging from autonomous driving
to healthcare it has demonstrated superhuman performance in
playing complex games like go however in recent years many
researchers have identiﬁed various vulnerabilities of drl keeping
this critical aspect in mind in this article we present a comprehen
sive survey of different attacks on drl and various countermea
sures that can be used for robustifying drl to the best of our
knowledge this survey is the ﬁrst attempt at classifying the attacks
based on the different components of the drl pipeline this article
will provide a roadmap for the researchers and practitioners to
develop robust drl systems
index termsadversarial machine learning cybersecurity
deepreinforcementlearningdrlmachinelearningmlrobust
machine learning
manuscript received may   revised july   and august 
 accepted september   date of publication september  
date of current version march   this work was supported by the qatar
national research fund a member of qatar foundation through the national
priorities research program under grant s this article was
recommended for publication by associate editor prof pablo estevez inaam
ilahi and muhammad usama contributed equally to this work corresponding
author inaam ilahi
inaam ilahi muhammad usama and muhammad umar janjua are
with the information technology university lahore  pakistan e
mail mscsituedupk muhammadusamaituedupk umarjanjua
ituedupk
junaid qadir is with the information technology university lahore
 pakistan and also with qatar university doha  qatar email
junaidqadirituedupk
ala alfuqaha is with the hamad bin khalifa university doha  qatar
email aalfuqahahbkueduqa
dinh thai hoang is with the university of technology sydney ultimo nsw
 australia email hoangdinhutseduau
dusit niyato is with the nanyang technological university
singapore
 email dniyatontuedusg
digital object identiﬁer tai
nomenclature
ac
asynchronous advantage actorcritic
ac
advantage actorcritic
asa
adversarialstrategic agent
age
adversarially guided exploration
ai
artiﬁcial intelligence
arpl
adversarially robust policy learning
atn
adversarial transformation networks
carrl
certiﬁed adversarial robustness for rl
cw
carlini and wagner
cdg
common dominant adversarial example genera
tion
cmarl
cooperative multiagent reinforcement learning
drl
deep reinforcement learning
ddpg
deep deterministic policy gradient
ddqns
double deep qnetworks
djsma
dynamic budget jsma
dl
deep learning
dqn
deep qnetworks
fgsm
fast gradient sign method
frarl
falsiﬁcationbased rarl
gb
gradient based
gps
graded policy search
ia
imagination augmented agents
irl
inverse reinforcement learning
itfgsm
iterative targetbased fgsm method
jsma
jacobianbased saliency map attack
kl
kullbackleibler
las
lookahead action space
mad
maximal action difference
mas
myopic action space
mbmfrl
modelbased priors for modelfree reinforcement
learning
mdp
markov decision process
metrpo
model ensemble trust region policy optimization
ml
machine learning
mlah
metalearned advantage hierarchy
mpc
modelpredictive control
mujoco
multijoint dynamics with contact
mve
modelbased value expansion
naf
normalized advantage function
nrmdp
noisy action robust mdp
pca
principal component analysis
pepg
parameter exploring policy gradients
pomdp
partially observable markov decision process
   ieee personal use is permitted but republicationredistribution requires ieee permission
see httpswwwieeeorgpublicationsrightsindexhtml for more information
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
ppo
proximal policy optimization
prmdp
probabilistic mdp
rararl
riskaverse rarl
rarl
robust adversarial rl
rbfq
radialbasisfunctionbased qlearning
rnn
recurrent neural network
sarsa
stateactionrewardstateaction algorithm
samdp
stateadversarial mdp
sdn
softwaredeﬁned networking
sfd
samplingbased ﬁnite difference
sgd
stochastic gradient descent
spg
stochastic policy gradient
steve
stochastic ensemble value expansion
tmdps
threatened markov decision processes
trpo
trust region policy optimization
wma
weighted majority algorithm
i introduction
t
he ultimate goal of research in ai is to develop artiﬁ
cial general intelligence agi agents that can perform
similar activities as humans in a more efﬁcient manner this
longstanding challenge for developing such agents is no longer
a pipe dream thanks to rapid growth in computational ai and
ml technologies in the last decade ml and especially dl
have revolutionized ﬁelds such as computer vision language
processing etc ml is divided into three categories  namely
supervised unsupervised and reinforcement learning rl in
supervised learning training data along with the corresponding
labels are available for decision making supervised learning is
by far the most wellstudied branch of ml for problems with
labeled data which has a lot of applications in practice such as
object recognition speech recognition spam detection pattern
recognition and many more in unsupervised learning the target
is to infer the underlying patterns and structures from unlabeled
data rl is deﬁned as a learning process that focuses on ﬁnding
the best strategies for agents based on the interactions with the
surrounding environment unlike supervised and unsupervised
learning processes which need training data to learn rl agents
can learn in an online manner based on observations obtained
through realtime interactions with the environment
rl utilizes a trialanderror process to solve sequential
decisionmaking problems in robotics control and many other
realworld problems rl algorithms also have some limitations
tobeutilizedinpracticemainlyduetotheirslowlearningprocess
and inability to learn in complex environments recently a new
technique combining the advancement of dl called drl has
been introduced  drl has shown great results in many
complex decisionmaking processes such as designated task
completion in robotics  navigating driverless autonomous
vehicles   healthcare  ﬁnancial trading  smart grid
management  automated transportation management 
wireless and data network management  and for playing
games such as pong  go  etc in  drl beat the
human champions in the game of go  and most recently
a team of ﬁve drl agents has beaten the world champion
human team in dota matches  this shows that drl is
promising and can address highly complex and timesensitive
decisionmaking problems in real time
with the rapid adoption of drl in critical realworld
applications the security of drl has become a very important
area of research   recently drl has been vulnerable
to adversarial attacks where an imperceptible perturbation
is added to the input to the drl schemes with a predeﬁned
goal of causing a malfunction in the working of drl 
thus it is crucial to understand the types and nature of these
vulnerabilities and their potential mitigation procedures before
deploying drlbased reallife critical systems eg smart
grids and autonomous vehicles here we want to note that the
securityofsupervisedandunsupervisedmliswellstudiedinthe
literature  but the security of drl has not yet received sim
ilar attention in  behzadan and munir  reviewed the
security vulnerabilities and open challenges in drl although
it provides a decent initial review of the security concerns it
does not properly cover the security issues associated with four
major components of the drl pipeline state action model
and reward and related robustness mechanisms furthermore
the survey does not cover the recent studies we aim to fulﬁll
these requirements by providing a more comprehensive survey
on attacks and defense techniques together with a discussion of
the future research directions on drl
contributions of this article in this article we build upon the
existing literature available on security vulnerabilities of drl
and their countermeasures and provide a comprehensive and
extensive review of the related work the major contributions
of this article are as follows
 we provide the drl fundamentals along with a nonex
haustive taxonomy of advanced drl algorithms
 we present a comprehensive survey of adversarial attacks
on drl and their potential countermeasures
 we discuss the available benchmarks and metrics for the
robustness of drl
 finally we highlight the open issues and research chal
lenges in the robustness of drl and introduce some
potential research directions
organization of this article the organization of this article
is depicted in fig  an overview of the challenges faced by ml
and drl schemes has been provided in section ii section iii
presents a comprehensive review of adversarial ml attacks
on the drl pipeline a detailed overview of countermeasures
proposed in the literature to ensure robustness against adver
sarial attacks is presented in section iv section v presents
the available benchmarking tools and metrics along with open
research problems in drl section vi describes the open issues
and research challenges in designing adversarial attacks and ro
bustness mechanisms for drl finally we conclude this article
in section vii for the convenience of the reader a summary
of the salient acronyms used in this article is presented in the
nomenclature
ii background
in this section we discuss the fundamentals of the drl
process then we provide a summary of the shortcomings of
the ml and drl techniques
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ieee transactions on artificial intelligence vol  no  april 
fig 
organization of this article
fig 
basic process of the mdp in rl
a fundamentals of drl
the important concepts used in drl are described as follows
 markov decision process a generic rl problem is
described as an mdp in terms of the state action reward
and dynamics of the system in an mdp at each time step
the agent observes the current state st and performs an
action at based on its current policy π after the action is
executed the agent observes its reward rt and next state
st the objective of an mdp is to ﬁnd the best actions
which maximize its longterm expected reward fig 
illustrates a typical mdp setup with an agent interacting
with its surrounding environment
 environment is a simulator or a realworld scenario in
which the agent interacts and learns at each time step
the agent governed by the policy interacts with the
environment and in return receives a reward environ
ment is divided into two categories namely partially
observable and fully observable in the case of a partially
observableenvironmenttheagentisonlyabletopartially
observe the environment for these partially observable
environments pomdps are used in a fully observable
environment the agent can observe all the states and we
use mdps for this mdps are a special case of pomdps
where the observation function is identity
 action is a stimulus used by the agent for interactions
with the environment the actions can be discrete or
continuous based on the environment and drl problem
formulation
 reward is an incentive expressed by a numerical value
that the agent receives after making an action the goal
of an agent is to maximize the accumulated reward to
reduce the impact of the reward r which the agent might
get in a later state due to taking a speciﬁc action a in the
current state st the notion of discounted rewards was
introduced it is usually denoted by γ and can take any
valuerangingfromtomathematicallythediscounted
reward rt given as
rt 
t
tt
γttrst
where t denotes the time step t is the ﬁnal time step r
denotes the reward for the time step and st denotes the
current state
 value function speciﬁes the value of a state value is
deﬁned as the maximum expected discounted reward of
a certain state mathematically it is determined as
vπs  eπrts  st
where π is the policy rt is the discounted reward and
st is the current state
 qfunction speciﬁes the qvalue of a state qvalue is
deﬁned as the maximum expected discounted reward an
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
fig 
nonexhaustive taxonomy of major drl schemes as proposed in 
agent may get by taking a speciﬁc action at a speciﬁc
state mathematically it can be calculated as follows
qπs  eπrts  st a  at
where π is the policy rt is the discounted reward st is
the current state and at is the current action
 advantage function is the difference of the qvalue of a
speciﬁc action at a state qs a from the value of that
state v s ie as a  qs a v s
 policy deﬁnes how the agent will behave in the envi
ronment at a particular time in other words it is a
mapping from the perceived states of the environment
to the actions taken in those conditions a policy is said
to be optimal if it achieves the maximum possible reward
at each state policies are further divided into two types
deterministic policy and stochastic policy when actions
taken by the agent are deterministic the policy is termed
as deterministic on the other hand when the actions
are sampled from the conditional probability distribution
of actions given the states the policy is called to be
stochastic
 onpolicy algorithms enable an agent to learn and update
its policy in an online manner through realtime interac
tion with the environment samples generated from the
current policy are used to train the algorithm to estimate
the policy in advance
 offpolicy algorithms use an online policy and a target
policy the target policy is used to estimate the action val
ues while the online policy is being learned hence the
agent can estimate the target policy without its complete
knowledge
 model mimics the behavior of the environment hence
allowing inferences to be made about the behavior of
the environment based on the availability of the system
models the drl schemes are divided into further two
categories namely modelbased and modelfree rl
 exploration and exploitation exploration is the process
when the agent tries to explore the surrounding environ
ment by taking different actions available at a given state
exploitation occurs after exploration the agent exploits
the optimal actions to achieve the maximum cumulative
reward an ϵgreedy policy is used to balance exploration
and exploitation the agent chooses a random action
with a certain probability otherwise it takes the action
followed by the policy the probability of the random
action being taken keeps decreasing with each time step
this change factor is usually denoted by λ
a taxonomy of major drl algorithms as proposed in  has
been provided in fig  we refer the interested readers to 
and  for further details on variations of drl schemes
b security of ml
although the utilization of ml techniques has revolutionized
many areas including vision language speech and control
it has also introduced new security challenges that are very
threatening in designing and developing new dynamic intelli
gent systems security attacks in ml can be divided into two
categories training phase attacks and inference phase attacks
for the training phase attacks the adversary tries to force the
learning process to learn faulty modelpolicy by introducing
small imperceptible perturbations to the input data inference
phaseattacksareperformedbytheadversaryattheinferencetest
time of the ml pipeline to fool the modelpolicy in providing
malfunctioned resultsactions
the malicious input generated by adding adversarial perturba
tions into the original input is known as an adversarial example
adversarial examples are classiﬁed into four major categories
based on the objective knowledge frequency and speciﬁcity
formally an adversarial example xis created by adding a
small imperceptible carefully crafted perturbation δ to the cor
rectly classiﬁed example x the perturbation δ is calculated
by approximating the optimization problem iteratively until the
crafted adversarial example gets classiﬁed by ml classiﬁer f
in targeted class t as follows
x x  arg min
δx δ fx  δ  t
where t is the targeted class fig  shows a basic taxonomy of
attacks on ml
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ieee transactions on artificial intelligence vol  no  april 
fig 
taxonomy of adversarial ml attacks classiﬁed according to the adversarys objective knowledge speciﬁcity and attack frequency
 attacks based on adversarys knowledge depending on
the adversarys knowledge about the targeted ml model adver
sarial attacks are divided into further three categories whitebox
attacks graybox attacks and blackbox attacks in the case of
whitebox attacks the adversary has perfect knowledge of the
target mldl algorithm ie the adversary knows the training
and testing data parameters of the model etc these attacks
are used for the worstcase security malfunction analysis of an
mldl system in the case of graybox attacks the adversary is
supposed to have limited knowledge knowledge about feature
representation and optimization algorithms only about the tar
geted mldl model the adversary designs a surrogate model
on the limited knowledge available and uses transferability
property  of the adversarial examples where an adversarial
example evading a classiﬁer will evade other similar classiﬁers
even if they are trained on another dataset to evade the mldl
based system the attacker may also have limited test access to
the model ie it may be able to ask the model the output on some
inputs in the case of blackbox attacks the adversary does not
know the model or any of its attributes the adversary can only
query the systems for labels or conﬁdence scores and develop
an adversarial perturbation based on the feedback provided by
the deployed mldl model
 attacks based on adversarys goals based on the adver
sarys objective adversarial attacks are divided into four types
 conﬁdence reduction attacks in which adversarial attacks
are launched to compromise the conﬁdence levels of the
predictions of the deployed mldlbased system
 misclassiﬁcation attacks in which adversarial attacks are
launched for disturbing the classiﬁcation boundary of any
class to cause misclassiﬁcation
 targeted misclassiﬁcation attacks in which adversarial
attacks are launched to misclassify only a targeted class
and
 sourcetarget misclassiﬁcation attacks in which adversar
ial attacks are launched to force misclassiﬁcation of a
speciﬁc source class into a speciﬁcally targeted class
 attacks based on adversarys speciﬁcity based on speci
ﬁcity adversarial examples can be classiﬁed into two types ie
targeted and nontargeted these concepts are similar to the ones
as in the case of the adversarys objective in the case of targeted
attacks the attackers target speciﬁc classes in the output while
in the case of nontargeted attacks the goal is to misclassify the
maximum number of samples
although adversarial examples are transferable from one
ml model to another in many cases the performance of the
transferred examples is not enough to further improve the
performance of blackbox attacks while reducing the number
of queries needed for the attack queryefﬁcient blackbox at
tacks are required different queryefﬁcient blackbox attack
methods are available in the literature cheng et al  use
randomized gradientfree methods for the creation of adversarial
examples and show their algorithm to require three to four
times fewer queries to achieve the same performance as the
stateoftheart attacks chen et al  uses the zerothorder
optimization technique for adversarial perturbation generation
and shows their blackbox attack to demonstrate the same
performance as stateoftheart whitebox attacks the queries
required by their technique  are less than those required
by that of  tu et al  propose a more queryefﬁcient
attack they propose autoencoderbased zerothorder optimiza
tion for adversarial image generation in blackbox attacks they
show a reduction of more than  in the mean query count
while maintaining the same performance as the stateofthe
art attacks more details on adversarial ml can be found in
 and 
c security of drl
the increasing use of drl in practical applications has led
to an investigation of the security risks it faces however the
security challenges faced by drl are different from those ex
perienced by other ml algorithms the major difference is that
a drl process is trained to solve sequential decisionmaking
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
fig 
taxonomy of adversarial attacks on drl classiﬁed according to the major parts of drl
problems in contrast to most other ml schemes that are trained
to solve singlestep prediction problems the independence of
the current actions from the previous ones increases the degrees
of freedom of adversarial attacks raising new challenges that
must be addressed this makes the adversarial attacks more
challenging to be recognized as we cannot discriminate between
the action intentionally taken by the agent and the action the
adversary forceslures the agent to take also the training is
done on a dataset from a ﬁxed distribution in the case of
ml in contrast to the drl where the agent begins with a
deterministic or stochastic policy and starts exploring for best
actions
usually rl problems are formulated as an mdp consisting
of four parts s a r p where s is the state space a is the
action space r is the reward function and p is the transition
matrix between states hence an adversary has more choices
to attack if the adversary targets the state space impercepti
ble perturbations can be added to the environment directly by
perturbing the sensors  similarly an adversary can target
any of the four major components of the mdp adversarial
attacks on drl are classiﬁed into inferencetime and training
time attacks  an adversary may compromise one or more
than one dimension of conﬁdentiality integrity and availability
based on the goal of the adversary the adversarial attacks
on drl can be classiﬁed into active or passive  for active
attacks the adversary desires to change the behavior of the agent
while for passive attacks the adversary desires to infer details
about the model reward function or other parts of drl an
adversary can use these details to either create a copy of the
model or use them to perform an attack on the model the
adversary may be limited by the part of the environment where
an adversary is only capable of making changes to a certain area
of the environment adding a lot of perturbation in a single time
instance may make the attack perceptible which is not preferred
by the adversary distinguishing the adversarial samples and
behavior from the normal ones in the case of drl is not as
easy as in supervised learning because of the increased possible
attack dimensions
iii adversarial attacks on drl
in this section we discuss the adversarial attacks on drl
we divide the attacks on drl into four categories based on the
functional components of the drl process a major portion
of the attacks involve the addition of adversarial perturbations
to the state space and a small portion of the proposed attacks
involve perturbing the reward and action space fig  shows a
basic taxonomy of the adversarial attacks on drl algorithms
a attacks perturbing the state space
we divide this subsection based on the access of the adversary
 manipulating the observations since dnns are vulner
able to adversarial attacks in supervised learning we would
expect dnns trained via drl to also be vulnerable indeed
behzadan and munir  show this vulnerability and verify
the transferability of adversarial examples across different dqn
models they consider a maninthemiddle adversary between
the environment and the drl agent where the adversary
perturbs the states from the environment and forwards these
perturbed states to the drl agent to take the desired action to
ensure the imperceptibility of the perturbation the amplitude
of the adversarial examples crafting algorithms fgsm and
jsma  is controlled the attack procedure is divided into
two phases initialization and exploitation the initialization
phase includes the training of a dqn on adversarial reward
function to generate an adversarial policy then a replica of the
targets dqn is created and initialized from random parameters
the exploitation phase includes generating adversarial inputs
such that the target dqn can be made to follow actions governed
by the adversarial policy furthermore they propose an attack
method to manipulate the policy of the dqn by exploiting the
transferability of adversarial samples they use a blackbox set
ting and show a success rate of  when adversarial examples
are transferred from one model to another the cycle of the
proposed policy induction attack is shown in fig  huang
et al  use the attack proposed in  and show a signiﬁcant
drop in the performance of dqn trpo and ac methods in
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ieee transactions on artificial intelligence vol  no  april 
fig 
process of policy induction attack  performed on the game of pong
both white and blackbox settings they show the dqn to be
more susceptible to adversarial attacks than the trpo and ac
lin et al  argue that the uniform attack schemes may
not be practically feasible and are easy to detect they con
sider a different approach and propose two adversarial attack
techniques on drl schemes strategically timed attack and
enchanting attacks for the former they propose to minimize
the reward of the drl schemes by using adversarial examples
on a subset of time steps in an episode of the drl operation
for the latter they propose a novel method of luring the drl
agent to a predeﬁned targeted state by using a generative model
and a sophisticated planning algorithm the generative model is
used to predict the next state in the space and the planning
algorithm is used to generate actions required for luring the
agent to the targeted state performance of the strategically
timed attack and the enchanting attack is reported on dqn and
ac agents playing atari games where  of the success
rate of the adversarial attacks is reported they use the cw
attack  for generating adversarial inputs it is also shown that
perturbing only  of the inputs using the proposed method
produces the same results as the previously proposed attacks
based on the fgsm the workﬂow of the enchanting attack is
shown in fig 
tretschk et al  propose a similar approach to the en
chanting attacks proposed by lin et al  where they use the
adversarial transformer network atn  to impose adver
sarial reward on the policy network of drl the atn makes
the agent maximize the adversarial reward through a sequence
of adversarial inputs complete information regarding the agent
and the target environment is required hence making the attack
white box it is shown that given a large enough threshold for
perturbation the agent can be made to follow the adversarial
policy at the test time
pattanaik et al  prove that fgsmbased attacks on
drl  do not use an optimal cost function for crafting the
adversarial inputs and propose a loss function that is guaranteed
to maximize the probability of taking the worst possible action
they propose three types of gb adversarial attacks on dqn and
ddpg techniques for reducing the expected reward by adding
perturbations to the observations the ﬁrst attack is based on
a naive approach of adding random noise to the drl states
to mislead the drl agent in selecting a suboptimal action
that decays the performance of the drl scheme the second
attack is a gb attack where a new cost function is introduced
for creating adversarial actions that outperforms the fgsm
in determining the worst possible discrete action to limit the
performance of drl schemes the third attack is an improved
version of the second attack instead of using a simple gb
approach for generating adversarial perturbation the authors
use sgd for adversarial action generation which ultimately
misleads the drl agent to end up in a predeﬁned adversarial
state
kos and song  discuss that the previous attacks require
perturbing several states to be successful which may not be
practically feasible they propose to use a value function to
guide the adversarial perturbation injection hence reducing the
number of adversarial perturbations needed for introducing a
malfunction in drl policies they propose three types of attack
situations  the addition of noise at a ﬁxed frequency  the
addition of specially designed perturbed inputs after n samples
and  the recalculation of the perturbation after n samples and
adding the previously calculated perturbation to the intermediate
steps the results show that their last approach performs as well
as the one in which all states are perturbed furthermore they
use the generated samples for retraining the model and show
that resilience can be improved against both fgsm and random
adversarial perturbations
a similar issue has been raised by sun et al  further
more they discuss that the previously proposed attacks are
not generalpurpose and have limitations eg  cannot be
used for continuous action spaces they propose two sample
efﬁcient generalpurpose attacks that can be used to attack any
drl algorithm while considering longterm damage impacts
namely critical point attack and antagonist attack the ﬁrst one
involves the building of a model by the adversary to predict
future environmental states and the agents actions the damage
of each possible attack strategy is then assessed and the optimal
one is chosen the antagonist attack involves automatic learn
ing of a domainagnostic model by the adversary to discover
the critical moments of attacking the agent in an episode to
be successful the critical point technique only requires one
torcs or two atari pong and breakout steps and the
antagonist technique needs fewer than ﬁve steps four mujoco
tasks
hussenot et al  discuss that the previously proposed
approaches are either practically infeasible or computationally
extensive they propose two types of adversarial attacks to take
full control of the drl agents policy the ﬁrst one called per
observation attack includes the generation of a new adversarial
perturbation for every observation of the agent and adding that
perturbationtotheenvironmentthesecondonecalleduniversal
mask attack includes the addition of one universal perturbation
created at the start of the attack to all the observations these
attacks are discussed in both targeted and nontargeted situations
it is also reported that the proposed attacks are more successful if
the fgsm is used for generating the perturbations in untargeted
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
fig 
illustration of enchanting attack on pacman is depicted which is highlighting all four components  action sequence planning  crafting an adversarial
example with a target action  the agent takes an action and  environment generates the next state st
attack situations whereas in the case of targeted attacks the
fgsm is not able to generate imperceptible adversarial samples
chan et al  take a different approach from other articles
and propose static reward impact maps which can be used to
quantify the inﬂuence on the reward of each feature in the state
space by the use of such a map the adversary can choose to
perturb only those features which have a large impact on the
cumulative reward the time complexity of the generation of
these static maps is posed as a limitation
cmarl algorithms are gaining attention in a wide range of
applications such as cellular base station control  trafﬁc
light control  and autonomous driving  the target of
the agent in cmarl is to learn to take action cooperatively
as a team to maximize a total team reward lin et al 
show the vulnerability of cmarl agents to adversarial attacks
by proposing a mechanism of adding perturbations to the state
space difﬁculty to estimate team rewards difﬁculty to measure
the effect of misprediction of an agent on the team reward
nondifferentiability of models and lowdimensionality of the
feature space make attacking such environments challenging
they hypothesize that the cooperative aspects of cmarl
agents make these agents more vulnerable to adversarial attacks
as compared to singleagent rl as the failure of a single agent
may cause the failure of the whole team they extend the fgsm
attack and propose two new approaches to decrease the team
reward more effectively itfgsm and djsma attack these
attacks involve training an adversarial policy network to search
for a suboptimal action from which the adversarial examples
are then introduced in the observations of one of the agents to
force it to take the targeted action they test their attack on the
startcraft ii multiagent benchmark they show that their attack
can decrease the reward from  to  by attacking only a single
agent out of the multiple possible agents when the perturbations
are added with an average of  l norm as a reaction to this
drop in reward the winning rate of the multiagent drops from
 to  furthermore they discuss the applicability of the
proposed attack in real environments as an adversary can gain
access to a single agent and use it to attack the whole system
despite the success of drl there is little research that
studies the impact of adversarial attacks in drl algorithms
that do not use images as inputs wang et al  propose
techniques that can degrade the performance of a welltrained
drlbased energy management system of an electric vehicle
causing them to either use much fuel or lead it into running
out of battery before the end of the trip they show their
adversarial inputs to be imperceptible for whitebox settings
theyusethefgsmastheadversarywhileforblackboxsettings
attack transferability and the ﬁnitedifference method  are
used they test their attacks on a dqn trained for energy
management of an electric vehicle and show the degradation of
performance
 manipulating the environment adversarial attacks on
the state space can also be carried out by adding perturbations
in the environment of the agent in turn this causes the agent
to consider the environment as the adversary desires chen et
al  propose cdg method for crafting adversarial examples
with high conﬁdence for the environment of drl the core
idea of their attack is the addition of confusing obstacles to the
original clean map for the case of pathﬁnding to confuse the
robot by messing with its local information for a perturbation
to be successful it should either stop the agent from reaching the
destination or otherwise delay the agent the proposed attack is
tested on ac and is shown to be successful at least  of
the time
bai et al  take a different approach than  and propose
a method of ﬁnding adversarial examples for dqns trained for
automatic pathﬁnding the proposed attack analyzes a trained
dqn for the task and identiﬁes the weaknesses present in the
qvalue curves especially designed perturbations are added to
these weaknesses in the environment to effectively refrain the
agent from learning an optimal solution to the maze
xiao et al  introduce online sequential attacks on the
environment of the drl agent by exploiting the temporal con
sistency of the states this attack performs faster than the fgsm
algorithm as no backpropagation is needed and is based on
model querying the authors provide two methods for model
querying adaptive dimension sfd method and optimal frame
selection method in addition to these sequential attacks they
also propose other attacks on the observations action selection
and environment dynamics
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ieee transactions on artificial intelligence vol  no  april 
gleave et al  propose to introduce an adversarial agent in
the same environment as the legitimate agent the adversary is
not able to manipulate the observations of the legitimate agent
but can create natural observations that can act as adversarial
inputs and make the agent follow the target policy this leads to a
zerosum game between the adversarial agent and the legitimate
agent both the adversarial and the victim agent are based on
ppo after showing the existence of such adversarial policies
they suggest that the learning of the deployed model must
be frozen to save them from undesired behaviors enforced by
adversaries such adversarial agents can also be used in making
the models better by constantly attacking and retraining
although the drl techniques work better in navigation tasks
they are more vulnerable to adversarial attacks than the classic
methods yang et al  introduce timingbased adaptive adver
sarialperturbationsforlearningbasedsystemsinrealworldsce
narios they introduce two attacks namely wma whitebox
setting and asa via a populationbased training method based
on pepg blackbox setting the ﬁrst one is based on online
learning while the other is based on evolutionary learning
through experiments they show that out of the two proposed
attacks the wma shows a better performance
 manipulating the training data the adversary can also
choose to perturb the training data to indirectly having the agent
follow a targeted policy an adversary may create and hide some
deﬁciencies in the policy to use them later for his beneﬁt kiourti
etalshowthisvulnerabilityofdrlmodelstotrojanattack
with adversary having access to the training phase of the model
it is reported that by only modifying  of the training data
an adversary can induce such hidden behaviors in the policy that
the models perform perfectly well until the trojan is triggered
the proposed attack is shown to be resistant against current
defense techniques for trojans
a similar approach has been proposed by behzadan and
hsu  to secure drl models from model extraction at
tacks but can be used for adversarial purposes this involves
the integration of a unique response to a speciﬁc sequence of
states while keeping its impact on performance minimum hence
saving from the unauthorized replication of policies it is also
shown that the unwatermarked policies are not able to follow
the identiﬁed trajectory which is speciﬁed during the training
as already discussed this can be used by adversaries to hide
speciﬁc patterns in the policy and use them to their beneﬁt later
 manipulating the sensors the research on realtime at
tacks on robotic systems in a dynamic environment has not
been much explored clark et al  evaluate a whitebox
adversarial attack on the drl policy of an autonomous robot in
a dynamic environment the goal of the drl robot is to reach
the destination by routing through the environment while the
goal of the adversary is to mislead the agent into the wrong
routes the adversary misleads the agent into following false
routes by tampering with sensory data they also observe that
once the adversarial input is removed the robot automatically
reverts to taking the correct route hence an attacker can modify
the behavior of the model temporarily and leave behind zero or
very little evidence their attack requires access to the trained
policy but not the hyperparameters used during training
a similar observation has been shown by usama et al 
theyarguethatalotofresearchisbeingdoneforcreatingaiml
solutions to problems in future networks such as internet of
things and g they show these ml systems to be vulnerable
by highlighting the adversarial dimension of these systems they
prove their point by attacking a drlbased channel autoencoder
framework and showing its drop in performance noise is added
to the feedback channel for a certain time interval furthermore
they show that when this noise is removed the drl system
automatically can regain its original performance leaving no
footsteps by the adversary
b attacks perturbing the reward function
han et al  discuss the reaction of the drl agent in sdn
to different adversarial attacks the adversary adopts whitebox
and blackbox settings for both inference and poisoning attacks
in an online setting they propose two types of attacks ﬂipping
reward signals and manipulating states for ﬂipping reward
signals the adversary can manipulate the binary reward signal
of the model by ﬂipping it a certain number of times for state
manipulation the attacker makes two changes in the ﬁrst few
steps of the training ie an adversary can change the binary
reward of two states from  to  and  to  respectively hence
the adversary can change the label of one compromised node to
be uncompromised and vice versa
a similar falsiﬁcation approach has been followed by huang
and zhu  leading the agent into taking targeted decisions
they characterize a robust region for policy in which the ad
versary can never achieve the desired policy while keeping the
cost in this region they use four terms to specify different types
of attackers  omniscient attacker who has all the information
before a certain time t  peer attacker who does not know about
the transition probabilities but has access to the knowledge the
agent has before a time t  ignorant attacker who only knows
the cost signals before a time t and  blind attacker that has no
information at time t all these attackers may be limited by the
budget of the attack and other constraints it is shown that by the
falsiﬁcation of the cost at each state all of these adversaries can
mislead the agent into learning a policy desired by the adversary
rakhsha et al  propose a trainingtime attack involving
the poisoning of the learning environment to force the agent into
executing a target policy chosen by the adversary they con
sider rl agents that maximize average reward in undiscounted
inﬁnitehorizon settings and argue this to be a more suitable
objective for many realworld applications that have cyclic tasks
or tasks without absorbing states eg a scheduling task and
an obstacleavoidance robot they suppose that the adversary
can manipulate the rewards and the transition dynamics in the
learning environment at training time in a stealthy manner they
test their attack in both ofﬂine and online settings in the former
the agent is planning in a poisoned environment while in the
latter the agent is learning a policy using a regretminimization
framework with poisoned feedback they show that the adver
sary can easily succeed in teaching an adversarial policy to the
rl agent
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
c attacks perturbing the action space
the adversary can have access to the actuators and might try
to perturb the actions taken yeow et al  propose two attacks
on the action space of the drl algorithms the ﬁrst one is an
optimization problem for minimizing the cumulative reward of
the drl agent with decoupled constraints called mas attack
the second one has the same objective as the ﬁrst one but with
temporally coupled constraints called las attack the results
show that las is more lethal in deteriorating the performance
of the drl algorithm as it can attack the dynamic information
of the agent this attack is also shown to perform well in the
case of limited resources such attacks can be used to gain
insights into the potential vulnerabilities of the drl model
they also speculate that their proposed attacks on reward signals
by perturbing the action space cannot be defended as the action
space is independent of the policy however it can be detected
by having a look at the decay in the reward
due to the difﬁculty of obtaining the complex models for
cyberphysical systems for traditional control they are being
shifted to drl lee et al  argue that before transferring
these systems to drl the security limitations of drl must be
understood they propose a querybased attack for perturbing
the action space of drl in such systems furthermore they
show that by the use of adversarial training the attack success
can be reduced to half
d attacks perturbing the model space
the adversary can have access to the model during or af
ter training based on this access the adversary might try to
manipulate the model into learning the adversarial behavior or
might also try to extract the learned model and use it later for
attack purposes behzadan and hsu  propose an adversarial
attack for targeting the conﬁdentiality of the drl policy the
proposed attack performs a model extraction attack by using
imitation learning while querying the original model iteratively
they show that the adversarial examples generated for the model
extractedaretransferredsuccessfullytotheoriginalmodelhence
affecting its performance in a blackbox setting they use the
fgsm for generating adversarial examples for the imitated
model it is also shown that by providing the attack a sufﬁcient
number of observations adversarial examples can be crafted
with high efﬁciency they use adversarial regret ie the differ
ence between maximum return achievable by the trained policy
π and return achieved from actions of adversarial policy as a
metric to measure the performance of their attacks they show
an increase in adversarial regret in case of an adversarial policy
chen et al  argue that the techniques used for model
extraction in supervised ml cannot be applied to rl due to high
complexity and limited observable information and propose a
technique for model extraction in drl at ﬁrst they use an
rnn classiﬁer to reveal the training algorithm of the target
blackbox drl model based on the predicted actions then
they use imitation learning to replicate the victim model from
the extracted algorithm a ppo is used for imitation learning
the extraction of models can be used by adversaries to generate
successful adversarial examples making deployed models even
more vulnerable to adversarial attacks
huai et al  propose an optimization framework for de
riving optimal adversarial attack strategy for model poisoning
attacks they propose two attacks one in which adversarial
perturbations are added to the observations of the agent and the
other in which the attacker modiﬁes the parameters of trained
models in such a way that their performance is not affected the
ﬁrst one is termed as universal adversarial attack against drl
interpretations uadrli while the latter is termed as a model
poisoning attack against drl interpretations mpdrli for
uadrli they assume that the adversary has access to a certain
area of the images states and cannot perturb pixels outside this
certainareatheperturbationsareonlyaddedatsometimesteps
e discussion
in this section we discuss the attacks on drl by categorizing
them based on the targeted part of the mdp the adversary
can target the state space action space reward function or the
modelspacebasedontheaccessavailabletotheadversarywhen
targeting the state space the adversary can add perturbations to
the environment training data observations and sensory data
in the case of perturbing the action space the adversary can
target the actuators in the case of perturbing the reward function
the adversary can perturb the reward signal or might ﬂip it in
the case of modelspace attacks the adversary can perturb the
learned parameters of the model or might attempt to extract
the learned model which might be proprietary ie owned and
copyrighted by some organization
in real environments the attacks that generate imperceptible
and natural perturbations are more practical than the attacks
that involve adding specially designed perturbations to states in
applications like autonomous driving getting direct access to the
sensors might not be possible for the adversary the only option
is to perturb the environment hence indirectly affecting the ob
servations actions rewards and policies the real environments
are often blackbox where the adversary has no knowledge of
the system being attacked and the number of queries is limited
the adversary has to improvise to attack the system where the
target of the adversary can be to cause a drop in performance
of the system or to evade the system this puts forward a need
for queryefﬁcient attacks similar to those proposed in  for
supervised ml to be proposed for drl
table i shows a summary of the adversarial attacks on drl
iv defenses against adversarial attacks on drl
in this section we provide a detailed review of the counter
measures proposed to deal with adversarial attacks on drl
fig  shows a basic taxonomy of the defenses that can be used
for securing drl algorithms
a adversarial training
adversarial training includes retraining of the ml model
using the adversarial examples along with the legitimate ex
amples this increases the robustness of the ml model against
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ieee transactions on artificial intelligence vol  no  april 
table i
summary of the adversarial attacks on drl pipeline highlighting the threat model and attack location in the drl pipeline
adversarial examples as the model is now able to learn a better
distribution although adversarial retraining can help improve
the robustness of the ml model the ml model can still be com
promised through adversarial examples generated through some
other methods the goal of adversarial training is to improve
the generalization outside of the training manifold kos and
song  proposed using adversarial training for robustifying
drl algorithms they retrain their agent on perturbations gen
erated using the fgsm and random noise and show performance
retention against similar attacks furthermore they observe that
the retrained agent is also resilient against fgsm perturbations
having magnitudes different than the one used for retraining
pattanaik et al  also adopt adversarial training as a
measure to make the algorithms robust against gb attacks they
show its equivalence to robust control they train the drl model
by using the adversarial samples generated from the gb attacks
this helps the algorithm to model uncertainties in the system
making them robust to similar adversarial attacks they show
that the addition of noise to the training samples while training
increases the resilience of the drl models against adversarial
attacks han et al  also propose adversarial training as a
method of robustifying the drl algorithms against adversarial
attacks they show that this technique is effective when coun
tering attacks such as node corruption and node falsifying in
sdn
behzadan and munir  ﬁnd the adversarially trained poli
cies to be more robust to testtime attacks they investigate the
robustness of drl algorithms to both training and testtime
attacks and ﬁnd out that under the trainingtime attack the dqn
can learn and become robust by changing the policy they
propose that for an agent to recover from adversarial attacks
the number of the adversarial samples in the memory needs to
reach a critical limit in this way when the agent samples a
random batch from the memory it can learn the perturbation
statistics they also compare the performance of ϵgreedy and
parameterspace noise exploration methods in case of adversar
ial attacks they show the ϵgreedy methods to be more robust to
trainingtime attacks than the noisy exploration technique they
also ﬁnd noisy exploration techniques to be able to recover faster
from attacks when compared to the ϵgreedy methods
later on behzadan and munir  compare the resilience
to adversarial attacks of two dqns one based on ϵgreedy
policy learning and another employed noisynets  which
is a parameterspace noise exploration technique their results
show the noisynets to be more resilient to trainingtime attacks
than that of the ϵgreedy policy they argue that this resilience is
due to the enhanced generalizability and reduced transferability
in noisynets they propose that by using parameterspace noise
exploration the drl algorithms can be made robust to attack
techniques like fgsm chen et al  propose a gb adversarial
training technique they use adversarial perturbations gener
ated using their proposed attacking algorithm ie cdg for
retraining the rl agent this approach can achieve a precision
of  in detecting adversarial samples they prove that
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
fig 
taxonomy of the major defense schemes used in drl
adversarial training using only a single adversarial example
generated using cdg can realize the generalized cdgattack
immunity of ac pathﬁnding with high conﬁdence behzadan
and hsu  propose age after considering the sample inef
ﬁciency of current adversarial training techniques their tech
niqueisbasedonamodiﬁedhybridoftheϵgreedyalgorithmand
the boltzmann exploration exploring probabilistically relative
to expected rewards the new adversarial training procedure
is tested on dqn trained for the cartpole environment with
different perturbation probabilities they show that for small
perturbationsprobabilitiesieandtheagentcanrecover
fromtheattackwhileinthecaseoflargeprobabilitiessuchas
or  the agent is not able to recover they compare the efﬁciency
of their proposed technique with ϵgreedy and parameterspace
noise exploration algorithms and prove its feasibility
tanetalarguethatalthoughthedrlalgorithmsusedfor
decision and control tasks are vulnerable to adversarial attacks
little research has been done to make them robust after showing
the vulnerability of welltrained drl agents to action space
attacks they use adversarial training to increase the robustness
oftheattackedmodelfurthermoreaperformanceimprovement
of the adversarially trained agent over the normal agent in
nonadversarial scenarios is also shown lee et al  also show
that the use of adversarial training with their proposed attack
decreases the attack success rate to half
vinitsky et al  argue that the existing literature on robust
learning in drl focuses on training a single rl agent against a
single adversary and these systems are bound to fail in the case
of a different adversary they propose a populationbased aug
mentation to the robust rl formulation in which a population
of adversaries is randomly initialized and samples are drawn
uniformly from the population during training
b gametheoretic approach
pinto et al  propose rarl as a method of robust policy
learning in the presence of an adversary they formulate policy
learning as a zerosum minimax objective function to ensure
robustness to differences in test and train conditions even in the
presence of an adversary they use a selfproposed adversarial
agent with an especially designed reward targeted at ﬁnding
the statespace trajectories that lead to the worst rewards they
call these trajectories hard examples an adversary is introduced
in the environment whose goal is to destabilize the rl agent
abdullah et al  propose a robust rl using a novel minmax
game with a wasserstein constraint for a correct and convergent
solver this technique shows a signiﬁcant increase in robustness
inthecaseofbothlowandhighdimensionalcontroltasks they
also discuss that by using their technique the ddpg algorithms
are not able to achieve signiﬁcant performance improvement
in robustness even in the case of inverted pendulum while
the other two drl schemes ie trpo and ppo demonstrate
acceptable performance and hence are reported in their results
bravo and mertikopoulos  examine a game approach
where the players adjust their actions based on past payoff
observations that are subject to adversarial perturbations in
the singleplayer case containing an agent trying to adapt to
an arbitrarily changing environment they show that irrespective
of the level of noise in the players observations the stochastic
dynamics under study leads to no regret almost surely in the
case of multiple players they show that the dominated strategies
become extinct and the strict nash equilibrium is stochastically
stable and attractive conversely a stable or attractive state
with better probability is the nash equilibrium finally they
provide an averaging principle and show that in the case of
twoplayer zerosum games with an interior equilibrium the
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ieee transactions on artificial intelligence vol  no  april 
time averages converge to nash equilibrium for any noise level
ogunmolu et al  propose an iterative minimax dynamic
game framework that helps in designing robust policies in the
presence of adversarial inputs they also propose a method of
quantifying the robustness capacity of a policy they evaluate
their proposed framework on a mecanumwheeled robot the
goal of this agent is to ﬁnd a locally robust optimal multistage
policy that achieves a given goalreaching task
c robust learning
robust learning is a training mechanism to ensure robust
ness against trainingtime adversarial attacks behzadan and
munir  propose adding noise to the parameter state while
training this technique is found very effective in mitigating the
effects of both training and testtime attacks for both blackbox
and whitebox settings the results of the proposed method are
tested on dqn trained for three atari games namely enduro
assault and blackout in particular the authors use the fgsm
for crafting adversarial samples then they show the perfor
mance of the normal agents to deteriorate signiﬁcantly while
the ones that were retrained using the parameter noise show
great performance even in the presence of adversarial inputs
mandlekar et al  show superior resilience to adversarial
attacks by introducing an arpl algorithm this involves the use
of adversarial examples during training to enable robust policy
learning they consider the addition of adversarial perturbations
not only to the image space but also to the whole state of the
system which in their case also included the parameters such
as friction mass and inertia they use the gb fgsm technique
for the generation of adversarial samples they show that in the
case of agents that do not follow their learning technique the
performance deteriorates drastically while their agent can retain
the training performance it is important to note that the agent
trained using the arpl algorithm does not perform as well as
the normal one in case of no perturbations
wang et al  point out that the reward function is suscep
tible to three kinds of noise namely inherent noise application
speciﬁc noise and adversarial noise as a remedy they propose a
rewardconfusionmatrixtogeneraterewardstohelptherlagent
to learn in cases of perturbednoisy inputs such rewards are
called to be perturbed rewards using these perturbed rewards
they can develop an unbiased reward estimatoraided robust rl
framework their algorithm not only achieves higher expected
rewards but also converges faster they experiment with their
technique extensively using several drl algorithms which are
trained for different classic atari gaming environments their
proposed technique can achieve  and  improve
ments in average reward when the error rate is  and 
respectively in the case of ppo they discuss both the cases
of the perturbations added to some samples and perturbations
being added to all samples
policies that can retain the performance in nonstationary
environments are also robust to adversarial attacks that involve
adding noise to the state space smirnova et al  propose
a distributionally robust policy iteration scheme to restrict the
agent from learning suboptimal policy while exploring in cases
of highdimensional stateaction space this induces a dynamic
level of risk to stop the agent from taking suboptimal actions
their scheme is based on robust bellman operators which
provide a lower bound guarantee on the policystate values
they also present a distributionally robust soft actorcritic
based on mixed exploration acting conservatively in the short
term and exploring optimistically in a long run leading to an
optimal policy the direct target in  is adversarial robust
ness while in  the direct target is distributional robust
ness hence the target of adversarial robustness was achieved
indirectly
tessler et al  propose prmdp and nrmdp as two
new criteria for robustness they modify the ddpg to form
arddpg for solving these mdps the proposed techniques
are evaluated in various mujoco environments and the results
prove that the learning of actionrobust policies can help in
making the proposed algorithms secure and perform better even
in the absence of these perturbations the adversarial robustness
was achieved here by making the agent action robust
kumar et al  present a technique to make the drl
algorithm learn in the presence of noisy rewards the proposed
scheme is based on using a neural network as a noise ﬁlter
targeted at estimating the true reward of the environment these
estimates are then compared with the reward the agent gets at
each state to ﬁlter out the noisy samples they show that beyond
the perturbation probability of  their agent starts to learn
based on adversarial samples rather than the normal ones
fisher et al  propose the idea of robust student dqn
rsdqntheyproposetosplitthestandarddqnintotwonet
works namely a student policy network s and a qnetwork
this s network is robustly trained and used for exploration
while the qnetwork is trained normally this permits online
robust training while keeping the competitive performance of
the qnetworks they show that in the case of no attacks the
dqn and the rsdqn show the same performance while
in the case of adversarial attacks the dqn fails while the
rsdqn remains robust furthermore they show that the rs
dqn when combined with stateoftheart adversarial training
provides resilience to strong adversarial attacks during training
and evaluation
pan et al  argue that training rl on physical hardware
is dangerous due to exploration and can also be slow due to
high sample complexity this puts forward a need for a robust
learning algorithm that can make sure that the policy performs
well in catastrophic situations they propose rararl using
a riskaverse agent and a riskseeking adversary they point
out that the rarl technique proposed by pinto et al 
has no explicit modeling and optimization of risk as they only
optimizetheexpectedcontrolobjectiveanensembleofqvalue
networks is used to model risk as to the variance of value
functions their technique is similar to bootstrapped dqns 
proposed to assist exploration but in this case the purpose of the
ensemble is to estimate variance they test their approach on a
selfdriving environment using the torcs simulator and show
that a riskaverse agent handles the risk better and leads to fewer
crashesthananormalagenttrainedinasimilarenvironment the
attacker and the victim agent are made to work independently in
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
the same environment this gives more options to the attacker
to perturb the environment hence serving as a strong adversary
the overﬁtting of rl policies to the training environments
cause them to fail to generalize to safetycritical scenarios
wang et al  argue that the rarl technique proposed
by pinto et al  requires handcrafting sophisticated reward
signals which is a difﬁcult task they say that safety falsiﬁcation
methodscanbeusedtoﬁndasetofinitialconditionsaswellasan
input sequence to make a system violate a given property formu
lated in temporal logic they propose an frarl technique for
integrating temporallogic falsiﬁcation in adversarial learning to
improve policy robustness this removes the requirement of the
construction of an extra reward function for the adversary their
experiments show that the policies trained with frarl gener
alize better and show less violation of the safety speciﬁcations
in test scenarios when compared to techniques similar to rarl
lütjens et al  argue that adversarial detection detecting
adversarial samples using specialized models can only detect
a perturbed input but they cannot propose an alternate action
in case of an attack they leverage the research on certiﬁed
adversarial robustness to develop an online certiﬁed defense for
drl algorithms called carrl carrl involves computing
lower bounds on the stateaction pairs and choosing a robust
action in case of an adversarial attack they make their tech
nique certiﬁable robust by using robust optimization to consider
worstcase uncertainties and to provide certiﬁcates on solution
quality they show the effectiveness of their technique on a
dqn trained for a collisionavoidance system and a classic
control task cartpole and using the targeted fgsm as an
adversary their experiments show that carrl can  recover
and avoid the obstacles in case of an adversarial attack on col
lision avoidance system and  recover and achieve a sufﬁcient
reward in cartpole environment furthermore they prove that
although their technique reduces the computational efﬁciency
of the dqn it increases the robustness
zhang et al  point out that the robustness for continuous
action space drl has not got any attention and existing ap
proaches lack proper theoretical justiﬁcation they prove that
the classiﬁcation techniques like that of adversarial training
prove to be inefﬁcient for many rl problems they develop
a theoretically principled policy regularization and propose an
samdp their technique improves robustness under strong
whitebox attacks on state observations including the two new
attacks that they have proposed the robust sarsa attack rs
attack and mad attack furthermore they show performance
improvement in nonadversarial scenarios they assume the ad
versary to  be stationary deterministic and markovian ie
the adversary does not change with time and  have bounded
adversary power ie the adversary can only perturb a speciﬁc
number of states
oikarinen et al  propose a method of training drl
agents robust to lpbounded attacks they termed their tech
nique radialrl furthermore they propose a new metric
greedy worstcase reward for evaluating the performance of
drl algorithms against adversarial attacks they show their
technique to outperform the stateoftheart robust learning tech
niques   under pgd attack
zhang et al  propose a technique to enhance the robust
ness of drl agents against learned adversary attacks ie at
tacks in which the adversary is continuously learning they term
their technique as alternating training with learned adversaries
atla their technique involves the training of an adversarial
agent online together with the victim agent using policy gradient
following the optimal adversarial attack framework
d adversarial detection
adversarial detection involves the detection of adversarial
samples using a model especially trained to segregate the true
samples from the adversarial ones in this way we can disregard
the adversarial inputs without modifying the original model lin
et al  propose a method of protecting the drl algorithms
from adversarial attacks by leveraging an actionconditioned
frame prediction module by using this technique they can
detect the presence of adversarial attacks and make the model
robust by using the predicted frame instead of the adversarial
frame they also compare their results with other ml defense
approaches to show the effectiveness of this technique the
techniques used for adversarial example generation are fgsm
cw  and basic iterative method  the present results
indicate that their proposed technique can detect adversarial
attacks with accuracy from  to 
havens et al  detect the presence of adversarial attacks
via a supervisory agent by learning separate subpolicies using
the mlah framework because this technique can handle the
attacks in the decision space it can mitigate the learned bias
introduced by the adversary they consider a policy learning
problem that is being attacked at speciﬁc periods the goal of
the adversary is the corruption of state space while the agent
is being trained they assume that while training the agent
learns subpolicies before learning the ultimate policy thus
the supervisory agent can detect the presence of the adversarial
examples due to them being unexpected they use a selfdeﬁned
adversarial agent having the ability to perturb the states before
they reach the agent for training the perturbations generated
by this agent are bounded by lnorm
xiang et al  propose an advanced qlearning algorithm
for automatic pathﬁnding in robots that is robust to adversarial
attacks by detecting the adversarial inputs speciﬁcally they
propose a model to predict the adversarial inputs based on a
calculation determined by ﬁve factors energy point gravitation
key point gravitation path gravitation included angle and the
placid point the weights for these ﬁve factors are calculated
based on the pca using these factors they train a model able
to achieve a precision of  in segregating adversarial inputs
from the normal ones
gallego et al  introduce tmdps a variant of mdp
this framework supports the decisionmaking process in the
drl setting against adversaries that affect the reward generating
process they propose a levelk thinking scheme resulting in a
new framework for dealing with tmdps they show that while
a normal qlearning algorithm is exploited by an adversary
a level learner can approximately estimate the adversarial
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ieee transactions on artificial intelligence vol  no  april 
behavior and achieve a positive reward integrating dqns to
tmdps is discussed as a future research path
e defensive distillation
papernot et al  propose the idea of using defensive dis
tillation to deal with adversarial attacks on ml schemes in this
techniqueonemodelistrainedtopredicttheoutputprobabilities
of another model that was trained with an emphasis on accuracy
using this technique dl models can be made less susceptible to
exploitation by adding ﬂexibility to an algorithms classiﬁcation
process using adversarial training carlini and wagner 
show that defensive distillation gives a false sense of robustness
against adversarial examples rusu et al  present a method
of extracting the policy of a dense network to train another
comparatively less dense network this new network can take
expertlevel decisions while being smaller in size this method
can also be used to merge multiple taskspeciﬁc policies into a
single policy they show that the distilled agents which were
four times smaller than dqns were able to achieve better
performance than the dqn they also show that the agents
having  times fewer parameters than the dqn were able to
achieve a performance of  as compared to  of the
dqn such networks are proved to be more stable and robust
to adversarial noise and attacks as they have fewer parameters
than their denser counterparts and hence decreasing the count
of attackable parameters
recently czarnecki et al  analyzed empirically and the
oretically each variant of distillation and reported the strengths
and weaknesses of each variant furthermore they propose ex
pected entropy regularized distillation which makes the training
much faster while guaranteeing convergence this technique can
be used in making the drl models robust to adversarial attacks
by leveraging learning information from a complex model into
a simpler one hence making the models robust to adversarial
attacks however as discussed by carlini and wagner 
using this technique alone may not be effective it needs to
be combined with other approaches like adversarial training
adversarial detection etc to be successful
qu et al  propose robust policy distillation ie a policy
distillation paradigm capable of achieving an adversarially ro
bust student policy without relying on any adversarial example
during student policy training they propose a policy distillation
loss consisting of a prescription gap maximization loss and a
jacobian regularization loss they perform a theoretical analysis
and show that their proposed mechanism ensures the learning of
robust policies during the distillation process they show their
technique to outperform the one proposed in 
f discussion
we discuss the stateoftheart defenses in this section by
categorizing them into adversarial training robust learning
adversarial detection defensive distillation and gametheoretic
approaches it can be seen that most of these techniques are only
effective against the speciﬁed type of adversarial attacks and
do not provide any guarantees against other types of attacks
most of these techniques focus on making the drl agent
learn a robust policy by use of different mechanisms such as
training using adversarial examples simulating minmax games
with adversaries using robust alternatives of mdps etc these
techniques are more practical than the ones involving adversarial
detection due to the advent of new attacking strategies by the
day one can never be sure that a detection mechanism will be
able to detect the attack furthermore it is worth noting that there
are very few defenses for drl algorithms that do not involve
images as the observations
table ii summarizes key information of the proposed defenses
for drl algorithms
v metrics tools and platforms for benchmarking
drl
as we have previously discussed drl is different from other
ml schemes and only reporting the accuracy is not sufﬁcient
to cover security aspects of the drl schemes in particular we
need to consider the temporaldomain aspect of the drl while
designing the drlbased attack or defense benchmarking the
drl performance in attacks and defenses is very important
the need for an applicable solution to evaluate the robustness
and resilience of drl policies is not fulﬁlled by the current
literature there is also a need for a quantitative approach to
measure and benchmark the resilience and robustness of drl
policies in a reusable and generalizable manner
therearefewbenchmarksproposedbuttheyarenotsufﬁcient
to cover the security aspects needed to measure the robustness
and resilience of drl algorithms the few proposed approaches
are discussed in this section behzadan and hsu  introduce
the terms of adversarial budget and adversarial regret as a
measure to quantify the robustness and resilience of drl algo
rithms adversarial budget is deﬁned as the maximum number
of features that can be perturbed in the observation and the prob
ability of perturbing each observation the adversarial regret is
the difference between the reward obtained by the unperturbed
agent and the reward obtained by the perturbed agent after an
episode based on these two terms behzadan and hsu 
deﬁne testtime resilience and testtime robustness
a testtime resilience and robustness
testtime resilience is described as the minimum number of
perturbations required to incur the maximum reduction to return
at time t while testtime robustness is described as the maximum
achievable adversarial regret
the following procedure is proposed to measure testtime
resilience for drl algorithms
 approximate the stateaction value function using policy
imitation in case it is not already given
 report the optimal adversarial return and maximum ad
versarial regret by training the adversarial agent against
the targets policy
 apply the obtained adversarial policy to the target for sev
eral episodes while recording the return for each episode
 report the average adversarial return over these episodes
as the mean testtime resilience of the target policy
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
table ii
summary of defenses against adversarial attacks on drl
the method of measuring the testtime robustness is the same
as testtime resilience the only difference is that in the testtime
case we measure the average adversarial regret in place of the
average adversarial reward
behzadan and munir  propose a novel framework for
benchmarking the behavior of drlbased collision avoidance
mechanisms under the worstcase scenario of dealing with an
adversarial agent which is trained to drive the system into unsafe
states they prove the practical applicability of the technique
by comparing the reliability of two collision avoidance systems
against intentional collision attempts more recently behzadan
and hsu  have presented a technique for watermarking
drl policies for robustness against model extraction attacks
this involves the integration of a unique response to a speciﬁc
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ieee transactions on artificial intelligence vol  no  april 
sequence of states while keeping its impact on performance
minimum hence saving from the unauthorized replication of
policies it is shown that unwatermarked policies are not able
to follow the identiﬁed trajectory
b metrics for attack performance
kiourti et al  introduce three metrics for measuring the
performanceofthedrlattacksperformancegappercentageof
target action and time to failure as the name suggests the per
formance gap is the difference between the performance of the
normal and the victim model for the second metric percentage
of target action they measure the number of times the adversar
ialtargeted action is performed by the victim policy the third
metric time to failure is the number of consecutive states that
need to be perturbed to trigger a complete failure of the model
as observed these proposed measurement techniques can
only cover a part of the drl algorithms and hence are not
sufﬁcient for measuring the performance of the drl algorithms
under the wide range of adversarial attacks and defenses there
is therefore a need for the development of benchmarks that can
be used as standards for drl algorithms as a measure of their
resilience and robustness to adversarial attacks
c attacking drl tools and platforms
drl can be implemented using several available toolkits or
by using a combination of these toolkits some of the ways to
implement drl are the following
 openai gym  is a toolkit for testing the rl al
gorithms which provides with multiple gaming environ
ments such as pong space invaders and lunar lander this
toolkit is combined with tensorﬂow  to test the drl
algorithms the dnn part can be implemented on the later
one and then the choice of actions based on the states is
done by the neural network
 openai baselines  provide a set of highquality
implementations of rl algorithms
 rlcoach  provides with integrated mechanisms of
implementing the dnn and testing drl algorithms
 horizon  is an opensource project now known as
reagent  that also provides integrated mechanisms
for testing multiple drl algorithms
 nsgym platform  provides with network environ
ments to test rl algorithms this again can be combined
with tensorﬂow  to test drl algorithms
all of these toolkits can be further combined with the toolkits
available for attacking dl  and drl  to test dif
ferent attacks and defenses on drl algorithms in simulated
environments
vi open issues and research challenges
we identify the following major open issues and research
challenges in drl techniques at the end of this section we
have also provided a roadmap to secure and robustify drl
a universally robust algorithms
despite the presence of the various defenses that have been
proposed the security of drl algorithms remains an open
challenge the proposed defenses are only able to defend from
attacks they are designed for hence they are still vulnerable
to attacks led by proactive adversaries moosavidezfooli et
al  point out that no matter how many adversarial examples
are added to the training data there are new adversarial examples
that can be generated to cheat those newly trained networks
moreover if the adversary is only targeting conﬁdence levels
then we may never be able to detect the attack until the adversary
uses his created deﬁciency for his beneﬁt we may not be even
able to trace the attacks as shown by clark et al  thus
methods to make the drl algorithms more robust are an urgent
need
b multitask learning
one of the major challenges for drl is learning to do
multiple tasks at a single time it requires a lot of samples
for this currently proposed drl algorithms can only learn to
perform one task perfectly they can be trained to play multiple
games like cartpole inverted pendulum etc but they need
to be trained from scratch for each game the algorithms are
expected to be scalable and be more generalizable so that their
learning can be transferred from one game to another multitask
learning can help in making robust models that can grip the
true essence of the tasks and hence become difﬁcult to be
fooled
c metrics for robustness and resilience
we need to study why vulnerabilities exist in drl models
and how we can mitigate them and train robust models a major
reason for the existence of these vulnerabilities is the use of drl
models without the proper knowledge of the domain there is a
need to properly deﬁne the benchmarks of drl in terms of the
robustness of drl against adversarial attacks behzadan and
hsu  have proposed techniques for quantifying the robust
ness and resilience of the rl algorithms some benchmarks are
also proposed by kiourti et al  but as previously discussed
these benchmarks are inadequate to measure the robustness and
resilience of an algorithm even though they can be used as
stepping stones to lead to a ﬁnal goal
d system design and transferability
system design remains an open challenge for the case of drl
there is a need to deﬁne standards for system design for drl
problems as in this case the learning process is not supervised
so the agent may not focus on the features that it needs to
learn this can introduce the error by mistake of the intermediary
and also even induce his behavior on the model we need to
have proper standards for designing the reward functions the
system design needs to be robust and resilient to adversarial
attacks
authorized licensed use limited to national university fast downloaded on april  at  utc from ieee xplore  restrictions apply 
ilahi et al challenges and countermeasures for adversarial attacks on deep reinforcement learning
e ensemble of defenses
various ensemble defenses have been proposed for the case
of dl however they may not be appropriate to apply in the
case of drl as it can lead to an exponential increase in the
complexity of the model which results in a signiﬁcant decrease
in performance in the case of drl the model is making a
realtime prediction so a small reduction in the computation
capabilities may cause a great loss to the agent this remains
a challenge to defend drl models using an ensemble with a
minimum loss of computations
f model privacy
privacy has become a leading issue these days and model
extraction attacks pose a serious threat to the integrity of the
learned models through illegal duplication a mitigation for this
suggested by behzadan and hsu  is to increase the cost of
such attacks or to watermark the policies we may experience
some randomness in the agent to save from such attacks but
that will incur an unacceptable loss of decreased performance
developing techniques that can incur constrained randomization
in the model to save from such attacks is a promising ﬁeld of
research
g explainable and transparent drl
fordeployingaisystemsinrealworldscenariostrustisakey
componentthedeveloperneedstobeconﬁdentoftheemployed
models decisions transparency ensures that the model is fair
and ethical while explainability helps to explain and justify the
models decisions there are a few articles that discuss explain
ability for drl   current techniques for explaining
drl do not speciﬁcally focus on targeting speciﬁc audiences
ie tester developer and the general public and there is a need
for the development of such techniques  only through
their development we might be able to make drl responsible
trustworthy and applicable in critical applications
h transfer learning for drl
transfer learning  is a ﬁeld of ml that does not require
the training data and test data to be independent and identically
distributed the model is not needed to be trained from scratch
in case of a different domain hence signiﬁcantly reducing the
demand of training data and training time of the target model
the research on transfer learning in the context of drl has been
limited and there are a few papers that test and discuss transfer
learning for only some speciﬁc drl algorithms  transfer
learning can help with saving training time for these models
hence making drl more applicable to realworld scenarios
i roadmap toward secure and robust drl
the ultimate goal of research in ai is to develop agi which
can perform similar activities as humans in a more efﬁcient
mannerinadditiontothealgorithmbeingabletolearnthetaskat
hand efﬁciently it also has to be computationally efﬁcient while
being robust to adversarial attacks furthermore algorithms
need to be sample efﬁcient to be able to learn quickly in real
environments as there might not always be enough time to wait
for the algorithm to converge meeting all these requirements
at the same time is a challenging task and one has to strike
intelligent tradeoffs between them
in this regard the ﬁrst task to achieve is the development
of sampleefﬁcient and inherently robust drl algorithms the
second task is the development of explainability techniques
whichcanexplainthebehaviorofthesealgorithmsinaccordance
with human perception the ﬁnal task will be the development of
metrics that can be used to quantify the robustness and resilience
of these drl algorithms based on these metrics and the sample
and computational efﬁciency one can then choose the most
suitable algorithm for the task at hand
vii conclusion
the broadening applicability of drl in the real world has
directed our concern to the security of these algorithms against
adversarial attacks this article has provided a comprehensive
survey of the latest techniques proposed for attacking drl
algorithms and the defenses proposed for defending against
these attacks we have also discussed the open research issues
and provided the list of available benchmarks for measuring the
resilience and robustness of drl algorithms
acknowledgment
the statements made herein are solely the responsibility of
the authors
