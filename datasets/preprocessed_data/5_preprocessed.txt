adversarial machine learning attack on
modulation classiﬁcation
muhammad usama muhammad asim junaid qadir ala alfuqaha muhammad ali imran
information technology university lahore punjab pakistan
hamad bin khalifa university qatar
university of glasgow scotland uk
email muhammadusama msee junaidqadirituedupk aalfuqahahbkueduqa muhammadimranglasgowacuk
abstractmodulation classiﬁcation is an important component
of cognitive selfdriving networks recently many mlbased
modulation classiﬁcation methods have been proposed we have
evaluated the robustness of  mlbased modulation classiﬁers
against the powerful carlini  wagner cw attack and showed
that the current mlbased modulation classiﬁers do not provide
any deterrence against adversarial ml examples to the best
of our knowledge we are the ﬁrst to report the results of the
application of the cw attack for creating adversarial examples
against various ml models for modulation classiﬁcation
index termsadversarial machine learning modulation clas
siﬁcation
i introduction
the success of machine learning ml in computer vi
sion and speech processing has motivated the networking
community to consider deploying ml for the automation of
networking operations recently new networking paradigms
like cognitive selfdriving networks  and most recently
knowledge deﬁned networking  have also emerged that de
pend on and facilitate the extensive utilization of ml schemes
for conducting networking tasks recently ml has success
fully applied on different cognitive selfdriving networking
tasks such as modulation classiﬁcation  and representation
learning of radio signals 
although ml schemes especially deep neural networks
dnn have outperformed traditional networking schemes in
many networking tasks it has been shown recently that dnn
and other ml schemes lacks robustness against adversarial
examples which are deﬁned as inputs to the ml model
specially crafted by an adversary to cause a malfunction in
the performance of the ml model these adversarial examples
are generated by adding small typicallyimperceptible pertur
bations to the legitimate examples for the express purpose of
misleading the ml model towards the production of wrong
results and to increase the prediction error of the model
based on the adversarys knowledge adversarial attacks are
classiﬁed into two major categories whitebox attacks and
blackbox attacks in whitebox attacks it is assumed that
the adversary has perfect knowledge about the victim model
whereas in blackbox attacks it is assumed that adversary has
no information about the victim model and the adversary can
only query the deployed ml model for a response and to later
use this information for crafting adversarial examples
more formally an adversarial example xis crafted by
adding a small imperceptible perturbation δ to the test example
x of the deployed trained classiﬁer f the perturbation
δ is computed by approximating iteratively the nonlinear
optimization problem given in equation  until the crafted
adversarial example gets classiﬁed by the trained ml classiﬁer
f in a wrong class t
x x  arg min
ηx η fx  η  t
adversarial examples are a direct consequence of an unsafe
assumption in ml that distribution encountered by the ml
model in training phase will also be encountered in the test
phase of the ml model
the effects of adversarial ml examples in cognitive self
driving networks have not been explored properly in the
literature in this paper we have performed an adversarial
attack on ml classiﬁers performing the task of modulation
classiﬁcation which is an important application in cogni
tive selfdriving networks our results clearly highlight that
a small optimallycalculated adversarial perturbation for the
test example can cause a serious drop in performance of
the classiﬁcation output of the ml model this paper also
highlights the vulnerability and brittleness associated with the
ml models used in the cognitive selfdriving networks
the major contributions of this work are
 we have performed an adversarial ml attack on  ml
based modulation classiﬁers to highlight the vulnerability
of these modulation classiﬁers to adversarial perturbation
 we demonstrate the transferability phenomenon in the
setting of modulation classiﬁers by showing that an
adversarial example compromising one ml scheme will
also be to evade other ml schemes with high probability
 to this best of our knowledge this is the ﬁrst experiment
where the carlini  wagner cw attack  has been
used to attack the modulation classiﬁcation task
the rest of the paper is organized as follows in the next sec
tion we will provide a brief review of the related research that
focuses on mlbased modulation classiﬁcation and adversarial
attacks on modulation classiﬁcation section iii describes the
methodology where we have discussed the assumed threat
model mlmodels used for modulation classiﬁcation and the
utilized adversarial attack for crafting adversarial examples
section iv provides the performance evaluation of the ad
versarial attack on the modulation classiﬁcation section v
concludes the study and provides future directions
arxivv  cscr   sep 
ii related work
a modulation classiﬁcation using ml schemes
the recent success of ml in computer vision and cyber
physical systems has inspired a surge in the utilization of ml
schemes in wireless and data networks it is conceived that ml
will be the backbone of future cognitive selfdriving networks
modulation classiﬁcation is an important problem in dynamic
spectrum allocation of cognitive selfdriving networks there
are few mlbased modulation classiﬁcation schemes available
in the literature wong et al  used a combination of
genetic algorithm and multilayer perceptron for digital modu
lation recognition aslam et al  used genetic programming
with knearest neighbor knn for modulation classiﬁcation
although genetic algorithms provides a good heuristicbased
solution but these algorithms do not scale efﬁciently with the
increase of the sample population
muller et al  employed a combination of discrimina
tive learning and support vector machines svm for mod
ulation classiﬁcation mendis et al  utilized deep belief
networks dbn for modulation classiﬁcation although dbn
has produced very impressive results but they are known
to be very difﬁcult to train and scale oshea at al 
used convolutional neural network cnn vgg and resnet
for performing modulation classiﬁcation schemes where they
have compared the deep mlbased modulation classiﬁcation
with the conventional modulation schemes under different
conﬁguration and noise levels and showed that mlbased
schemes performed better even in low signal to noise ratio
snr using ml schemes have produced very good results
but they are vulnerable to adversarial examples crafted by the
adversary to fool the mlbased classiﬁer to perform incorrect
classiﬁcation
b adversarial attacks on mlbased modulation classiﬁcation
there has not been much work available on exploring the
threat of adversarial ml examples on modulation classiﬁca
tion sadeghi et al  used a variant of fast gradient sign
method fgsm  to perform an adversarial ml attack on
cnnbased modulation classiﬁcation and successfully showed
a considerable drop in classiﬁcation accuracy fgsm is a
technique for crafting adversarial example where one step
gradient update is performed in the direction of the sign
associated with the gradient at each feature in the test example
the fgsm perturbation η is given as
η  ϵsignxjθx l
flowers et al  provided an evaluation framework for
testing modulation classiﬁers against adversarial ml attacks
they have tested the modulation classiﬁer against fgsm and
gaussian random noise base adversarial attacks and showed
that fgsm causes more destruction than the random gaussian
noise similarly kokalj et al  used the fgsm attack
to demonstrate the vulnerability of modulation classiﬁcation
against adversarial examples bair et al  highlighted the
limitations of the targeted adversarial attack on modulation
classiﬁcation in whitebox settings
most of the results of the adversarial attacks reported on
modulation classiﬁcation have used fgsm attack without con
sidering that fgsm was not designed to generate the optimal
amount of adversarial perturbation it was only designed with
an absolute motivation of generating adversarial perturbations
quickly rather than optimally  in this paper we have
performed cw attack  on modulation classiﬁcation to
compute the optimal adversarial perturbation
iii methodology
in this section we describe our procedure for performing an
adversarial attack on modulation classiﬁcation to the best of
our knowledge there is no standardized mlbased solution for
modulation classiﬁcation in the cognitive selfdriving networks
available yet in the literature so for completeness we have
used both conventional and deep ml schemes for modulation
classiﬁcation before delving deep into the details of the
mlmodels used for modulation classiﬁcation and adversarial
attack on it we describe the threat model and a few related
assumptions
a threat model
this subsection describes the major assumptions considered
for performing adversarial attack on modulation classiﬁer
 adversary knowledge we have assumed a whitebox
settings for performing an adversarial attack on dnn based
modulation classiﬁcation which means adversary has the
complete knowledge about the model architecture related
hyperparameters and the test data this assumption is fairly
standard in the adversarial ml domain we have transferred
the adversarial examples for dnnbased modulation classiﬁer
to other conventional mlbased modulation classiﬁers in this
paper we have only assumed test time adversarial attacks
poisoning attacks are left for future considerations
 adversarial goals our goal in this experiment is to
compromise the integrity of the modulation classiﬁer through
adversarial examples and the success of the adversarial attack
in this paper will be measured by the comparison of the
accuracy before and after the adversarial attack
b modulation classiﬁcation models
we have used dnn knn svm na
ıve bayes nb linear
discriminant analysis lda decision tree dt random
forest rf and ensemble methods for modulation classiﬁ
cation to the best of our knowledge this is the ﬁrst paper
that uses almost all the famous ml schemes for modulation
classiﬁcation and then performs adversarial ml attack on these
schemes to highlight that conventional ml deep ml and
ensemble methods do not provide robustness against small
carefullycrafted perturbations
for the dnn classiﬁer we have used four dense hidden
layers network with rectiﬁed linear units as a nonlinear
activation in hidden layers and softmax for calculating the
classiﬁcation probabilities of each class stochastic gradient
descent sgd has been used as an optimizer and categorical
crossentropy as the associated loss function for training the
dnn based modulation classiﬁer for the knn classiﬁer we
have used  neighbors as an optimal number of neighbors for
performing the classiﬁcation we have used radial basis func
tion rbf kernel for performing the svm based modulation
classiﬁcation for the nb classiﬁer we have assumed gaussian
distribution as the underlying modulation data distribution for
the ldabased classiﬁcation we have used singular value
decomposition svd solver as an svd solver can better
handle a large number of modulation data features for the
dt classiﬁer we have used maximum unfolding depth of 
for achieving good classiﬁcation result for the rf classiﬁer
we have a maximum of  trees forest for estimating the
classiﬁcation results for ensemble methods for modulation
classiﬁcation we have employed adaboost and gradient
boosting algorithms the obtained classiﬁcation results are
provided in section iv
c adversarial attack
we have performed cw  attack on mlbased mod
ulation classiﬁers to demonstrate the lack of robustness of
the mlbased modulation classiﬁcation scheme in cognitive
selfdriving networks carlini et al  proposed three very
powerful adversarial ml perturbation crafting techniques by
using three distance matrices l l and l and these
attacks have successfully evaded the defensive distillation
method  a popular early scheme for defending against
adversarial examples
we have used lbased cw attack for crafting adversarial
examples instead of formulating the adversarial ml problem
as in equation  which is highly nonlinear formulation that
is difﬁcult to optimize an alternative formulation provided
in equation  where gx is the new objective function such
that gx  iff gx  t here t can be any label but the
true label is used by the the cw attack that can be solved
by gradient descent the best performing objective function
g used for crafting adversarial examples for modulation
classiﬁcation is provided in equation  where z denotes the
softmax function
minimize
η
ηp  cgx
such that
x  n
gx  max max
it zxi zxt
we have only opted to use an lbased adversarial attack
because we want to keep the perturbation η to a minimum
while minimizing the squared error between adversarial mod
ulation example and the original modulation example
many defenses against adversarial examples have been
proposed in literature but this powerful attack has beaten all
of them  and to the best of our knowledge there does not
exist any defense that ensures robustness against lbased c
w adversarial attack
in our experiments we wish to achieve the following
objectives
 objective  do the ml schemes used for modulation
classiﬁcation in the literature provide necessary robust
ness against adversarial perturbations
 objective  we want to experimentally verify that the
adversarial examples breaching one ml schemes will
breach other ml models with high probability even if
the deployed ml model is unknown
before explaining how we have met these objectives through
our experiment in the next section we provide a detailed
description of the dataset used for performing the experiments
d dataset
we have used highly cited gnu radio ml rmla
dataset  for our experimentation the reason for selecting
this dataset is its public availability and utilization in the
literature dataset consists of  input examples where
each example is associated with a modulation scheme at a
speciﬁc snr dataset has  modulation schemes namely
amdsb amssb wbfm pam bpsk qpsk psk
qam qam cpfsk and gfsk out of these  mod
ulation schemes  are digital modulations and  are analog
modulation schemes
for this experiment we have used eight digital modulation
schemes and excluded three analog modulation schemes the
excluded schemes are amdsb amssb and wbfm the
total number of examples used in these experiments is 
each example is a  size vector with  inphase and 
quadraturephase components this dataset was generated for
 different snr levels from  db to  db more details
of the dataset preparations can be found in 
iv performance evaluation
in this section we have provided a detailed evaluation
of the mlbased modulation classiﬁers against adversarial
perturbations
a performance impact
we have evaluated the mlbased modulation classiﬁcation
before and after the adversarial attack we have used the
accuracy as the performance metric the decay in the modu
lation classiﬁcation describes the adversarial attack success
figure  provides a detailed comparison of accuracy and snr
before and after the adversarial attack the clear drop in the
accuracy of the classiﬁers with increasing snr fulﬁlls the
ﬁrst objective of this experiment where we set out to show
that the mlschemes proposed in the literature for modulation
classiﬁcation does not provide the necessary robustness against
adversarial examples
b transferability of adversarial examples
here we want to note that adversarial examples were only
crafted for dnnbased modulation classiﬁer under white
box assumptions the adversarial examples compromising
the integrity of the dnn classiﬁers were transferred to the
rest of ml classiﬁers under blackbox assumptions and it
turns out that modulation classiﬁers based on conventional
ml techniques are also equally vulnerable to the adversarial
examples which fulﬁll the second objective we wanted to
achieve through this experiment
v conclusions
in this paper we have highlighted the lack of robustness
of mlmodels utilized in modulation classiﬁcation by suc
cessfully evading  different mlbased modulation classiﬁers
we have also successfully shown that transferability of the
adversarial examples from one model to another model for
performing the adversarial attack this work has also provided
a glimpse of the security and robustness issues associated
with the utilization of ml models in cognitive selforganizing
networks designing new defenses against adversarial attacks
on cognitive selfdriving networks are left as a future direction
figure  the accuracy of ml models used for modulation classiﬁcation before and after adversarial ml attack is provided in the ﬁgure a clear drop in the
accuracy with improving snr after the adversarial attack clearly indicates the lack of deterrence against small carefully crafted adversarial perturbations
